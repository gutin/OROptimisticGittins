\section{Model and Preliminaries} \label{sec:model_and_prelim}
We consider a multi-armed bandit problem with a finite set of arms  $\mathcal{A} = \{1,\ldots,A\}$. Arm $i \in \mathcal{A}$ if pulled at time $t$, generates a stochastic reward $X_{i,N_i(t)}$ where $N_i(t)$ denotes the cumulative number of pulls of arm $i$ up to and including time $t$. $(X_{i,s}, s \in \mathbb{N})$ is an i.i.d. sequence of random variables, each distributed according to $p_{\theta_i}(\cdot)$ where $\theta_i \in \Theta$ is a parameter. Denote by $\theta$ the tuple of all $\theta_i$. The expected reward from the $i$\textsuperscript{th} arm is denoted by $\mu_i(\theta_i) := \E[X_{i,1} | \theta_i]$. We denote by $\mu^*(\theta)$ the maximum expected reward across arms;  $\mu^*(\theta) := \max_i \mu_i(\theta_i)$ and let $i^*$ be an optimal arm. The present paper will focus on the Bayesian setting, and so we suppose that each $\theta_i$ is an independent draw from some prior distribution $q$ over $\Theta$. All random variables are defined on a common probability space $(\Omega, \mathcal{F}, \mathbb{P})$. We define a policy, $\pi := (\pi_t, t \in \mathbb{N})$, to be a stochastic process taking values in $\mathcal{A}$. We require that $\pi$ be adapted to the filtration $\mathcal{F}_t$ generated by the history of arm pulls and their corresponding rewards up to and including time $t-1$.

Over time, the agent accumulates rewards, and we denote by 
\[
V(\pi, T, \theta) := 
\E\left[
	\sum_t
	X_{
		\pi_t,
		N_{\pi_t}(t)
	}
	\given[\Big]
	\theta
\right]
\] 
the reward accumulated up to time $T$ when using policy $\pi$. We write $V(\pi,T) := \E{V(\pi,T,\theta)}$. The regret of a policy over $T$ time periods, for a specific realization $\theta \in \Theta^A$, is the expected shortfall against always pulling the optimal arm, namely
\[
\Regret{\pi, T, \theta} := 
T \mu^*(\theta) -
V(\pi, T, \theta)
\]
In a seminal paper, \cite{lai1985asymptotically} established a lower bound on achievable regret. They considered the class of policies under which for {\em any} choice of $\theta$ and positive constant $a$, any policy in the class achieves $o(n^a)$ regret. They showed that for any policy $\pi$ in this class, and any $\theta$ with a unique maximum, we must have
\begin{equation}
\label{eq:lai_robbins_lb}
\liminf_T
\frac{
	\Regret{\pi, T, \theta}
}
{
	\log T
}
\geq
\sum_{i}
\frac{\mu^*(\theta) - \mu_i(\theta_i)}{d_{\rm KL}\left(p_{\theta_i},p_{\theta_{i^*}} \right)}
\end{equation}
where $d_{\rm KL}$ is the Kullback-Liebler divergence. The Bayes' risk (or Bayesian regret) is simply the expected regret over draws of $\theta$ according to the prior $q$:
\[
\Regret{\pi, T} := T \E[\mu^*(\theta)] - V(\pi, T).
\]
In yet another landmark paper, \cite{lai1985asymptotically} showed that for a restricted class of priors $q$ a similar class of algorithms to those found to be regret optimal in \cite{lai1985asymptotically} were also Bayes optimal. Interestingly, however, this class of algorithms ignores information about the prior altogether. A number of algorithms that {\em do} exploit prior information have in recent years received a good deal of attention; these include Thompson sampling \cite{thompson1933likelihood}, Bayes-UCB \cite{kaufmann2012thompson}, KL-UCB \cite{garivier2011kl}, and Information Directed Sampling \cite{russo2014learning}. 

The Bayesian setting endows us with the structure of a (high dimensional) Markov Decision process. An alternative objective to minimizing Bayes risk, is the maximization of the cumulative reward discounted over an infinite horizon. Specifically, for any positive discount factor $\gamma < 1$, define
\[
V_\gamma(\pi) := 
\E_q \left[
	\sum_{t=1}^\infty \gamma^{t-1} X_{\pi_t,N_{\pi_t}(t)}
\right].
\]
The celebrated Gittin's index theorem provides an {\em optimal, efficient} solution to this problem that we will describe in greater detail shortly; unfortunately as alluded to earlier even a minor `tweak' to the objective above -- such as maximizing cumulative expected reward over a finite horizon renders the Gittins index sub-optimal \cite{nino2011computing}. 

As a final point of notation, every scheme we consider will maintain a posterior on the mean of an arm at every point in time. We denote by $q_{i,s}$ the posterior on the mean of the $i$th arm after $s-1$ pulls of that arm; $q_{i,1} := q$. Since our prior on $\theta_i$ will frequently be conjugate to the distribution of the reward $X_i$, $q_{i,s}$ will permit a succinct description via a sufficient statistic we will denote by $y_{i,s}$; denote the set of all such sufficient statistics $\mathcal{Y}$. We will thus use $q_{i,s}$ and $y_{i,s}$ interchangeably and refer to the latter as the `state' of the $i$th arm after $s-1$ pulls.  

