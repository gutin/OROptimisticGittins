\section{Analysis and Regret bounds} \label{sec:analysis_of_regret}
We establish a regret bound for the OGI algorithm that applies when the prior distribution $q$ is uniform and arm rewards are Bernoulli. The result shows that the algorithm, in that case, meets the Lai-Robbins lower bound and is thus asymptotically optimal in a frequentist sense. After stating the main theorem, we briefly discuss a generalization to the algorithm.

In the sequel, we will simplify notation  and let $d(x,y) := d_{\rm KL}(\text{Ber}(x),\text{Ber}(y))$ denote the KL divergence between Bernoulli random variables with parameters $x$ and $y$. We will also refer to the OGI policy, which uses a look-ahead parameter of $K$, as $\pi^{{\rm OG},K}$ and will write the Optimistic Gittins index of the $i$\textsuperscript{th} arm at time $t$ as $v^K_{i,t} \defeq v^K_{1-1/t}(y_{i,N_i(t-1)})$. That way, for the sake of brevity, we will suppress the index's dependence on $y_{i,N_i(t-1)}$. We are ready to state the main result below.
\begin{theorem} \label{thm:frequentist_optimal_bound}
	Let $\epsilon > 0$ and consider an OGI policy configured with a parameter $K \in \N$ and that assumes Beta($1,1$) priors. For the multi-armed bandit problem with Bernoulli rewards and any parameter vector $\theta \subset [0,1]^A$, there exists $T^* = T^*(\epsilon, \theta, K)$ and $C = C(\epsilon,\theta, K)$ such that for all $T \ge T^*$,
	\begin{equation}
	\Regret{\pi^{{\rm OG},K}, T, \theta} \le \sum_{\substack{i=1,\ldots,A \\ i \ne i^*}} \frac{(1+\epsilon)^2(\theta^* - \theta_i)}{d(\theta_i, \theta^*)} \log T  + C(\epsilon,\theta, K)
	\end{equation}
	where $C(\epsilon,\theta, K)$ is a constant that is determined by $\epsilon$, the parameter $\theta$ and $K$.
\end{theorem}
\begin{myproof}[Proof.]	
	Assume, without loss of generality, that the first arm is uniquely optimal so that $\theta^* = \theta_1$. Fix an arbitrary sub-optimal arm, which for convenience we will say is the second arm. We will strategically fix three constants in between the expected rewards of the first and second arms, namely $\theta_1$ and $\theta_2$. In particular, we let $\eta_1,\eta_2,\eta_3 \in (\theta_2, \theta_1)$ be chosen such that $\eta_1 < \eta_2 < \eta_3$, $d(\eta_1, \eta_3) = \frac{d(\theta_2, \theta_1)}{1+\epsilon}$ and $d(\eta_2,\eta_3) =\frac{d(\eta_1, \eta_3)}{1+\epsilon} $. Next, we define the constant $L(T) := \frac{\log T}{d(\eta_2,\eta_3)}$ to be, intuitively, the optimal length of the exploration period.
	
	The main step in this proof will be to upper bound the expected number of pulls of the second arm, as follows,
	\begin{align}
	\E[N_2(T)] & \le L(T) + \sum_{t=\floor{L(T)}+1}^T \P{\pi^{{\rm OG},K}_t = 2, \; N_2(t-1) \ge L(T)} \nonumber \\
	& \le L(T) +   \sum_{t=1}^T \P{v^K_{1,t} < \eta_3} + \sum_{t=1}^T \P{\pi^{{\rm OG},K}_t = 2,\; v^K_{1,t} \ge \eta_3, \; N_2(t-1) \ge L(T)} \nonumber \\
	& \le L(T) +   \sum_{t=1}^T \P{v^K_{1,t} < \eta_3} + \sum_{t=1}^T \P{\pi^{{\rm OG},K}_t = 2,\; v^K_{2,t} \ge \eta_3, \; N_2(t-1) \ge L(T)} \nonumber \\
	& \le \frac{(1+\epsilon)^2 \log T}{d(\theta_2, \theta_1)} + \underbrace{\sum_{t=1}^\infty \P{v^K_{1,t} < \eta_3}}_{A} + \underbrace{\sum_{t=1}^T \P{\pi^{{\rm OG},K}_t = 2,\;v^K_{2,t} \ge \eta_3, \; N_2(t-1) \ge L(T)}}_{B}, \label{bound:final_step_in_freq_regret}
	\end{align}
	where the first step is the same as in the analysis of \cite{auer2002finite} and applies to any bandit policy. All that remains is to show that terms $A$ and $B$ are bounded by constants. These bounds are given in Lemmas~\ref{lemma:underestimation} and \ref{lemma:overestimation} whose proofs we will now describe at a high-level and defer the full details to the Appendix.
	\begin{lemma}[Bound on term A] \label{lemma:underestimation}
		For any $\eta < \theta_1$, the following bounds holds for some constant $C_1 = C_1(\epsilon, \theta_1, K)$
		\begin{equation*}
		\sum_{t=1}^\infty \P{v^K_{1,t} < \eta} \le C_1.
		\end{equation*}
	\end{lemma}
	\begin{myproof}[Proof outline]
		The goal is to bound $\P{v^K_{1,t} < \eta}$ by an expression that decays fast enough in $t$ so that the series converges. 
		This demonstrates that the algorithm encourages enough exploration such that the optimal arm is never underestimated for too long, in expectation.
		Specifically, we show that there exists a positive constant $h$ so that $\P{v^K_{1,t} < \eta} = O\left(\frac{1}{t^{1+h}}\right)$ using an induction argument. Proving the base case requires using properties specific to Beta and Bernoulli random variables, while the inductive step is more general.
		The full proof is contained in Appendix~\ref{proof:underestimation_proof}.
		
		We remark that the core steps in the proof of Lemma~\ref{lemma:underestimation}, at least in the base case of the induction, rely on properties of the Beta and Bernoulli variables. Because of this, we suspect our analysis can strengthen a similar theoretical result for the Bayes UCB algorithm. In particular, the main theorem of \cite{kaufmann2012thompson} states that the quantile parameter in the Bayes UCB algorithm should be $1 - 1/(t \log^c T)$ for some constant $c \ge 5$. However, what is perplexing is that their simulation experiments suggest that using a simpler sequence of quantiles, namely $1 -1/t$, results empirically in a lower mean regret. By utilizing techniques in our analysis, it is possible to prove that the use of the quantiles $1-1/t$ would lead to the same optimal regret bound. Therefore the `scaling' by $\log^c T$ is unnecessary.
	\end{myproof}
	\begin{lemma}[Bound on term B] \label{lemma:overestimation}
		There exists $T^* = T^*(\epsilon, \theta)$ sufficiently large and a constant $C_2 = C_2(\epsilon, \theta_1, \theta_2)$ so that for any $T \ge T^*$, we have
		\begin{equation*}
		\sum_{t=1}^T \P{\pi^{{\rm OG},K}_t = 2,\; v^K_{2,t} \ge \eta_3, \; N_2(t-1) \ge L(T)} \le C_2.
		\end{equation*}
	\end{lemma}
	\begin{myproof}[Proof outline]
		This relies on a concentration of measure result and the assumption that the 2\textsuperscript{nd} arm was sampled at least $L(T)$ times. Because our index is non-increasing in $K$, from Lemma~\ref{lemma:approx_bound}, it is enough to only consider the simplest case when $K = 1$. The full proof is given in Appendix~\ref{proof:overestimation_proof}.
	\end{myproof}
	Lemma~\ref{lemma:underestimation} and \ref{lemma:overestimation}, together with \eqref{bound:final_step_in_freq_regret}, imply that
	\[
	\E[N_2(T)] \le \frac{(1+\epsilon)^2 \log T}{d(\theta_2, \theta_1)} +  C_1 +  C_2,
	\]
	and from this the regret bound follows.
\end{myproof}
\subsection{Generalizations and a tuning parameter}
As we have shown, the OGI algorithm is regret optimal for the Bernoulli bandit problem. Morever, it is possible to generalize our algorithm to problems with any bounded reward distribution and prove a weaker $O(\log T)$ regret bound. We see this immediately from the discussion in \cite{agrawalanalysis}, where it is shown that any bandit algorithm that is regret optimal for the Bernoulli bandit problem can be modified to yield an algorithm that has $O(\log T)$ regret in a general setting with (bounded) stochastic rewards. Informally, this would require `emulating ' a Bernoulli bandit problem and assuming Beta($1,1$) priors as before.% Thus, the same approach together with the algorithm in this paper, can be combined to produce a policy with $\mathcal{O}(\log T)$ regret for general problems with (bounded) stochastic rewards.

% slight modification to Optimistic Gittins Indices gives an algorithm that has $O(\log T)$ regret for the general stochastic bandit problem with bounded rewards. Specifically, when arms have arbitrary reward distributions, bounded in the interval $[a,b]$, the approach is to each time sample the reward $X_{i,t}$, then generate an `artificial' Bernoulli reward with probability $X_{i,t}/(b-a)$ and provide that as input to the policy. The choice of arm pulls from the resulting algorithm leads to an $O(\log T)$ regret bound, as is shown in that paper. The constant in front of $\log T$, however, depends on the KL divergences of \emph{Bernoulli} random variables as opposed to the actual underlying distributions.
Yet another key observation is that the discount factor for Optimistic Gittins Indices does not need to be exactly $1-1/t$. In fact, a tuning parameter can be included to make the discount factor $\gamma_{t+\alpha} = 1-1/(t+\alpha)$ instead. Intuitively, this would encourage a greater degree of `exploration' over the arms. An inspection of the proofs of Lemmas~\ref{lemma:underestimation} and \ref{lemma:overestimation} shows that the result in Theorem~\ref{thm:frequentist_optimal_bound} would still hold were one to use such a tuning parameter. In practice, performance is remarkably robust to our choice of $K$ and $\alpha$.