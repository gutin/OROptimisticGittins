\section{Conclusions} \label{sec:conclusions}
This paper proposed a novel way for designing Bayesian Multi-Armed Bandit algorithms by treating the problem of minimizing regret as a sequence of separate Markov Decision problems where the discount factor increases from one problem to the next, according to a carefully chosen rate. We  showed that the fundamental idea of using such a heuristic results in sub-linear regret and, when applied to a binary bandit problem, that a simple and efficient algorithm with a flat Beta prior achieves the optimal rate of growth in regret.

There are some open questions following this work. First, it remains to be proven that playing arms with maximum (exact) Gittins indices together with the increasing discount factor schedule, does produce an algorithm whose regret matches the Lai-Robbins lower bound. We have a strong reason to suspect this due to the findings in our numerical experiments. Secondly, it is worth exploring whether the idea of this framework can be extended to contextual bandit problems where dependencies between arms exist. In our setting, the fact that arms were independent allowed us to exploit the Gittins index but there could be other ways to approximate optimal solutions to bandit problems with dependent arms.