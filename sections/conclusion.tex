\section{Conclusions} \label{sec:conclusions}
This paper proposed a novel way for designing Bayesian Multi-Armed Bandit algorithms by treating the problem of minimizing regret as a sequence of separate Markov Decision problems where the discount factor increases from one problem to the next, according to a carefully chosen rate. We  showed that the fundamental idea of using such a heuristic results in sub-linear regret and, when applied to a binary bandit problem, using a simple and efficient algorithm with a flat Beta prior achieves the optimal rate of growth in regret.

There are many open questions following this work. First, it remains to be proven that using Gittins Indices, or equivalently playing an arm optimally for each MDP, using the increasing discount factor technique does produce an algorithm whose regret matches the Lai-Robbins lower bound. Secondly, it is worth exploring whether the idea of this framework can be extended to contextual bandit problems or those where dependencies between arms exist. In our setting, the fact that arms were independent allowed us to exploit the Gittins Index but there could be other ways to approximate solutions to bandit problems with dependent arms.