\appendix

\section{Proof of Proposition~\ref{prop:gittins_log3T}} \label{proof:prop_log3T}
\begin{myproof}[Proof.]
	First, letting $\gamma_n = 1 - 1/n$, we show that
	\begin{equation} \label{eq:basic_finite_time_gittins}
	\Regret{\pi^{G, \gamma_n}, n} = O\left( \log^2(n) \right).
	\end{equation}
	Let $H \sim \text{Geo}(1/n)$ be an exogenous geometric random variable that is independent of $\theta$ and not observed by the agent. As an abbreviation, define $\mu^* = \E_{q}[\mu^*(\theta)]$. We then have
	\begin{align}
	\sum_{t=1}^\infty \gamma^{t-1} \E\left[X_{\pi^{G,\gamma_n},t}\right] & = \E\left[\sum_{t=1}^H X_{\pi^{G,\gamma_n}_t,t}\right] \nonumber \\
	& =  \E\left[H \mu^*(\theta) -\Regret{\pi^{G,\gamma_n}, H}\right] \\
	& =  n \mu^* - \E\left[\Regret{\pi^{G,\gamma_n}, H}\right] \nonumber \\
	& \le n \mu^* - \E\left[\Regret{\pi^{G,\gamma_n}, H} \given H > n\right]\P{H > n} \nonumber \\
	& \le n \mu^* - \E\left[\Regret{\pi^{G,\gamma_n}, n}\right](1-1/n)^n \nonumber \\
	& = n \mu^* - \E\left[\Regret{\pi^{G,\gamma_n}, n}\right](e^{-1} + o(1)) \label{bound:geometric_regret1}.
	\end{align}
	Let $q, Q$ be the density and CDF, respectively, of the prior distribution. Now, by Theorem 3, part 1 of \cite{lai1987adaptive}, there exists (an efficient) policy $\tilde \pi$, such that as $n$ becomes sufficiently large
	\begin{equation*}
	\Regret{\tilde \pi, n} \sim \left( A(A-1)  \int_\Theta q^2(\theta) Q^{A-2}(\theta) \; d\theta \right) \log^2 n.
	\end{equation*}
	Therefore for some prior-dependent constant $C_q$, we have $\Regret{\tilde \pi, n} \le C_q \log^2 n$. Let $\Delta(\theta)$ denote worst case  single period regret under parameter $\theta$, that is, $\Delta(\theta) =  \max_{i} \mu(\theta^*) - \mu(\theta_i)$. Let $\Delta$ denote its expectation over $\theta$, from which we obtain the lower bound,
	\begin{align}
	\E{\sum_{t=1}^H X_{\pi^{G,\gamma_n}_t,t}} & \ge \E\left[\sum_{t=1}^H X_{ \tilde \pi_t,t}\right] \label{bound:gittins_optimality}\\
	&  = \E\left[H\mu^*(\theta) - \Regret{\tilde \pi, H}\right] \nonumber \\
	& \ge \E\left[H\mu^*(\theta) -\Regret{\tilde \pi, H}\ind{H \ge e} - 2\ind{H < e}\Delta(\theta)\right] \nonumber \\
	& \ge n \mu^* - C_q \Ee{(\log (H))^2 \ind{H \ge 3} } - 2\Delta \nonumber \\
	& \ge n \mu^* - C_q \Ee{(\log (H))^2 \given H \ge 3}\P{H \ge 3} - 2\Delta \nonumber \\
	& \ge n \mu^* - C_q \log^2 (n+3)\P{H \ge 3} - 2\Delta\label{bound:jensens_ineq_for_log3t_proof}
	\end{align}
	where \eqref{bound:gittins_optimality} holds by optimality of the Gittins Index. The bound \eqref{bound:jensens_ineq_for_log3t_proof} follows from the memoryless property of the Geometric distribution, from Jensen's inequality and the fact that function $\log^2 x$ is a concave function on $[e,+\infty)$. Thus, equation \eqref{eq:basic_finite_time_gittins} is implied by the bounds \eqref{bound:geometric_regret1} and \eqref{bound:jensens_ineq_for_log3t_proof}.
	
	Now, for any policy $\pi$, we define $\tilde L_\pi(m) := m\mu^* - \sum_{t=1}^m X_{\pi_t,t}$ to be the random $m$ period shortfall against the expected Bayes' optimal arm and let $g_k = 1 - 1/2^{k-1}$. We break up the time horizon $T$ into geometrically growing epochs and bound, conservatively, the Bayes' risk in each one:
	\begin{align}
	\Regret{\pi^D, T} & \le \Regret{\pi^D, 2^{\ceil{\log_2 T}}}\\
	& = \sum_{k=1}^{\ceil{\log_2 T}} \Ee{\Ee{ \tilde L_{\pi^D}(2^{k-1}) \given[\Big] \mathcal F_{2^{k-1}-1}}} \nonumber \\
	& =\sum_{k=1}^{\ceil{\log_2 T}} \Ee{\Ee{ \tilde L_{\pi^{G, g_k}}(2^{k-1}) \given[\Big] \mathcal F_{2^{k-1}-1}}} \nonumber \\
	& \le \sum_{k=1}^{\ceil{\log_2 T}} \Ee{\Ee{ \tilde L_{\pi^{G,g_k}}(2^{k-1}) \given[\Big] \mathcal F_{0}}} \label{bound:super_mg} \\
	& = \sum_{k=1}^{\ceil{\log_2 T}} \Regret{\pi^{G, g_k}, 2^{k-1}} =  O\left(\sum_{k=1}^{\ceil{\log_2 T}} k^2 \right)\label{eq:order_optimal_bayes} \\
	& = O(\log^3 T) \nonumber
	\end{align}
	where \eqref{eq:order_optimal_bayes} follows from equation \eqref{eq:basic_finite_time_gittins} and \eqref{bound:super_mg} holds because regret increases if history is discarded.
\end{myproof}

\subsection{Proof of Lemma~\ref{lemma:approx_bound}} \label{prf:approx_bound}
\begin{myproof}[Proof.]
	We begin with a fundamental step. Let $\tau_1$ and $\tau_2$ be any stopping times such that $\tau_1$ precedes $\tau_2$ almost surely, that is $\tau_1 \le \tau_2$. Recall that the expected reward of the $i$th arm satisfies $\Ee{X_{i,t} \given \theta_i} = \mu(\theta_i)$ for all $t$. Let $\hat \theta_i \in \Theta$ denote a realization of the random variable $\theta_i$ and let us suppose first that $\lambda > \mu(\hat \theta_i)$. In this case, we have that
	\begin{align*}
	\Ee{\sum_{t=\tau_1}^{\tau_2-1} \gamma^{t-1} X_{i,t} + \gamma^{\tau_2-1}\frac{\lambda}{1 - \gamma} \given[\Bigg] \theta_i = \hat \theta_i} & =\mu(\hat \theta_i) \Ee{ \sum_{t=\tau_1}^{\tau_2-1} \gamma^{t-1}  \given[\Bigg] \theta_i = \hat \theta_i} + \Ee{\frac{\gamma^{\tau_2-1}}{1-\gamma}\given[\Bigg] \theta_i = \hat \theta_i}\lambda \\
	& =\mu(\hat \theta_i) \Ee{ \sum_{t=\tau_1}^{\tau_2-1} \gamma^{t-1} \given[\Bigg] \theta_i = \hat \theta_i } + \Ee{\sum_{t=\tau_2}^{\infty} \gamma^{t-1} \given[\Bigg] \theta_i = \hat \theta_i}\frac{\lambda}{1 - \gamma} \\
	& \le \Ee{\gamma^{\tau_1-1} \given \theta_i = \hat \theta_i} \frac{\lambda}{1-\gamma} \\
	& = \Ee{\gamma^{\tau_1-1} \given \theta_i = \hat \theta_i}  \frac{\max(\lambda, \mu(\hat \theta_i))}{1-\gamma}
	\end{align*}
	where the inequality is tight for $\tau_2 = \tau_1$. The same bound, for the other case when $\lambda \le \mu(\hat \theta_i)$, can be shown in a similar way. Thus we conclude, because $\hat \theta_i$ was arbitrary, that almost surely,
	\begin{equation} \label{ineq:fundamntal_bound_for_lemma_2}
	\Ee{\sum_{t=\tau_1}^{\tau_2-1} \gamma^{t-1} X_{i,t} + \gamma^{\tau_2-1}\frac{\lambda}{1 - \gamma} \given[\Bigg] \theta_i}  \le \Ee{\gamma^{\tau_1-1} \given \theta_i }  \frac{\max(\lambda, \mu(\theta_i))}{1-\gamma}.
	\end{equation}
	Let $\tau^\star$ be a stopping time that achieves the supremum in the RHS of \eqref{eqn:gittins_index} and define the stopping time $\tau^\star_K \defeq (K+1) \wedge \tau^\star$. Consider the (conditional) cumulative rewards in the Gittins problem, from time $\tau^\star_K$ onwards, given the sufficient statistic observed at time $\tau_K^\star - 1$, that is, $\E\left[\sum_{t=\tau_K^\star}^{\tau^\star}  \gamma^{t-1} X_{i,t} + \gamma^{\tau^\star-1} \lambda/(1-\gamma)
	\given[\Bigg] y_{i,\tau_K^\star-1} \right]$. We upper bound this random variable as follows. Firstly, we note that, at any time $s$ and for any statistic $\hat y \in \mathcal{Y}$, the following statement holds
	\begin{equation}\label{eqn:dist_equal_theta_i}
	\P{R(\hat y_{i,s}) \le r} = \P{\mu(\theta_i) \le r \given y_{i,s} = \hat y}, \qquad \forall r \in \Re
	\end{equation}
	meaning that the posterior distribution of the arm's expected reward $R(y_{i,s})$ is the same as $\mu(\theta_i)$ \emph{conditioned} on having observed statistic $\hat y$ about the arm. This is true by definition, but we emphasize the point here.
	
	Now, let us condition on the event $\tau^\star \ge (K+1)$. In that case we have $\tau^\star_K = K+1$ and therefore
	\begin{align} 
	&\E\left[\sum_{t=\tau_K^\star}^{\tau^\star-1} \gamma^{t-1} X_{i,t} + \gamma^{\tau^\star-1} \frac{\lambda}{1-\gamma}
	\given[\Bigg] \tau^\star \ge (K+1), \;y_{i,\tau^\star_K-1} \right] \nonumber \\
	&\qquad = \E\left[ \Ee{\sum_{t=(K+1)}^{\tau^\star-1} \gamma^{t-1} X_{i,t} + \gamma^{\tau^\star-1}\frac{\lambda}{1 - \gamma} \given[\Bigg] \theta_i} 
	\given[\Bigg]\tau^\star \ge (K+1), \; y_{i,\tau^\star_K-1} \right] \label{eqn:proof_lemma2_toer_prop} \\
	&\qquad \le \E\left[ \gamma^{\tau^\star_K - 1} \frac{\max(\mu(\theta_i), \lambda)}{1 - \gamma}\given[\Bigg] \tau^\star \ge (K+1), \; y_{i,\tau^\star_K-1}  \right]  \label{ineq:proof_lem_2_use_of_first_bound} \\
	&\qquad = \E\left[ \gamma^{\tau^\star_K - 1} \frac{\max(R(y_{i,\tau^\star_K-1}), \lambda)}{1 - \gamma}\given[\Bigg] \tau^\star \ge (K+1), \; y_{i,\tau^\star_K-1}  \right]  \label{ineq:obvious_step} \\
	&\qquad = \Ee{\frac{\gamma^{\tau^\star_K-1}R_{\lambda,K}(\tau^\star_K, y_{i,\tau^\star_K-1})}{1-\gamma} \given[\Bigg]  \tau^\star \ge (K+1), \; y_{i,\tau^\star_K-1}} \label{eqn:use_of_def_of_R}
	\end{align}
	where \eqref{eqn:proof_lemma2_toer_prop} is the tower property and \eqref{ineq:proof_lem_2_use_of_first_bound} follows from the bound in \eqref{ineq:fundamntal_bound_for_lemma_2} because $\tau^\star_K \le \tau^\star$, almost surely. Equation \eqref{ineq:obvious_step} follows from statement \eqref{eqn:dist_equal_theta_i} and that the event $\tau^\star \ge K + 1$ is $\mathcal{F}_K$-measurable (we can decide whether to pull arm $i$ or retire based on prior information). Finally equation \eqref{eqn:use_of_def_of_R} is derived by substituting in the definition of $R_{\lambda,K}$ (as given in Section~\ref{sec:gittins_and_approx}) and noting that $\tau^\star_K = K+1$.
	
	We now condition on the complement of the previous event we considered, namely, $\tau^\star < (K+1)$. Under that event, $\tau^\star$ occured early enough before time $K+1$ and thus $\tau^\star_K = \tau^\star$. Therefore, it follows that
	\begin{align} 
	&\E\left[\sum_{t=\tau_K^\star}^{\tau^\star-1}\gamma^{t-1} X_{i,t} + \gamma^{\tau^\star-1} \frac{\lambda}{1-\gamma}
	\given[\Bigg] \tau^\star < (K+1), \;y_{i,\tau^\star_K-1} \right] \nonumber  \\
	&\qquad = \E\left[\gamma^{\tau^\star-1}  \frac{\lambda}{1-\gamma}
	\given[\Bigg]\tau^\star < (K+1), \; y_{i,\tau^\star_K-1} \right] \nonumber \\
	&\qquad = \E\left[\gamma^{\tau^\star_K-1}  \frac{R_{\lambda,K}(\tau^\star_K, y_{i,\tau^\star_K-1})}{1-\gamma}
	\given[\Bigg]\tau^\star < (K+1), \; y_{i,\tau^\star_K-1} \right] \label{eqn:proof_lem_2_second_use_of_R_def}
	\end{align}
	where \eqref{eqn:proof_lem_2_second_use_of_R_def} is obtained by using the definition of $R_{\lambda,K}$ and the fact that we conditioned on $\tau^\star < (K+1)$. Thus, by the law of total expectation and \eqref{eqn:use_of_def_of_R}, \eqref{eqn:proof_lem_2_second_use_of_R_def}, we establish that
	\begin{equation} \label{ineq:proof_lem_2_main_bound_in_proof}
	\E\left[\sum_{t=\tau_K^\star}^{\tau^\star-1}\gamma^{t-1} X_{i,t} + \gamma^{\tau^\star-1} \frac{\lambda}{1-\gamma}
	\given[\Bigg] \;y_{i,\tau^\star_K-1} \right] \le \Ee{\gamma^{\tau^\star_K-1}  \frac{R_{\lambda,K}(\tau^\star_K, y_{i,\tau^\star_K-1})}{1-\gamma} \given[\Bigg] y_{i,\tau^\star_K-1}}.
	\end{equation}
	We are ready to complete our main argument in this proof by using the above bound and `breaking up' the RHS of \eqref{eqn:gittins_index} into rewards from times before $\tau_K^\star$ and after (and bounding the latter terms). More precisely, we obtain that
	\begin{align}
	\E_{y}\left[\sum_{t=1}^{\tau^\star-1}\gamma^{t-1} X_{i,t} + \gamma^{\tau^\star-1}\frac{\lambda}{1-\gamma}\right] & = \E_{y}\left[\sum_{t=1}^{\tau^\star_K-1}\gamma^{t-1} X_{i,t} + \sum_{t'=\tau^\star_K}^{\tau^\star-1}\gamma^{t'-1} X_{i,t'} +  \gamma^{\tau^\star-1}\frac{\lambda}{1-\gamma}\right] \nonumber \\
	& = \E_{y}\left[\sum_{t=1}^{\tau^\star_K-1}\gamma^{t-1} X_{i,t} + \Ee{\sum_{t'=\tau^\star_K}^{\tau^\star-1}\gamma^{t'-1} X_{i,t'} +  \gamma^{\tau^\star-1}\frac{\lambda}{1-\gamma} \given[\Bigg] y_{i,\tau^\star_K-1} }\right] \label{eqn:proof_lem_2_tower_prop_again} \\
	& \le  \E_{y}\left[\sum_{t=1}^{\tau^\star_K-1}\gamma^{t-1} X_{i,t} + \Ee{\gamma^{\tau^\star_K-1}  \frac{R_{\lambda,K}(\tau^\star_K, y_{i,\tau^\star_K-1})}{1-\gamma} \given[\Bigg] y_{i,\tau^\star_K-1}}\right] \label{eqn:proof_lem2_using_the_main_argument} \\
	& =  \E_{y}\left[\sum_{t=1}^{\tau^\star_K-1}\gamma^{t-1} X_{i,t} +  \gamma^{\tau^\star_K-1}  \frac{R_{\lambda,K}(\tau^\star_K, y_{i,\tau^\star_K-1})}{1-\gamma}\right] \label{eqn:proof_lem_2_tower_prop_yet_again} \\
	& \le \sup_{1 < \tau \le (K+1)}  \E_{y}\left[\sum_{t=1}^{\tau-1}\gamma^{t-1} X_{i,t} +  \gamma^{\tau-1}  \frac{R_{\lambda,K}(\tau, y_{i,\tau-1})}{1-\gamma}\right] \nonumber
	\end{align}
	where Equations \eqref{eqn:proof_lem_2_tower_prop_again}, \eqref{eqn:proof_lem_2_tower_prop_yet_again} use the tower property and \eqref{eqn:proof_lem2_using_the_main_argument} is gotten immediately by using the bound of \eqref{ineq:proof_lem_2_main_bound_in_proof}. 
	
	Let us now use the shorthand $V^K_{\gamma}( y, \lambda) \defeq \sup_{1 < \tau \le (K+1)} \E_{y}\left[\sum_{t=1}^{\tau-1}\gamma^{t-1} X_{i,t} + \gamma^{\tau-1} \frac{R_{\lambda,K}(\tau, y_{\tau-1})}{1-\gamma}\right]$ and $V_{\gamma}(y, \lambda) \defeq \sup_{\tau > 1} \E_{y}\left[\sum_{t=1}^{\tau-1}\gamma^{t-1} X_{i,t} +  \gamma^{\tau-1} \frac{\lambda}{1-\gamma}\right]$. We have just shown that $V^K_{\gamma}( y, \lambda) \ge V_{\gamma}(y, \lambda)$ for all $K \ge 1$. 
	
	To finish the proof let us assume that $v^K_\gamma(y) < v_\gamma(y)$ in order to get a contradiction. As shown in \cite{gittins2011multi}, Chapter 2, the function $V_\gamma(y, \lambda) - \lambda/(1-\gamma)$ is convex and decreasing in $\lambda$, therefore it has a single root, which we know is $v_\gamma(y)$. Because $v^K_\gamma < v_\gamma(y)$, by our current assumption, we have that 
	\begin{equation}
		V_\gamma(y, v^K_\gamma(y)) > v^K_\gamma(y)/(1-\gamma) = V^K_\gamma(y, v^K_\gamma(y)),
	\end{equation}
	which contradicts what we just showed, that $V^K_{\gamma}( y, \lambda) \ge V_{\gamma}(y, \lambda)$ for all $\lambda$.
\end{myproof}
\subsection{Results for the frequentist regret bound proof}
\subsubsection{Definitions and properties of Binomial distributions.}
We list notation and facts related to Beta and Binomial distributions, which are used through this section.
\begin{definition}
	$F^B_{n,p}(.)$ is the CDF of the Binomial distribution with parameters $n$ and $p$, and $F^\beta_{a,b}(.)$ is the CDF of the Beta distribution with parameters $a$ and $b$.
\end{definition}

\begin{lemma} \label{fact:equation_for_beta_binomial_cdfs}
	Let $a$ and $b$ be positive integers and $y \in [0,1]$, 
	\[
	F^\beta_{a,b}(y) = 1 - F^B_{a+b-1,y}(a-1)
	\]
\end{lemma}
\begin{myproof}[Proof.]
	Proof is found in \cite{agrawalanalysis}.
\end{myproof}
\begin{lemma} \label{fact:median_of_binomial_dist}
	The median of a Binomial$(n,p)$ distribution is either $\ceil{np}$ or $\floor{np}$.
\end{lemma}
\begin{myproof}[Proof]
	Proof is found in \cite{jogdeo1968monotone}.
\end{myproof}

\begin{corollary}[Corollary of Fact~\ref{fact:median_of_binomial_dist}] \label{cor:corollarly_of_binomial_median_property}
	Let $n$ be a positive integer and $p \in (0,1)$. For any non-negative integer $s < np$
	\[
	F_{n,p}(s) \le 1/2
	\]
\end{corollary}

\begin{lemma} \label{fact:relationship_with_binom_cdfs}
	Let $n$ be a positive integer and $p \in [0,1]$. Then for any $k \in \{0,\ldots,n\}$,
	\[
	(1-p)F_{n-1,p}(k)\le F_{n,p}(k) \le F^B_{n-1,p}(k)
	\] 
\end{lemma}
\begin{myproof}[Proof]
	To prove $F_{n,p}(k) \le F^B_{n-1,p}(k)$, we let $X_1,\ldots,X_{n}$ be i.i.d samples from a Bernoulli($p$) distribution. We then have
	\begin{align*}
	F^B_{n,p}(k)  = \P{\sum_{i=1}^{n} X_i \le k}  \le  \P{\sum_{i=1}^{n-1} X_i \le k}  = F^B_{n-1,p}(k)
	\end{align*}
	Now to prove $(1-p)F_{n-1,p}(k)\le F_{n,p}(k)$, it's enough to observe that $F_{n,p}(k) = p F_{n-1,p}(k-1) + (1-p) F_{n-1,p}(k)$.
\end{myproof}

\subsubsection{Ratio of Binomial CDFs.} \label{sec:ratio_of_bin_cdfs}
\begin{lemma} \label{lemma:ratio_of_cdfs}
	Let $0< q < p < 1$. Let $n$ be a positive integer such that $e^{\frac{n}{2} d(q,p)} \ge (n+1)^4$ and let $k$ be a non-negative integer such that $k < nq$. It then follows that
	\[
	F^B_{n,q}(k)/F^B_{n,p}(k) >  e^{\frac{n}{2} d(q,p)}.
	\]
\end{lemma}
\begin{proof}[Proof.]
	From the method of types  (see \cite{cover2012elements}), we have for any $r \in (0,1)$ and $j < nr$
	\begin{equation} \label{eqn:appl_of_sanovs}
	\frac{e^{-nd(j/n, r)}}{(1+n)^2}\le F_{n,r}(j) \le (n+1)^2 e^{- n d(j/n, r)}.
	\end{equation}
	Because $k < nq < np$, by applying \eqref{eqn:appl_of_sanovs} to both the numerator and denominator, we get
	\begin{align*}
	\frac{F_{n,q}(k)}{F_{n,p}(k)} & \ge  \frac{e^{-nd(k/n, q)}}{(n+1)^4 e^{- n d(k/n, p)}} = \frac{e^{n(d(k/n,p) - d(k/n,q))}}{(n+1)^4}.
	\end{align*}
	Examining the exponent, we find
	\begin{align*}
	d(k/n, p) - d(k/n,q) & = \frac{k}{n} \log \frac{q}{p} + \left(1-\frac{k}{n}\right)\log \frac{1-q}{1-p} \\
	& > q \log \frac{q}{p} + (1-q)\log \frac{1-q}{1-p} \\
	& = d(q,p)
	\end{align*}
	where the bound holds because the expression is decreasing in $k$, and $k < nq$. Therefore,
	\begin{align}
	\frac{F_{n,q}(k)}{F_{n,p}(k)} & > \frac{e^{n  d(q,p)}}{(n+1)^4} = \frac{e^{\frac{n}{2}d(q,p)}}{(n+1)^4} e^{\frac{n}{2}d(q,p)} \ge e^{\frac{n}{2}d(q,p)} \label{bound:log_1minusq_etc}.
	\end{align}
	The final lower bound in \eqref{bound:log_1minusq_etc} follows from the assumption on $n$.
\end{proof}

\section{Optimistic Gittins Index results.} \label{sec:amgi_results}
For this section it is useful to define the value of a stopping problem used in the calculation of the Optimistic Gittins Index. For any fixed arm $i$, we write
\begin{align*}
V_K(y; x, \gamma) & := \sup_{1 < \tau \le K} \Ee{\sum_{t=1}^{\tau-1} (1-\gamma)X_{i,t} + \gamma^{\tau-1} R_{x, K}(\tau, y_{i,\tau})	\given y_{i,1} = y}.
\end{align*}
Therefore the Optimistic Gittins Index, in state $y$ with discount factor $\gamma$, is the solution in $x$ to $V_K(y; x, \gamma) = x$. We show some key properties of $V_K$, which we exploit later on. For fixed $y$, $K$ and $\gamma$, we will call $V(x) \defeq V(y; x, \gamma)$.
\begin{fact}
	$V(x)$ is convex and differentiable.
\end{fact}
\begin{myproof}[Proof.]
	We prove this by induction. For $K = 1$, we have $\tau = 2$ almost surely and so
	\begin{align*}
	V(x) & = V_1(y; x, \gamma) \\
	& = (1-\gamma_t)\Ee{X_{i,1} \given y_{i,1 = y}} + \gamma_t \Ee{R_{x, 1}(2, y_{i,1}) \given y_{i,1} = y} \\
	& = (1-\gamma_t)\Ee{X_{i,1} \given y_{i,1 = y}} + \gamma_t \Ee{\max(x, \E{X_{i,1}}) \given y_{i,1} = y}
	\end{align*}
	The function is convex because for any random variable $Z$ the term $\max(g, Z)$ is convex and taking expectations preserves convexity. Also, we verify through the bounded convergence theorem that $V(x)$ is differentiable and the event $\{\E X_{i,1}=z \given y_{i,1} = y\}$, at which $V$ is not differentiable, has measure zero.
	
	For $K > 1$, assume that $V_{K-1}$ is convex and differentiable. By writing the Bellman equation
	\begin{align*}
		V_K(y; x, \gamma) & = (1-\gamma_t)\Ee{X_{i,1} \given y_{i,1}=y} + \Ee{\gamma_t \max(x, V_{K-1}(y_{i,t+1}; x, \gamma)) \given y_{i,1} = y}
	\end{align*}
	we again notice a maximum of convex functions in $x$. This form for $V_K$ implies that it is convex and differentiable.
\end{myproof}
\begin{lemma} \label{eq:important_fact}
	Let $\gamma \in (0,1)$ and
	\begin{equation} \label{eq:def_lambda_in_important_equiv}
	\lambda = \sup\{ x \in [0,1] : V(x) \ge x\}
	\end{equation} 
	For all $x \in (0,1)$, the following equivalence holds
	\begin{equation} \label{eq:equivlance_lambda1}
	\lambda < x  \Longleftrightarrow V(x) < x.
	\end{equation}
\end{lemma}
\begin{myproof}[Proof.]
	Figure~\ref{fig:visaulize_gx_proof} gives a visualization of the proof. In the plot, the point $x^*$, where $V(x^*)$ and $x^*$ intersect, is the Optimistic Gittins Index. Notice how if $x > x^*$, $V(x)$ is below $x$ and otherwise $V(x)$ is above $x$; this is the crux of the proof.
	
	We also give a formal proof. Firstly let's assume that $\lambda < x$. If $x\le V(x)$, then $\lambda$ would not be the supremum over all $z \in [0,1]$ such that $ z \le V(z)$. Therefore $V(x) < x$.
	
	\begin{figure}
		\centering
		\input{plots/visualize_gx.pgf}
		\caption{Visualization of Lemma~\ref{eq:important_fact}'s proof for a Beta-Bernoulli problem with parameters $(3,7)$ and $\gamma=0.7$. The intersection of the two lines represents the Optimistic Gittins Index.}
		\label{fig:visaulize_gx_proof}
	\end{figure}
	
	For the converse, assume $\lambda \ge x$. Recall that $V(z)$ is convex. Since $V$ is continuous on $[0,1]$, by the Intermediate Value Theorem, there exists a point $\lambda$ at which $\lambda =V(\lambda)$. Therefore let $\eps < (1-\lambda)/2$ and from the first direction of the proof, we have $\lambda + \eps > V(\lambda + \eps)$. Thus
	\[
	V(\lambda + \eps) \ge V(\lambda) + \eps V'(\lambda) = \lambda + \eps V'(\lambda)
	\]
	where the inequality follows from $V$ being convex and differentiable. This implies that $V'(\lambda) < 1$ and, moreover, because $V$ is also increasing, it follows that $V'(\lambda) \in (0,1)$, whence
	\begin{align*}
	V(x) & \ge V(\lambda) - (\lambda - x) V'(\lambda) \\
	& = \lambda - (\lambda - x) V'(\lambda) \\
	& = (1-V'(\lambda)) \lambda + V'(\lambda) x \\
	& \ge \min(x,\lambda) = x.
	\end{align*}
	This completes the proof.
\end{myproof}
\begin{corollary} \label{cor:equivalent_event}
	Let $v_{i,t}$ be the Optimistic Gittins Index of arm $i$ at time $t$ and let $x \in (0,1)$. The following equivalence holds
	\[
	\{v^K_{i,t} < x \} = \{V_K(y_{i,t}; x, \gamma_t) < x\}\]
	where $y_{i,t}$ is the sufficient statistic for estimating the $i$th arm's parameter $\theta_i$.
\end{corollary}
\begin{myproof}[Proof.]
	By the definition in Equation~\eqref{eqn:ogi_k1}, $v^K_{i,t}$ can be characterized with the relation 
	\begin{equation*}% \label{eq:form4}
	v_{i,t} = \sup\left\{ x \in [0,1] : x \le V_K(y_{i,t}; x, \gamma_t)  \right\}.
	\end{equation*}
	The conclusion then follows from Lemma~\ref{eq:important_fact}.
\end{myproof}
The next Lemma will be the final property for the function $V_K$ that we prove. This will subsequently be used in the proof of Lemma~\ref{lemma:underestimation}.
\begin{lemma} \label{lemma:vk_bound}
	Let $i$ be any arm. For any state $y$, look-ahead parameter $K \in \mathbb{Z}_+$ and $\gamma, \eta \in [0,1]$, we have
	\begin{equation*}
		\Ee{V_K(y'; \gamma, \eta) \given y} \ge V_K(y; \gamma, \eta)
	\end{equation*}
	where $y'$ is the next state (posterior) for that arm.
\end{lemma}
\begin{myproof}[Proof.]
	Let $\tau^\star(y)$ be the optimal stopping time for an initial state $y$ in the problem for computing $V_K$. Then
	\begin{align}
		\Ee{V_K(y'; \gamma, \eta) \given y} & = \Ee{\Ee{\sum_{s=1}^{\tau^\star(y')-1} \gamma^{s-1} X_{i,s} + \frac{\gamma^{\tau^\star(y')-1} R_{\eta, K}(\tau, y_{i,\tau^\star(y')})}{1-\gamma} \given[\Bigg] y_{i,1} = y'} \given[\Bigg] y} \nonumber \\
		& \ge  \Ee{\Ee{\sum_{s=1}^{\tau^\star(y)-1} \gamma^{s-1} X_{i,s} + \frac{\gamma^{\tau^\star(y)-1} R_{\eta, K}(\tau, y_{i,\tau^\star(y)})}{1-\gamma} \given[\Bigg] y_{i,1} = y'} \given[\Bigg] y} \label{ineq:subopt_of_y}\\
		& = \Ee{\sum_{s=1}^{\tau^\star(y)-1} \gamma^{s-1} X_{i,s} + \frac{\gamma^{\tau^\star(y)-1} R_{\eta, K}(\tau, y_{i,\tau^\star(y)})}{1-\gamma} \given[\Bigg] y_{i,1} = y} \label{eq:tower_prop} \\
		& = V_K(y; \gamma, \eta). \nonumber
	\end{align}
	The bound \eqref{ineq:subopt_of_y} is due to the sub-optimality of $\tau^\star(y)$ when the actual starting state is $y'$ and equation \eqref{eq:tower_prop} is the tower property.
\end{myproof}
\subsection{Proof of Lemma~\ref{lemma:underestimation}} \label{proof:underestimation_proof}
\begin{myproof}[Proof.]
	This proof involves induction. We state what the base case is, in the following Lemma, but we defer its proof to the end of the section.
	\begin{lemma} \label{lemma:underestimation_base_case}
		Suppose the OGI algorithm uses only one look-ahead step, and so $K = 1$. Then we have
		\[
		\P{v^1_{1,t} < \eta} = \mathcal{O}\left(\frac{1}{t^{1 + h(\eta)}}\right)
		\]
		where $h$ is a positive yet decreasing function of $\eta$.
	\end{lemma}
	Now we show the induction step. Suppose that for $K \ge 2$, it holds that
	\[
	\P{v^{K-1}_{1,t} < \eta} = \mathcal{O}\left(\frac{1}{t^{1 + h(\eta)}}\right).
	\]
	We show the same is true for $v^{K}_{1,t}$. Indeed, we have for $t > 1$
	\begin{align}
	\P{v^K_{1,t} < \eta} & = \P{V_K(y_{1,t}; \eta, \gamma_t) < \eta} \label{eq:appl_of_lemma_9}\\
	& = \P{(1-\gamma_t)\Ee{X_{1,t} \given y_{1,t}} + \gamma_t \Ee{\max(\eta, V_{K-1}(y_{1,t+1}; \eta, \gamma_{t})) \given y_{1,t}}< \eta} \nonumber \\
	& \le \P{\frac{1}{t}\Ee{X_{1,t} \given y_{1,t}} + \gamma_t \max(\eta, \Ee{V_{K-1}(y_{1,t+1}; \eta, \gamma_{t}) \given y_{1,t}})< \eta} \label{ineq:appl_jensens} \\
	& \le \P{\frac{1}{t}\Ee{X_{1,t} \given y_{1,t}} + \left(1-\frac{1}{t}\right) \Ee{V_{K-1}(y_{1,t+1}; \eta, \gamma_{t}) \given y_{1,t} }< \eta} \nonumber \\
	& \le \P{\frac{1}{t}\Ee{X_{1,t} \given y_{1,t}} + \left(1-\frac{1}{t}\right) V_{K-1}(y_{1,t}; \eta, \gamma_{t})< \eta} \label{ineq:missing_step} \\
	& \le \P{ V_{K-1}(y_{1,t}; \eta, \gamma_{t})< \eta \left(\frac{t}{t-1}\right)} \nonumber \\
	& \le \mathcal{O}\left(\frac{1}{t^{1 + h(\eta t/(t-1))}}\right) =  \mathcal{O}\left(\frac{1}{t^{1 + h(\eta)}}\right) \nonumber
	\end{align}
	where \eqref{eq:appl_of_lemma_9} follows from Lemma~\ref{cor:equivalent_event} and \eqref{ineq:appl_jensens} uses Jensen's inequality. Lemma~\ref{lemma:vk_bound} give us \eqref{ineq:missing_step}.
	
	We finish the proof by using the following asymptotic argument. Take $M$ to be a large enough integer, then we have, using the result of the induction proof,
	\begin{align*}
	\sum_{t=1}^\infty \P{v^K_{1,t} < \eta} & \le M +  \sum_{t=M+1}^\infty \frac{C_1}{t^{1 + h(\eta)}} \le M +  C_2
	\end{align*}
	where $C_2 = C_2(\eta)$ is the limit of the series and $C_1$ is a constant used in the definition of the big-Oh.
\end{myproof}
\subsubsection{Proof of the base case in Lemma~\ref{lemma:underestimation_base_case}}
\begin{myproof}[Proof.]
	For simplicity let's abbreviate $v^1_{1,t}$ as $v_{1,t}$. Define $\delta := (\theta_1 - \eta)/2$ and  $\eta' :=  \eta + \delta$. In other words, $\delta$ is half the distance between $\eta$ and $\theta_1$; $\eta'$ is the point half-way.
	
	The proof consists of showing two claims
	\subsubsection*{Claim 1: $\{v_{1,t} < \eta\} \subseteq \left\{F^B_{N_1(t)+1, \eta'}(S_1(t)) < \frac{1}{\delta t}\right\}$:}
	Let $V_t \sim $Beta$(S_1(t)+1,N_1(t) - S_1(t) + 1)$ be the agent's posterior on the optimal arm. Using Corollary~\ref{cor:equivalent_event} and the simplified form for $K=1$ \[V_K((S_1(t)+1, N_1(t) - S_1(t) + 1); \eta, \gamma_t) = \E{V_t} + \gamma_t\Ee{(\eta - V_t)^+}\] we find that
	\begin{align}
	\left\{v_{1,t} < \eta \right\} & = \left\{ \Ee{V_t } + \gamma_t\Ee{(\eta - V_t)^+} < \eta \right\} \nonumber\\
	& =  \left\{ (1-1/t)\Ee{(\eta - V_t)^+} < \Ee{\eta - V_t} \right\} \label{eq:def_gamma_t} \\
	& =  \left\{ \Ee{(\eta - V_t)^+} - \Ee{\eta - V_t} <  \frac{1}{t}\Ee{(\eta - V_t)^+}\right\} \nonumber\\
	& =  \left\{ \Ee{(V_t - \eta)^+}<  \frac{1}{t}\Ee{(\eta - V_t)^+}\right\} \nonumber\\
	& \subseteq \left\{ \Ee{ (V_t - \eta)^+}< \frac{1}{t}  \right\}  \label{eq:intermediate_event}
	\end{align}
	where \eqref{eq:def_gamma_t} follows from the definition of $\gamma_t$ and \eqref{eq:intermediate_event} is due to $V_t, \eta$ both lying in the interval $[0,1]$. We approximate the conditional expectation in \eqref{eq:intermediate_event} with
	\begin{align}
	\Ee{(V_t - \eta)^+ \given S_1(t), N_1(t)} & = \Ee{(V_t - \eta) \ind{V_t \ge \eta} }\nonumber \\
	& = \Ee{(V_t - \eta) \ind{\eta + \delta > V_t \ge \eta} }  \nonumber \\
	& \qquad + \Ee{(V_t - \eta) \ind{ V_t \ge \eta + \delta} } \nonumber \\
	& > \Ee{(V_t - \eta) \ind{ V_t \ge \eta + \delta} } \nonumber \\
	& \ge \delta\P{ V_t \ge \eta' } \nonumber \\
	& = \delta (1 - F_{S_1(t)+1,N_1(t)-S_1(t)+1}(\eta')) = \delta F^B_{N_1(t)+1,\eta'}(S_1(t)) \label{ineq:lower_bound_on_ppart_term}
	\end{align}
	The last equality is due to Fact~\ref{fact:equation_for_beta_binomial_cdfs} and this proves the claim.
	\subsubsection*{Claim 2: $ \sum_{t=1}^\infty \mathbb{P}\left(F^B_{N_1(t)+1, \eta'}(S_1(t)) < \frac{1}{\delta t}\right) \le C_1$ where $C_1$ is a constant:}
	Let us fix the sequence $f_t = -\frac{\log \delta t }{\log (1-\eta')}-1 = O(\log t)$. We then have
	\begin{align}
	\P{F^B_{N_1(t)+1, \eta'}(S_1(t)) < \frac{1}{\delta t}} & = \P{F^B_{N_1(t)+1, \eta'}(S_1(t)) < \frac{1}{\delta t}, \; N_1(t) > f_t}  \nonumber \\
	& \qquad + \P{F^B_{N_1(t)+1, \eta'}(S_1(t)) < \frac{1}{\delta t}, \; N_1(t) \le f_t} \label{eq:decomp2}.
	\end{align}
	For the second term in the RHS of \eqref{eq:decomp2} we have the following bound,
	\begin{align}
	\P{F^B_{N_1(t)+1, \eta'}(S_1(t)) < \frac{1}{\delta t}, \; N_1(t) \le f_t}  &  \le \P{F^B_{N_1(t)+1,\eta'}(0) < \frac{1}{\delta  t}, \; N_1(t) \le f_t} \nonumber \\
	& = \P{(1-\eta')^{N_1(t)+1} <  \frac{1}{\delta  t}, \; N_1(t) \le f_t} \nonumber \\
	& \le \P{(1-\eta')^{f_t+1} <  \frac{1}{\delta  t}} = 0.\label{bound:bdd_by_zero}
	\end{align}
	Now we use the following fact to bound the left term on the RHS of \eqref{eq:decomp2}. Define the function
	\[
	F^{-B}_{n,p}(u) := \inf\{x : F^B_{n,p}(x) \ge u\}
	\]
	which is the inverse CDF. Then it is known that if $U \sim \text{Unif}(0,1)$, then $F^{-B}_{n,p}(U) \sim \text{Binomial}(n,p)$. Furthermore, $F^B_{n,p}(F^{-B}_{n,p}(U)) \ge U$ due to the definition of the inverse CDF.
	
	Now let us only consider large $t$, in particular $t > M = M(\theta_1, \eta')$ where:
	\begin{enumerate}
		\item $M$ is such that $e^{d(\eta', \theta_1)f_{M}/2} > (f_M + 1)^4$
		\item $M > \frac{4}{(1-\eta')\delta }$
		\item $\ceil{f_M} > 0$ and $F^B_{\ceil{f_M},\eta'}(f_M \eta') > 1/4$. Note that there is a large enough integer for this because $F^B_{\ceil{f_t},\eta'}(f_t \eta') \to \frac{1}{2}$ as $t \to \infty$.
	\end{enumerate} 
	Suppose that $t > M$. It then follows that the event $\{F^B_{N_1(t), \eta'}(S_1(t)) < \frac{1}{(1-\eta')\delta t},\; S_1(t) \ge N_1(t) \eta', \; N_1(t) > f_t\}$ has measure zero because of the assumptions made on $M$. Therefore if $t > M$, we have
	\begin{align}
	\mathbb{P}\bigg(F^B_{N_1(t)+1,  \eta'}(S_1(t)) &< \frac{1}{\delta t }  , \; N_1(t) > f_t \bigg) \nonumber \\
	& \le \P{F^B_{N_1(t),  \eta'}(S_1(t)) < \frac{1}{(1-\eta')\delta t}, \; N_1(t) > f_t} \label{eqn:part1_decomp_the_cdf_of_y} \\ 
	& = \P{F^B_{N_1(t),  \eta'}(S_1(t)) < \frac{1}{(1-\eta')\delta t}, \; S_1(t) < N_1(t) \eta', \; N_1(t) > f_t} \nonumber \\ 
	& =  \P{F^B_{N_1(t),\theta_1}(S_1(t)) \frac{F^B_{N_1(t),\eta'}(S_1(t))}{F^B_{N_1(t),\theta_1}(S_1(t))} < \frac{1}{(1-\eta')\delta  t}, \;S_1(t) < N_1(t) \eta', \; N_1(t) > f_t} \nonumber \\
	& \le  \P{F_{N_1(t),\theta_1}^B(S_1(t))  e^{N_1(t) D} < \frac{1}{(1-\eta')\delta  t} , \; N_1(t) > f_t} \label{eqn:part1_app_of_lemma2} \\
	& \le  \P{F_{N_1(t),\theta_1}^B(S_1(t)) e^{f_t D} < \frac{1}{(1-\eta')\delta  t}} \nonumber \\
	& =  \P{F_{N_1(t),\theta_1}^B(F^{-B}_{N_1(t),\theta_1}(U)) < \frac{e^{-f_t D}}{(1-\eta')\delta  t} } \label{eqn:part1_propert_of_inverse_sampling}\\
	& \le  \P{U < \frac{e^{-f_t D}}{(1-\eta')\delta  t} } \nonumber \\  
	& =  \frac{e^{-f_t D}}{(1-\eta')\delta  t} \nonumber  \nonumber\\
	& = O\left( \frac{1}{t^{1+Dc_{\eta'}}} \right)  \label{bound:one_over_t_plus_eps} 
	\end{align}
	where $D = d(\eta',\theta_1) > 0$ and $c_{\eta'} = -\log^{-1}(1-\eta') > 0$ are constant. The bound \eqref{eqn:part1_decomp_the_cdf_of_y} holds due to Fact~\eqref{fact:relationship_with_binom_cdfs}. Bound \eqref{eqn:part1_app_of_lemma2} follows from an application of Lemma~\ref{lemma:ratio_of_cdfs} and the fact that $t > M$. Equation \eqref{eqn:part1_propert_of_inverse_sampling} follows from $S_1(t) \sim \text{Binomial}(N_1(t), \theta_1)$ and the inverse sampling technique. By combining bounds \eqref{bound:one_over_t_plus_eps}, \eqref{bound:bdd_by_zero} and \eqref{eq:decomp2}, we get the big-Oh bound.
\end{myproof}

\subsection{Proof of Lemma~\ref{lemma:overestimation}} \label{proof:overestimation_proof}

\begin{proof}[Proof.]
	See the main proof of Theorem~\ref{thm:frequentist_optimal_bound} to recall the definition of constants $\eta_1$, $\eta_3$ and their relationship with $\theta_2$ and $\theta_1$. As an abbreviation we let $L = L(T)$. Moreover, because for any arm $i$ $v^K_{i,t} \le v^{K-1}_{i,t} \le \ldots \le v^1_{i,t}$, it will be sufficient to consider this proof only for $v^1_{2,t}$, which we also will abbreviate as $v_{2,t} \defeq v^1_{2,t}$.
	
	Firstly, by the law of total probability, we find that
	\begin{align} 
	\sum_{t=1}^T \mathbb{P}(v_{2,t} & \ge \eta_3 ,\; N_2(t) \ge L,\; \pi^{\rm OG}_t = 2) \nonumber \\
	& = \sum_{t=1}^T \P{v_{2,t} \ge \eta_3 ,\; N_2(t) \ge L, \; S_2(t) < \floor{N_2(t) \eta_1}, \; \pi^{\rm OG}_t = 2} \nonumber \\
	& \qquad + \sum_{t=1}^T \P{v_{2,t} \ge \eta_3 ,\; N_2(t) \ge L, \; S_2(t) \ge \floor{N_2(t) \eta_1},\; \pi^{\rm OG}_t = 2} \nonumber \\
	& \le \sum_{t=1}^T \P{v_{2,t} \ge \eta_3 ,\; N_2(t) \ge L, \; S_2(t) < \floor{N_2(t) \eta_1}} + \sum_{t=1}^T \P{\pi^{\rm OG}_t = 2,\; S_2(t) \ge \floor{N_2(t) \eta_1}} \label{eqn:splitting_not_underestimate}
	\end{align}
	Let $V_t \sim \text{Beta}(S_2(t) + 1, N_2(t)- S_2(t) + 1)$ denote the agent's posterior on the second arm at time $t$, then
	\begin{align}
	\sum_{t=1}^T \mathbb{P}(v_{2,t} \ge \eta_3 ,\; & \; N_2(t) \ge L,\; S_2(t) < \floor{N_2(t) \eta_1})  \nonumber\\
	& = \sum_{t=1}^T \P{\Ee{V_t} + \gamma_t \Ee{(\eta_3 - V_t)^+} \ge \eta_3, \; N_2(t) \ge L,\; S_2(t) < \floor{N_2(t) \eta_1}} \nonumber \\
	& = \sum_{t=1}^T \P{\frac{\Ee{(\eta_3-V_t)^+ }}{  \Ee{(V_t - \eta_3)^+ }} \le t , \; N_2(t) \ge L,\; S_2(t) < \floor{N_2(t) \eta_1}} \label{eq:complicated_rv_in_part2}
	\end{align}
	where the second equality follows from Corollary~\ref{cor:equivalent_event} in Appendix~\ref{sec:amgi_results}. The following result lets us bound \eqref{eq:complicated_rv_in_part2},
	\begin{lemma} \label{lem:lb_rv2}
		Let $0 < x < y < 1$. For any non-negative integers $s$ and $k$ with $s < \floor{kx}$, it holds that
		\begin{equation*}
		\frac{\Ee{(y-V)^+ }}{  \Ee{(V - y)^+ } } \ge \frac{(y-x) \exp(k d(x,y))}{2}
		\end{equation*}
		where $V \sim \text{Beta}(s+1,k-s+1)$.
	\end{lemma}
	\begin{myproof}[Proof.]
		See Appendix~\ref{prf:proof_of_lb_rv2}.
	\end{myproof}
	Therefore, from equation \eqref{eq:complicated_rv_in_part2} and Lemma~\ref{lem:lb_rv2}, we find that whenever $T > \left(\frac{2}{\eta_3-\eta_1}\right)^{1/\eps} =: T^*(\eps, \theta)$,
	\begin{align}
	\sum_{t=1}^T \mathbb{P}(v_{2,t} \ge \eta_3 ,\; & \; N_2(t) \ge L,\; S_2(t) < \floor{N_2(t) \eta_1}) \nonumber \\
	& \le  \sum_{t=1}^T\P{  (\eta_3-\eta_1) \exp\{N_2(t) d(\eta_1,\eta_3) \} \le 2t,\; N_2(t) \ge L} \nonumber \\
	& \le  \sum_{t=1}^T\P{  (\eta_3-\eta_1) \exp\{L d(\eta_1,\eta_3) \} \le 2t} \nonumber \\
	& =   \sum_{t=1}^T\P{  (\eta_3-\eta_1) T^{1+\eps} \le 2t} = 0 \label{bound:equal_to_zero}
	\end{align}
	All that is left is to bound the second term in \eqref{eqn:splitting_not_underestimate}, and to do so we apply the following Lemma whose proof is in Appendix~\ref{prf:proof_of_acc_sub_means}
	\begin{lemma} \label{lem:accurate_suboptimal_mean}
		There exist positive constants $C = C(\theta_2,\eta_1)$ and $x' > \theta_2$ such that
		\begin{equation*}
		\sum_{t=1}^T \P{S_2(t) \ge \floor{N_2(t) \eta_1}, \; \pi^{\rm OG}_t = 2} \le  K + \frac{1}{1 - e^{-d(x',\theta_2)}} 
		\end{equation*}
	\end{lemma}
	Combining Lemma~\ref{lem:accurate_suboptimal_mean}, \eqref{bound:equal_to_zero}, \eqref{eqn:splitting_not_underestimate} and \eqref{eq:complicated_rv_in_part2} shows the claim.
\end{proof}

\subsubsection{Proof of Lemma~\ref{lem:lb_rv2}.} \label{prf:proof_of_lb_rv2}
\begin{myproof}[Proof.]
	We upper bound the denominator as follows. Given that $s < \floor{k x}$, we have $s \le kx - 1$. Let $B(a,b)$ denote the Beta function, then
	\begin{align}
	\Ee{(V - y)^+ } & = \frac{1}{B(s+1,k-s+1)}\int_{y}^1 (t-y) t^s (1-t)^{k-s} \; dt \nonumber \\
	& = \frac{1}{B(s+1,k-s+1)}\int_{y}^1 t^{s+1} (1-t)^{k-s}  dt - y \P{V \ge y} \nonumber \\
	& = \frac{B(s+2,k-s+1)}{B(s+1,j-s+1)}\left( \frac{1}{B(s+2,k-s+1)} \right)\int_{y}^1 t^{s+1} (1-t)^{k-s}  dt - y \P{V \ge y} \nonumber \\
	& = \frac{s+1}{k+2} F^B_{k+2,y}(s+1)  - y \P{V \ge y} \label{eq:part2_use_of_equiv_between_beta_and_binom} \\
	& \le \frac{s+1}{k+2} F^B_{k+2,y}(s+1) \le  F^B_{k,y}(k x) \le \exp\left\{- k d(x,y) \label{ineq:chernoff_app} \right\}
	\end{align}
	where we use Fact~\ref{fact:equation_for_beta_binomial_cdfs} and the definition of the Beta CDF to establish equation \eqref{eq:part2_use_of_equiv_between_beta_and_binom}. The final bound in \eqref{ineq:chernoff_app} is the result of the Chernoff-Hoeffding theorem and Fact~\ref{fact:relationship_with_binom_cdfs}. Let $\delta:=y-x$, and note that $s < kx \Longrightarrow s \le \floor{(k+1)x}$ due to $s$ being integer, whence
	\begin{align}
	\Ee{(y - V)^+ } & =  \Ee{(y - V) \ind{V \le y} \given s, k} \nonumber \\
	& = \Ee{(y - V) \ind{y - \delta \le V \le y} \given s, k} +  \Ee{(y - V) \ind{V < y - \delta} \given s, k} \nonumber\\
	& > \Ee{(y - V) \ind{V < y - \delta} \given s, k}\nonumber \\
	& \ge \delta\Ee{\ind{V < y-\delta} \given s, k}\\
	& = \delta \P{V < x \given s} \nonumber \\
	& = \delta\left(1 - F^B_{k+1,x}(s) \right) \label{eq:use_of_bin_beta_identity}  \\
	& \ge \delta/2  \label{eq:use_of_median_prop}
	\end{align}
	where equation \eqref{eq:use_of_bin_beta_identity} relies on Fact~\ref{fact:equation_for_beta_binomial_cdfs}. The bound \eqref{eq:use_of_median_prop} is justified from Fact~\ref{fact:median_of_binomial_dist} and $s \le \floor{(k+1) x}$. Thus using the inequalities for both the numerator and denominator, we obtain the desired bound.
\end{myproof}
\subsubsection{Proof of Lemma~\ref{lem:accurate_suboptimal_mean}.} \label{prf:proof_of_acc_sub_means}
\begin{proof}[Proof.]
	The steps in this proof follow a similar one in \cite{agrawal2013further} but we show them for completeness. We bound the number of times the sub-optimal arm's mean is overestimated. Let $\tau_\ell$ be the time step in which the  sub-optimal arm is sampled for the $\ell$\textsuperscript{th} time. Because for any $x$, $\lim_{n\to\infty}\frac{\floor{nx}}{nx} = 1$, we can let $N$ be a large enough integer so that if $\ell \ge N$, then $\eta_1 \frac{\floor{\ell \eta_1}}{\ell \eta_1} > x' := (\theta_2 + \eta_1)/2 > \theta_2$. In that case,
	\begin{align}
	\sum_{t=1}^T\P{S_2(t) \ge \floor{N_2(t) \eta_1}, \; \pi^{\rm OG}_t = 2} & \le \Ee{\sum_{\ell=1}^T \sum_{t=\tau_\ell}^{\tau_{\ell+1}-1}\ind{S_2(\ell) \ge \floor{N_1(\ell) \eta_1}} \ind{\pi^{\rm OG}_t = 2}} \nonumber \\
	& = \Ee{\sum_{\ell=1}^T \ind{S_2(\tau_{\ell}) \ge \floor{(\ell-1) \eta_1}} \sum_{t=\tau_\ell}^{\tau_{\ell+1}-1} \ind{\pi^{\rm OG}_t = 2}} \nonumber\\
	& = \Ee{\sum_{\ell=0}^{T-1} \ind{S_2(\tau_{\ell+1}) \ge \floor{\ell \eta_1}}} \nonumber\\
	& \le  N + \sum_{\ell=N+1}^{T-1} \P{ S_2(\tau_{\ell+1}) \ge \ell \eta_1 \frac{\floor{\ell \eta_1}}{\ell \eta_1}} \nonumber \\
	& \le N + \sum_{\ell=N+1}^{T-1} \P{ S_2(\tau_{\ell+1}) \ge \ell x'} \nonumber \\
	& \le  N + \sum_{\ell=1}^{\infty} \exp(-\ell d(x', \theta_2)) \label{bound:cf_thm} \\
	& = N + \frac{1}{1 - e^{-d(x',\theta_2)}} \nonumber
	\end{align}
\end{proof}
The bound \eqref{bound:cf_thm} follows from the Chernoff-Hoeffding theorem and that $S_2(\tau_{\ell+1}) \sim \text{Binomial}(N_1(\ell+1), \theta_2) \sim \text{Binomial}(\ell, \theta_2)$.

\section{Further experiment results} \label{sec:further_exp}
\subsection{Bayes UCB experiment} \label{exp:bayes_ucb}
This experiment is motivated by \cite{kaufmann2012thompson} and in it we simulate the Bernoulli bandit problem with a $T = 500$ and two arms. Since we are interested in measuring expected regret over the prior, we draw the arms' mean rewards at random from the uniform distribution. There are 5,000 independent trials and we show the results in Figures~\ref{fig:kaufmann_regret}. OGI offers notable performance improvements over both Thompson Sampling and IDS for this modest horizon.
\begin{figure}[h!]
	\centering
	\input{plots/kaufmann_fig1.pgf}
	\caption{Frequentist regret. The OGI policy is configured $K=1$ and $\alpha=100$.}
	\label{fig:kaufmann_regret}
\end{figure}

\subsection{Additional tables for Section~\ref{sec:experiments}}
\begin{table}
	\centering
	\begin{tabular}{rrrrrr} 
		\toprule
		{}    $\alpha$ &   $\beta$ &  OGI(1) &  OGI(3) &  OGI(5) &  Gittins \\
		\midrule
		   1 & 1 &   0.760 &   0.721 &   0.712 &    0.703 \\
		   1 & 2 &   0.571 &   0.522 &   0.511 &    0.500 \\
		   1 & 3 &   0.452 &   0.401 &   0.389 &    0.380 \\
		   1 & 4 &   0.374 &   0.321 &   0.312 &    0.302 \\
		  2 & 1 &   0.853 &   0.818 &   0.809 &    0.800 \\
		  2 & 2 &   0.702 &   0.657 &   0.646 &    0.635 \\
		  2 & 3 &   0.591 &   0.543 &   0.530 &    0.516 \\
		  2 & 4 &   0.508 &   0.458 &   0.445 &    0.434 \\
		  3 & 1 &   0.893 &   0.864 &   0.855 &    0.845 \\
		  3 & 2 &   0.771 &   0.729 &   0.719 &    0.707 \\
		  3 & 3 &   0.671 &   0.626 &   0.613 &    0.601 \\
		  3 & 4 &   0.592 &   0.545 &   0.532 &    0.518 \\
		 4 & 1 &   0.916 &   0.890 &   0.882 &    0.872 \\
		  4 & 2 &   0.813 &   0.776 &   0.765 &    0.754 \\
		  4 & 3 &   0.724 &   0.682 &   0.670 &    0.658 \\
		  4 & 4 &   0.651 &   0.607 &   0.593 &    0.581 \\
		\bottomrule
	\end{tabular}
	\caption{Optimistic and exact Gittins Indices when $\gamma = 0.9$ for different Beta-Bernoulli parameters}
	\label{table:ogi_table_for_gamma_9}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{rrrrrr}
		\toprule
		{}    $\alpha$ &   $\beta$ &  OGI(1) &  OGI(3) &  OGI(5) &  Gittins \\
		\midrule
		 1.0 & 1.0 &   0.817 &   0.784 &   0.774 &    0.761 \\
		 1.0 & 2.0 &   0.637 &   0.590 &   0.577 &    0.560 \\
		 1.0 & 3.0 &   0.514 &   0.463 &   0.449 &    0.433 \\
		 1.0 & 4.0 &   0.430 &   0.376 &   0.364 &    0.348 \\
		 2.0 & 1.0 &   0.890 &   0.860 &   0.851 &    0.838 \\
		 2.0 & 2.0 &   0.752 &   0.710 &   0.698 &    0.681 \\
		 2.0 & 3.0 &   0.643 &   0.596 &   0.581 &    0.562 \\
		2.0 & 4.0 &   0.558 &   0.509 &   0.494 &    0.475 \\
		 3.0 & 1.0 &   0.921 &   0.896 &   0.887 &    0.874 \\
		3.0 & 2.0 &   0.811 &   0.773 &   0.762 &    0.744 \\
		 3.0 & 3.0 &   0.715 &   0.672 &   0.658 &    0.639 \\
		 3.0 & 4.0 &   0.637 &   0.591 &   0.575 &    0.556 \\
		4.0 & 1.0 &   0.938 &   0.916 &   0.908 &    0.895 \\
		4.0 & 2.0 &   0.847 &   0.812 &   0.801 &    0.784 \\
		4.0 & 3.0 &   0.763 &   0.722 &   0.709 &    0.690 \\
		4.0 & 4.0 &   0.691 &   0.648 &   0.633 &    0.613 \\
		\bottomrule
	\end{tabular}
	\caption{Optimistic and exact Gittins Indices when $\gamma = 0.95$ for different Beta-Bernoulli parameters}
	\label{table:ogi_table_for_gamma_95}
\end{table}
\begin{table}
	\centering
	\begin{tabular}{llrrrrr}
		\toprule
		&       &  Diff. B-UCB &  Diff. TS &    OGI &  Thompson &  Bayes UCB \\
		\midrule
		& mean &           28.9 &            3.0 &    99.7 &      102.7 &      128.5 \\
		& SD &          407.2 &          410.6 &   493.8 &      297.5 &      302.3 \\
		$A = 10$& 25\% &           20.0 &           12.0 &   -79.7 &      -62.6 &      -43.5 \\
		& 50\% &           33.0 &           22.0 &    57.3 &       70.4 &       86.5 \\
		& 75\% &           59.0 &           34.0 &   228.2 &      244.9 &      270.1 \\ \hline
		& mean &           48.4 &           20.8 &   184.1 &      205.0 &      232.5 \\
		& SD &          540.8 &          556.9 &   565.2 &      237.7 &      214.0 \\
		$A = 40$& 25\% &           46.0 &           31.0 &    37.4 &       79.7 &       92.8 \\
		& 50\% &           87.0 &           67.0 &   119.6 &      159.8 &      183.1 \\
		& 75\% &          127.0 &           91.0 &   236.1 &      286.9 &      324.0 \\ \hline
		& mean &           75.2 &           42.4 &   239.4 &      281.7 &      314.5 \\
		& SD &          443.2 &          438.3 &   451.8 &      181.4 &      203.1 \\
		$A = 70$ & 25\% &           50.0 &           37.0 &    98.9 &      157.5 &      170.3 \\
		& 50\% &          124.0 &           97.0 &   180.3 &      241.0 &      265.7 \\
		& 75\% &          182.0 &          139.0 &   276.6 &      366.0 &      412.3 \\ \hline
		& mean &           93.6 &           60.6 &   287.1 &      347.7 &      380.7 \\
		& SD &          448.3 &          442.7 &   447.2 &      184.3 &      208.8 \\
		$A = 100$  & 25\% &           57.0 &           41.0 &   149.3 &      217.9 &      234.0 \\
		& 50\% &          144.0 &          121.5 &   228.5 &      309.0 &      334.5 \\
		& 75\% &          232.0 &          182.0 &   325.4 &      434.0 &      483.7 \\
		\bottomrule
	\end{tabular}
	\caption{Summary statistics (mean, standard deviation, lower, median and upper quartiles) for regret in the \cite{chapelle2011empirical} inspired experiment. For each value of $A$ we simulated 5,000 trials. The first two columns show statistics for the absolute difference in realized regret between the two competing algorithms and OGI.}
	\label{table:additional_cli_table}
\end{table}