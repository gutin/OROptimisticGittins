\appendix
\section{Proof of Lemma~\ref{le:linearregret}} \label{proof:linearregret}
\begin{myproof}[Proof.]
	Consider an instance of the MAB with $A=2$ arms and Bernoulli rewards. We assume that the prior on arm $1$ is degenerate with mean $\lambda = 1/2$ while arm $2$ has a {\rm Beta}$(\alpha,\alpha)$ prior where $\alpha$ is a parameter we set later. Then, it is simple to check that $\pi^{G,\gamma}$ must pull arm $2$ at the first time period. With probability $1/2$, we receive a reward of $0$, so that the posterior on arm $2$ is given by a {\rm Beta}$(\alpha,\alpha+1)$ prior. Now, the continuation value from pulling arm $1$ at this stage is lower bounded by $\frac{1/2}{1-\gamma}$, while the continuation value from pulling arm $2$ is upper bounded by
	\[
	\frac{\alpha}{1+2\alpha}
	+
	\frac{
		\gamma
		\E
		\left[
		\max\left(
		R(y_{2,0}),1/2
		\right)
		\Big| y_{2,0} = \left(\alpha, \alpha+1 \right)
		\right]
	}
	{
		1-\gamma
	}.
	\]
	It follows that any optimal policy must pull arm $1$ if
	\[
	\frac{1/2}{1-\gamma}
	>
	\frac{\alpha}{1+2\alpha}
	+
	\frac{
		\gamma
		\E
		\left[
		\max\left(
		R(y_{2,0}),1/2
		\right)
		\Big| y_{2,0} = \left(\alpha, \alpha+1 \right)
		\right]
	}
	{
		1-\gamma
	},
	\]
	{\color{blue}an inequality which in turn is satisfied if
	\[
	1/2
	>
	\frac{(1-\gamma)\alpha}{1+2\alpha}
	+
	\gamma
	\left(\P{
		R(y_{2,0})>1/2 \
		\Big| y_{2,0} = \left(\alpha, \alpha+1 \right)
	} + 1/2\right) ,
	\]
	But the right hand side of the above expression goes to $\gamma/2 < 1/2$ as $\alpha \tends 0$}. Consequently, we can choose an $\alpha$ such that any optimal policy (for the discounted infinite horizon problem) chooses to pull the first arm; let $\alpha^*$ be the largest such $\alpha$. Since the state of the first arm does not change (the prior on that arm was assumed degenerate), the same condition must hold at subsequent iterations. Consequently, $\pi^{G,\gamma}$ must incur $T$-period regret lower bounded by 
	$\frac{T}{2}
	\E
	\left[
	(
	R((\alpha^*,\alpha^*+1)) - 1/2
	)^+
	\right].
	$
	The result follows. 
\end{myproof}
\section{Proof of Proposition~\ref{prop:gittins_log3T}} \label{proof:prop_log3T}

{\color{blue}
We first establish notation useful in the proof. Specifically, we let $h_t^{t'}$ denote the sequence of arms pulled and the corresponding rewards earned between periods $t$ and $t'$ ($h_t^{t'} = \emptyset$ is $t' < t$) . Recall that $\mathbf y$ denotes the $A$-tuple of sufficient statistics of priors on the the arm means of each of the $A$ arms. We denote by $g(\mathbf y, h_t^{t'})$ the $A$-tuple of sufficient statistics of posteriors on the arm means of each of the $A$ arms, obtained starting with the prior $\mathbf y$ and observing the sequence of arm pulls $h_t^{t'}$. 


We begin by establishing a simple Lemma concerning the allocation rule proposed in \cite{lai1987adaptive}; we note that the allocation rule there is specified for a {\em fixed} time horizon and the Lemma below extends it to a policy that specifics a choice of arm for every epoch. 

\begin{lemma} 
\label{lemma:laianytime}
Let the prior with sufficient statistic $\mathbf y$ satisfy the requirements of Theorem~$3$ in \cite{lai1987adaptive}. Then, there exists a policy $\tilde \pi_{\mathbf y}$, and a constant $C_{\mathbf y}$ for which
\[
{\rm Regret}\left(
\tilde \pi_{\mathbf y}, T
\right)
\triangleq 
\E_{\mathbf y}
\left[
{\rm Regret}\left(
\tilde \pi_{\mathbf y}, T,\theta
\right)
\right]
\leq
C_{\mathbf y} \log^3 T
\] 
for all $T$. 
\end{lemma}
\begin{myproof}[Proof.]
By Theorem $3$ in \cite{lai1987adaptive}, we know that there exists a constant $\hat C_{\mathbf y}$ and a sequence of policies, $\tilde \pi_{\mathbf y, T}$ \footnote{this entails a minor abuse of our definition of a policy: $\tilde \pi_{\mathbf y, T}$ is not specified for $t > T$}  such that 
\[
\lim_T 
\frac{
\E
_{\mathbf y}
\left[
{\rm Regret}\left(
\tilde \pi_{\mathbf y, T}, T,\theta
\right)
\right]
}
{
\log^2 T
}
=
\hat C_{\mathbf y}.
\]
Consequently, there exists a constant $\bar C_{\mathbf y}$, so that for any $T$, 
\[
\E
_{\mathbf y}
\left[
{\rm Regret}\left(
\tilde \pi_{\mathbf y, T}, T,\theta
\right)
\right]
\leq
\bar C_{\mathbf y} \log^2 T
\]
Now consider the `doubling' policy $\tilde \pi_{\mathbf y}$ which at time $t$ selects arms according to the policy $\tilde \pi_{\mathbf y,2^k}$ applied at the state $g\left(\mathbf y, h_{2^{k(t)}-1}^{t-1}\right)$ where $k(t) = \lfloor \log_2(t+1) \rfloor$. In words, this is the policy obtained wherein (a) time is divided into epochs such that the $k$th epoch extends from time $2^{k}-1$ to $2^{k+1}-2$, and (b) at the start of the $k$th epoch we `forget' everything learned up until that time and subsequently use the policy $\tilde \pi_{\mathbf y,2^k}$ over the course of that epoch. Thus,
\[
\begin{split}
\E
_{\mathbf y}
\left[
{\rm Regret}\left(
\tilde \pi_{\mathbf y}, T,\theta
\right)
\right]
&
\leq
\sum_{k=1}^{\lfloor \log (T+2) \rfloor}
\E_{\mathbf y}
\left[
\E_{\mathbf y_{2^k -2}}
\left[
{\rm Regret}\left(
\tilde \pi_{\mathbf y,2^k}, 2^k,\theta
\right)
\right]
\right]
\\
&
=
\sum_{k=1}^{\lfloor \log (T+2) \rfloor}
\E_{\mathbf y}
\left[
{\rm Regret}\left(
\tilde \pi_{\mathbf y,2^k}, 2^k,\theta
\right)
\right]
\\
&
\leq
\sum_{k=1}^{\lfloor \log (T+2) \rfloor}
\bar C_{\mathbf y} \log^2 2^k
\\
&
\leq
\bar C_{\mathbf y} C \log^3 T
\end{split}
\]
where the equality follows from the tower property and where $C$ is some absolute constant. 
\end{myproof}

Next, we establish a simple result related to the Gittins index policy that relates the finite horizon performance of the policy to the (discounted) infinite horizon performance. 

\begin{lemma}
\label{lemma:gittinsfiniteinfinite}
For any $\mathbf{\hat y} \in \Yscr$, and horizon $T' \geq 2$, we have
\[
\E
_{\hat{\mathbf y}}
\left[
{\rm Regret}\left(
\pi^{G,1-1/{T'}}, T',\theta
\right)
\right]
\leq
4
\E
_{\hat{\mathbf y}}
\left[
{\rm Regret}\left(
\pi^{G,1-1/{T'}}, H_{T'},\theta
\right)
\right]
\]
where $H_{T'}$ is an independent Geometrically distributed random variable with mean $T'$. 
\end{lemma}
\begin{myproof}[Proof.]
We have
\[
\begin{split}
\E_{\hat{\mathbf y}}\left[\Regret{\pi^{G, 1-1/{T'}}, T', \theta}\right]  
& = 
\E_{\hat{\mathbf y}}\left[\Regret{\pi^{G, 1-1/{T'}}, T', \theta}\right]\frac{\P{H_{T'} > T'}}{(1 - 1/{T'})^{T'}}
\\
& \leq
\E_{\hat{\mathbf y}}\left[\Regret{\pi^{G, 1-1/{T'}}, H_{T'}, \theta} \given H_{T'} > T'\right]\frac{\P{H_{T'} > T'} }{(1 - 1/{T'})^{T'}}
\\ 
& \leq
{(1 - 1/{T'})^{-T'}} \E_{\hat{\mathbf y}}\left[\Regret{\pi^{G, 1-1/{T'}}, H_{T'}, \theta}\right]
\\
& \leq
4 \E_{\hat{\mathbf y}}\left[\Regret{\pi^{G, 1-1/{T'}}, H_{T'}, \theta}\right]
\end{split}
\]
where the first inequality follows from the fact that $\E_{\hat{\mathbf y}}\left[\Regret{\pi^{G, 1-1/{T'}}, n, \theta}\right]$ is non-decreasing in $n$, the second inequality follows from the fact that regret is non-negative. 
\end{myproof}

We can now proceed with the proof of the proposition. First, we note that since the Gittins policy with discount factor $1-1/{T'}$ is optimal for a geometrically distributed horizon with mean $T'$, we must have for any $\mathbf{\hat y} \in \Yscr$, 
\begin{equation}
\label{eq:gittinsopt}
\E_{\hat{\mathbf y}}\left[\Regret{\pi^{G, 1-1/{T'}}, H_{T'}, \theta}\right]
\leq
\E_{\hat{\mathbf y}}\left[\Regret{\tilde \pi_{\mathbf{y}}, H_{T'}, \theta}\right]
\end{equation}
But we have
\[
\begin{split}
\E_{{\mathbf y}}\left[\Regret{\pi^{D}, T, \theta}\right]
&\leq
\sum_{k=1}^{\lfloor \log (T+2) \rfloor}
\E_{{\mathbf y}}\left[
\E_{\hat{\mathbf y}_{2^k-2}}\left[
\Regret{\pi^{G,1-1/2^k}, 2^k, \theta}\right]
\right]
\\
&\leq
\sum_{k=1}^{\lfloor \log (T+2) \rfloor}
\E_{{\mathbf y}}\left[
4
\E_{\hat{\mathbf y}_{2^k-2}}\left[
\Regret{\pi^{G,1-1/2^k}, H_{2^k}, \theta}\right]
\right]
\\
&\leq
\sum_{k=1}^{\lfloor \log (T+2) \rfloor}
\E_{{\mathbf y}}\left[
4
\E_{\hat{\mathbf y}_{2^k-2}}\left[
\Regret{\tilde \pi_{\mathbf{y}}, H_{2^k}, \theta}
\right]
\right]
\\
&
=
4\sum_{k=1}^{\lfloor \log (T+2) \rfloor}
\E_{{\mathbf y}}\left[
\Regret{\tilde \pi_{\mathbf{y}}, H_{2^k}, \theta}
\right]
\\
&
\leq 
4 C_{\mathbf y}\sum_{k=1}^{\lfloor \log (T+2) \rfloor}
\E \left[ \log^3 H_{2^k} \right]
\\
&
\leq 
4 C_{\mathbf y}\sum_{k=1}^{\lfloor \log (T+2) \rfloor}
k^3
\end{split}
\]
where the first inequality follows simply from the definition of $\pi^D$, the second inequality follows from Lemma~\ref{lemma:gittinsfiniteinfinite}, the third inequality follows from the aforementioned optimality of the Gittins policy (namely, \eqref{eq:gittinsopt}), the first equality follows from the tower property, the fourth inequality follows from Lemma~\ref{lemma:laianytime}, and the fifth and final inequality is simply Jensen's inequality. 
}



%\begin{myproof}[Proof.]
%	%First, letting $\gamma_n = 1 - 1/n$, we show that
%	%\begin{equation} \label{eq:basic_finite_time_gittins}
%	%\Regret{\pi^{G, \gamma_n}, n} = O\left( \log^2(n) \right).
%	%\end{equation}
%	We start by proving a regret bound for the Gittins index policy over a finite horizon.
%	Since we are dealing with Bayesian regret, we will prove our bound by comparing against the Information Directed Sampling (IDS) policy, denoted by $\pi^{IDS}$. It was shown in \citep{russo2014learning} (Proposition 2) that for any horizon $T \in \mathbb N$ and any prior, described by a vector $\hat{\mathbf y}$, that
%	\[
%	\E_{\hat{\mathbf y}}\left[ \Regret{\pi^{IDS}, T, \theta} \right] 
%	 \le  \sqrt{\frac{1}{2} (A \log A) T}.
%	\]
%	whenever the reward distribution is uniformly bounded.
%	We will leverage this useful result in the next step of the proof.
%	
%	Fix any $n \ge 2$ and sufficient statistic $\hat{\mathbf y}$. Define $H$ to be a geometric random variable with mean $n$, then	
%	\begin{align}
%	\E_{\hat{\mathbf y}}\left[\Regret{\pi^{G, \gamma_n}, n, \theta}\right]  & = \E_{\hat{\mathbf y}}\left[\Regret{\pi^{G, \gamma_n}, n, \theta}\right]\frac{\P{H > n}}{(1 - 1/n)^n} \nonumber \\
%	&  \le  \E_{\hat{\mathbf y}}\left[\Regret{\pi^{G, \gamma_n}, H, \theta} \given H > n\right]\frac{\P{H > n} }{(1 - 1/n)^n} \nonumber\\
%	& \le  (1 - 1/n)^{-n}  \E_{\hat{\mathbf y}}\left[ \Regret{\pi^{G,\gamma_n}, H, \theta} \right] \nonumber   \\
%	& \le  4 \E_{\hat{\mathbf y}}\left[ \Regret{\pi^{G,\gamma_n}, H, \theta} \right]   \nonumber \\
%	& \le 4\E_{\hat{\mathbf y}}\left[ \Regret{\pi^{IDS}, H, \theta} \right] \label{eqn:opt_gittins_over_geo_horizon} \\
%	& \le 4 \E\left[  \sqrt{\frac{1}{2} (A \log A) H} \right] \nonumber\\
%	& \le  \E\left[  \sqrt{8 (A \log A) H} \right] \nonumber \\
%	& \le  \sqrt{8 (A \log A) \E[H]}  \label{eqn:jensens_ids} \\
%	& =  \sqrt{8 (A \log A) n} \label{eqn:regret_bound_for_ids}   
%	\end{align}
%	where \eqref{eqn:opt_gittins_over_geo_horizon} follows from the Gittins index policy being optimal over the random \& geometrically distributed horizon of $H$ (because $\E[H] = n$ and the discount factor is assumed to be $1 - 1/n$ here). Equation~\eqref{eqn:jensens_ids} follows from Jensen's inequality.
%	
%	Back to the doubling trick policy. Recall that this entails pulling the arms according to $\pi^{G,1 - 1/2^{k-1}}$ during periods $\{2^{k-1},\ldots,2^{k}-1\}$ for each $k \in \mathbb{N}$.
%	For convenience, we define the function $\text{Regret}(\pi, t_1, t_2, \theta)$ to be the total regret accumulated from periods $t_1$ up to $t_2$ under policy $\pi$ and where the arms' parameters are set to $\theta$. We will also denote the tuple of sufficient statistics for the arms in period $t$ as $\mathbf y_t$, to reflect all the Bayesian updates up to and inlcuding that time.
%	
%	Now we can bound the Bayes risk up to time $T$ as follows:
%	\begin{align}
%		\E_{\mathbf y}\left[\Regret{\pi^D, T, \theta} \right] &
%		\leq 1 + \sum_{k=2}^{\ceil{\log_2 T}} \E_{\mathbf y}\left[\Regret{\pi^D, 2^{k-1}, 2^k - 1, \theta} \right] \nonumber \\
%		& = 1+ \sum_{k=2}^{\ceil{\log_2 T}} \E_{\mathbf y}\left[ \E_{\mathbf y_{2^k-1}}\left[\Regret{\pi^{G, 1 - 1/2^{k-1}}, 2^{k-1}, \theta} \right] \right] \nonumber \\
%		& \le 1 + \sum_{k=2}^{\ceil{\log_2 T}}  \sqrt{8(A \log A) 2^{k-1}} \label{eqn:application_of_ids}\\
%		& = \mathcal O\left( \sqrt{T} \right), \nonumber
%	\end{align}
%	where inequality \eqref{eqn:application_of_ids} is implied by  \eqref{eqn:regret_bound_for_ids}.
%\end{myproof}

\section{Properties of the Optimistic Gittins index}\label{sec:appendix_properties_of_ogi}
This section gives proofs for a few properties of the Optimistic Gittins index that are used throughout the paper and particularly in the proof of Theorem~\ref{thm:frequentist_optimal_bound}.  
It shall be useful, in what follows, to define the continuation value for the Vittles's retirement problem (\cite{whittle1980multi}) as
\[
V_\gamma(y, \lambda)  \defeq \sup_{\tau > 0} \E_y\left[\sum_{t=1}^{\tau} \gamma^{t-1} X_{i,t} + \gamma^{\tau} \frac{\lambda}{1-\gamma}\right],
\]
so that the Gittins index is then the solution in $\lambda$ to $\lambda/(1-\gamma) = V_\gamma(y, \lambda)$. In an analogous fashion, we define the optimistic continuation value, for parameters $K$ and $\lambda$, to be
\[
V^K_\gamma(y, \lambda) \defeq \sup_{1 \le \tau \le K} \E_y\left[\sum_{t=1}^{\tau} \gamma^{t-1}  X_{i,t} + \gamma^{\tau} \frac{R_{\lambda, K}(\tau, y_{i,\tau-1})}{1-\gamma}\right].
\]
From this definition, it follows that the solution for $\lambda$ to the equation $\lambda/(1-\gamma) = V^K_\gamma(y, \lambda)$ is the Optimistic Gittins index.

Throughout this section, we will sometimes discuss the value of the index at some particular time $t$ during the execution of the algorithm, which depends on the statistic gathered about the arm using information up to but strictly \emph{not including} time $t$. As such, we will define the number of pulls of arm $i$ up to time $t-1$ as
\[
\Ntg{i}{t} \defeq N_i(t-1)
\]
where we recall $N_i(t)$ is the counter for the number of total number of pulls up to and including $t$. From the $\Ntg{i}{t}$ pulls of the arm, the total reward accumulated is defined as
\[
S_i(t) \defeq \sum_{s=1}^{\Ntg{i}{t}} X_{i,s}.
\]

We begin by investigating the effect of the parameter $\lambda$, which gives the deterministic payoff in \eqref{eqn:ogi_general}, on the continuation value $V^K_\gamma(y, \lambda)$ and use that to find out how close an approximation $v^K_\gamma(y)$ is to the Gittins index.
\begin{fact}\label{fact:v_is_convex}
	For any state $y \in \mathcal{Y}$, discount factor $\gamma$ and parameter $K$, the function $V^K_\gamma(y,\lambda)$ is convex in $\lambda$.
	{\color{blue}Moreover, the function $V^K_\gamma(y,\lambda)$ is Lipschitz continuous in $\lambda$ with a Lipshitz constant of $\gamma/(1-\gamma)$.}
	% Moreover, if $R(y)$ is a continuous random variable, the function is also differentiable.
\end{fact}
\begin{myproof}[Proof.]
	Fix an arbitrary state $y$ and discount factor $\gamma \in (0,1)$. Our proof is by induction on the parameter $K$. For $K = 1$, recall from Section~\ref{sec:gittins_and_approx} that
	\begin{align*}
	{\color{blue} V^1_\gamma(y, \lambda) = \E_y\left[X_{i,1} \right] +  \frac{\gamma}{1-\gamma} \E_y\left[\max(\lambda, R(y_{i,0}))\right].}
	\end{align*}
	Thus the function is convex because it is an expectation over a convex piecewise linear function of random variables $X_{i,1}$ and $R(y_{i,0})$.
	{\color{blue}To prove Lipschitz continuity, it's enough to note that
	for any $\lambda_1, \lambda_2 \in \R$, that
	\begin{align*}
		| V^1_\gamma(y, \lambda_1) - V^1_\gamma(y, \lambda_2)| = & \frac{\gamma}{1-\gamma} |\E_y[\max(\lambda_1, R(y_{i,0})) - \max(\lambda_2, R(y_{i,0}))  ]  | \\
	& \leq \frac{\gamma}{1-\gamma} |\lambda_1 - \lambda_2 |	.
	\end{align*}
		
	}
	
	%Now assume that $R(y)$ is a continuous random variable. We will verify through the bounded convergence theorem that $V^1_\gamma(y, \lambda)$ is differentiable. Indeed, this holds because the event $\{ R(y)= \lambda/(1-\gamma)\}$, at which the random variable inside the expectation is not differentiable, has measure zero, precisely because $R(y)$ is a continuous random variable.
	Now we prove the inductive step. For any $K > 1$, assume that $V^{K-1}_\gamma(y, \lambda)$ is convex and Lipshitz continuous. By writing the Bellman equation,
	{\color{blue}
	\begin{align*}
	V^K_\gamma(y, \lambda) & = \E_y\left[X_{i,1}\right] + \gamma\E_y\left[\max\left(\lambda, V^{K-1}_\gamma(y_{i,1}, \lambda)\right) \right],
	\end{align*}
	}
	we again notice an expectation over a maximum of convex functions in $\lambda$. This form for $V^K_\gamma(y, \lambda)$ implies that  it is also convex in $\lambda$. % If $V^{K-1}$ is differentiable and $R(y)$ is a continuous random variable, then the form of $V^{K-1}$ also implies that it is differentiable in $\lambda$.
	{\color{blue}Finally, to prove Lipschitz continuity, we will use the fact that the pointwise maximum of two Lipschitz functions, having respective constants $L_1$ and $L_2$, is also Lipshitz with a constant of $\max(L_1, L_2)$. Because of this, by fixing $\lambda_1, \lambda_2 \in \R$, we have that
		\begin{align*}
		| V^K_\gamma(y, \lambda_1) - V^K_\gamma(y, \lambda_2)| & =  \gamma \left|\E_y[\max(\lambda_1, V^{K-1}_\gamma(y_{i,1}, \lambda_1)) - \max(\lambda_2, V^{K-1}_\gamma(y_{i,1}, \lambda_2))  ]  \right| \\
		& \leq \frac{\gamma^2}{1-\gamma} |\lambda_1 - \lambda_2| \\
		& \leq \frac{\gamma}{1-\gamma} |\lambda_1 - \lambda_2|
		\end{align*}
	where the second-to-last inequality follows from the induction hypothesis that $V^{K-1}_\gamma(y_{i,1}, \lambda))$ is Lipschitz continuous in $\lambda$ with a constant of $\gamma/(1-\gamma)$, and also from the fact that the identity function for $\lambda$ (within the maximum expression) is trivially Lipschitz continuous.
	}
\end{myproof}

\begin{lemma} \label{cor:equivalent_event}
	Suppose that arm rewards are bounded. That is, there exists a constant $B \in \Re_+$ such that $X_{i,t} \in [0, B]$ for every arm $i$ and time $t$. 
	
	Let $v^K_{i,t}$ be the Optimistic Gittins Index of arm $i$ at time $t$ and let $\eta$ be a scalar, then the following equivalence holds
	\[
	\{v^K_{i,t} < \eta \} = \{(1-\gamma_t)V^K_{\gamma_t}(y_{i,\Ntg{i}{t}}, \eta) < \eta\}\]
	where {\color{blue}$y_{i,\Ntg{i}{t}}$} is the sufficient statistic for estimating the $i$th arm's parameter $\theta_i$ at time $t$.
\end{lemma}
{
\color{blue}
\begin{myproof}[Proof.]
	Fix any state $y$ and discount factor $\gamma$. At $\lambda = 0$, we have
	\[
		(1-\gamma)V^K_{\gamma}(y, 0) \ge 0
	\]
	because $V^K_{\gamma}(0,y)$ is the expectation of a sum of nonnegative terms. Also, in the other extreme case when $\lambda = B$, the function in question evaluates to
	\[
		V^K_{\gamma}(y, B) = \E_y\left[X_{i,1}\right] + \frac{\gamma B}{(1-\gamma)} < \frac{B}{(1-\gamma)}.
	\]
	and so $(1-\gamma)V^{K}_\gamma(y, B) < B$.
	
	Next we prove that $V^K_\gamma(y, \lambda)$ is monotonically increasing in $\lambda$. To show this, pick any $\lambda' < \lambda''$ and let $\tau^*(\lambda')$ and $\tau^*(\lambda'')$ denote the two optimal stopping times under $\lambda', \lambda''$, respectively. It then follows, from $R_{\lambda,\tau}(.)$ being an increasing function of $\lambda$ on every sample path, that
	\begin{align*}
	V^K_{\gamma}(y, \lambda') & = \E_y\left[\sum_{t=1}^{\tau^*(\lambda')} \gamma^{t-1}  X_{i,t} + \gamma^{\tau^*(\lambda')} \frac{R_{\lambda', K}(\tau, y_{i,\tau^*(\lambda')-1})}{1-\gamma}\right] \\
	& \leq \E_y\left[\sum_{t=1}^{\tau^*(\lambda')} \gamma^{t-1}  X_{i,t} + \gamma^{\tau^*(\lambda')} \frac{R_{\lambda'', K}(\tau, y_{i,\tau^*(\lambda')-1})}{1-\gamma}\right] \\
	& \leq \E_y\left[\sum_{t=1}^{\tau^*(\lambda'')} \gamma^{t-1}  X_{i,t} + \gamma^{\tau^*(\lambda'')} \frac{R_{\lambda'', K}(\tau, y_{i,\tau^*(\lambda'')-1})}{1-\gamma}\right] \\
	& = V^K_{\gamma}(y, \lambda'').
	\end{align*}
	Let's put together these observations:
	\begin{itemize}
		\item $(1-\gamma)V^{K}_\gamma(y, \lambda) \ge \lambda $ at $\lambda = 0$.
		\item   $(1-\gamma)V^{K}_\gamma(y, \lambda) < \lambda $ at $\lambda = B$
		\item $(1-\gamma)V^{K}_\gamma(y, \lambda)$ is monotonically increasing in $\lambda$.
	\end{itemize}
	These together with Fact~\ref{fact:v_is_convex} show that the univariate function  $(1-\gamma)V^K_\gamma(y, \lambda) - \lambda$ is continuous and decreasing in $\lambda$. Moreover, this function is non-negative for any $\lambda \le v^K_\gamma(y)$ (since $v^K_\gamma(y)$ is the root of the function) and is also negative for $\lambda > v^K_\gamma(y)$.
	\begin{figure}
		\centering
		\input{plots/visualize_gx.pgf} 
		\caption{Visualization of Lemma~\ref{cor:equivalent_event}'s proof for a instance of the problem with a Beta prior corresponding to the pair $y = (4,5)$, a discount factor of $\gamma=0.95$ and $K = 2$. The intersection of the two lines represents the Optimistic Gittins index.}
		\label{fig:visaulize_gx_proof}
	\end{figure}
	This proves the result in question. Figure~\ref{fig:visaulize_gx_proof} also provides a visualization.
\end{myproof}
}

\subsection{Proof of Lemma~\ref{lemma:approx_bound}} \label{prf:approx_bound}
\begin{myproof}[Proof.]
	Let $K < M$ be two look-ahead parameters used in the definition of OGI. We will show that $V^K_\gamma(y, \lambda) \le V^M_\gamma(y, \lambda)$ where we recall the definitions of these functions from the beginning of Section~\ref{sec:appendix_properties_of_ogi}.
	
	We begin with a fundamental step. Let $\tau_1$ and $\tau_2$ be any predictable stopping times (i.e. $\mathcal F_{t-1}$-measurable random times) such that $\tau_1$ precedes $\tau_2$ almost surely, that is $\tau_1 < \tau_2$. Recall that the expected reward of the $i$th arm satisfies $\Ee{X_{i,t} \given \theta_i} = \mu(\theta_i)$ for all $t$. Let $\hat \theta_i \in \Theta$ denote a realization of the random variable $\theta_i$ and let $\zeta(\hat \theta_i)$ be a real-valued, measurable function of $\hat \theta_i$. In this case, we have that
	\begin{align*}
	\Ee{\sum_{t=\tau_1+1}^{\tau_2} \gamma^{t-1} X_{i,t} + \gamma^{\tau_2}\frac{\zeta(\hat \theta_i)}{1 - \gamma} \given[\Bigg] \theta_i = \hat \theta_i} & =\mu(\hat \theta_i) \Ee{ \sum_{t=\tau_1+1}^{\tau_2} \gamma^{t-1}  \given[\Bigg] \theta_i = \hat \theta_i} + \Ee{\frac{\gamma^{\tau_2}}{1-\gamma}\given[\Bigg] \theta_i = \hat \theta_i}\zeta(\hat \theta_i) \\
	%& =\mu(\hat \theta_i) \Ee{ \sum_{t=\tau_1+1}^{\tau_2} \gamma^{t-1} \given[\Bigg] \theta_i = \hat \theta_i } + \Ee{\sum_{t=\tau_2+1}^{\infty} \gamma^{t-1} \given[\Bigg] \theta_i = \hat \theta_i}\frac{\zeta(\hat \theta_i)}{1 - \gamma} \\
	& \le \Ee{\gamma^{\tau_1} \given \theta_i = \hat \theta_i}  \frac{\max(\zeta(\hat \theta_i), \mu(\hat \theta_i))}{1-\gamma}.
	\end{align*}
	Thus we conclude, because $\hat \theta_i$ was arbitrary, that almost surely,
	\begin{equation} \label{ineq:fundamntal_bound_for_lemma_2}
	\color{blue}
	\Ee{\sum_{t=\tau_1+1}^{\tau_2} \gamma^{t-1} X_{i,t} + \gamma^{\tau_2}\frac{\zeta( \theta_i)}{1 - \gamma} \given[\Bigg] \theta_i}  \le \Ee{\gamma^{\tau_1} \given \theta_i }  \frac{\max(\zeta( \theta_i), \mu(\theta_i))}{1-\gamma}.
	\end{equation}
	Let $\tau^\star$ be a stopping time that achieves the supremum in  $V_\gamma^M(y, \lambda)$ and define the predictable stopping time $\tau^\star_K \defeq K \wedge \tau^\star$. Consider the (conditional) cumulative rewards in the definition of $V^M_\gamma(y)$, from time $\tau^\star_K+1$ onwards, given the sufficient statistic observed at time $\tau_K^\star$. That is, 
	\[
		\E\left[\sum_{t=\tau_K^\star+1}^{\tau^\star}  \gamma^{t-1} X_{i,t} + \gamma^{\tau^\star} R_{\lambda,M}(\tau^\star, y_{i,\tau^\star-1})/(1-\gamma)
	\given[\Bigg] y_{i,\tau_K^\star-1} \right].
	\]
	We upper bound this random variable as follows. First, we note that, at any time $s$ and for any statistic $\hat y \in \mathcal{Y}$, the following statement holds
	\begin{equation}\label{eqn:dist_equal_theta_i}
	\P{R(\hat y) \le r} = \P{\mu(\theta_i) \le r \given y_{i,s} = \hat y}, \qquad \forall r \in \Re
	\end{equation}
	meaning that the posterior distribution of the arm's expected reward $R(y_{i,s})$ is the same as $\mu(\theta_i)$ \emph{conditioned} on having observed statistic $\hat y$ about the arm. This holds by definition of the random variable $R(y)$. Because of this observation, we have that the following inequality  holds almost surely,
	{\color{blue}
	\begin{align*}
		& \gamma^{\tau^\star} \frac{R_{\lambda,M}(\tau^\star, y_{i,\tau^\star-1})}{1-\gamma}
		 \nonumber \\
		&\quad =  \gamma^{\tau^\star}\left( \ind{\tau^\star =M}\frac{\max(\lambda, R(y_{i,\tau^\star-1}))}{1-\gamma} + \ind{\tau^\star < M}\frac{\lambda}{1-\gamma}\right)
		 \nonumber\\
		&\quad= \ind{\tau^\star =M} \gamma^{M}\frac{\max(\lambda, R(y_{i,M-1}))}{1-\gamma} + \ind{\tau^\star < M} \gamma^{\tau^\star}\frac{\lambda}{1-\gamma}
	\nonumber\\
		& \quad\overset{(*)}{=} \ind{\tau^\star =M} \gamma^{M}\frac{\Ee{\max(\lambda, R(y_{i,M-1})) \given y_{i,M-1} }}{1-\gamma} + \ind{\tau^\star < M} \gamma^{\tau^\star}\frac{\lambda}{1-\gamma}
		\\
		& \quad\overset{(\dagger)}{=} \ind{\tau^\star =M} \gamma^{M}\frac{\Ee{\max(\lambda, \mu(\theta_i)) \given y_{i,M-1} }}{1-\gamma} + \ind{\tau^\star < M} \gamma^{\tau^\star}\frac{\lambda}{1-\gamma}
		 \\
		&\quad \overset{(**)}{\le} \frac{  \Ee{\gamma^{\tau^\star}\max(\lambda, \mu(\theta_i)) \given y_{i,\tau^\star-1} }}{1-\gamma}
	\end{align*}
	}
	where $(*)$ and $(**)$ both use the fact that for any $t$, $\tau^\star \le t$ is measurable with respect to the $\sigma$-algebra generated by $y_{i,t-1}$, namely $\mathcal F_{t-1}$. Equation ($\dagger$) follows from \eqref{eqn:dist_equal_theta_i}. Therefore, immediately using the above inequality and conditioning on the event $\tau^\star > K$, we have that
	\begin{align} 
	&\E\left[\sum_{t=\tau_K^\star+1}^{\tau^\star} \gamma^{t-1} X_{i,t} + \gamma^{\tau^\star} \frac{R_{\lambda,M}(\tau^\star, y_{i,\tau^\star-1})}{1-\gamma}
	\given[\Bigg] \tau^\star > K, \;y_{i,\tau^\star_K-1} \right] \nonumber \\
	&\qquad \le  \E\left[\sum_{t=\tau_K^\star+1}^{\tau^\star} \gamma^{t-1} X_{i,t} +  \Ee{ \gamma^{\tau^\star}\frac{\max(\lambda,\mu(\theta_i)))}{1-\gamma}\given[\Bigg] y_{i,\tau^\star-1}}
	\given[\Bigg] \tau^\star > K, \;y_{i,\tau^\star_K-1} \right] \nonumber\\
	&\qquad = \E\left[\sum_{t=\tau_K^\star+1}^{\tau^\star} \gamma^{t-1} X_{i,t} + \gamma^{\tau^\star} \frac{\max(\lambda, \mu(\theta_i))}{1-\gamma}
	\given[\Bigg] \tau^\star > K, \;y_{i,\tau^\star_K-1} \right] \label{eqn:another_toer_prop_use}\\
	&\qquad = \E\left[ \Ee{\sum_{t=K+1}^{\tau^\star} \gamma^{t-1} X_{i,t} + \gamma^{\tau^\star}\frac{\max(\lambda,\mu(\theta_i))}{1 - \gamma} \given[\Bigg] \theta_i} 
	\given[\Bigg]\tau^\star > K, \; y_{i,\tau^\star_K-1} \right] \label{eqn:proof_lemma2_toer_prop} \\
	&\qquad \le \E\left[ \gamma^{\tau^\star_K} \frac{\max(\mu(\theta_i), \lambda)}{1 - \gamma}\given[\Bigg] \tau^\star > K, \; y_{i,\tau^\star_K-1}  \right]  \label{ineq:proof_lem_2_use_of_first_bound} \\
	&\qquad = \E\left[ \gamma^{\tau^\star_K} \frac{\max(R(y_{i,\tau^\star_K-1}), \lambda)}{1 - \gamma}\given[\Bigg] \tau^\star > K, \; y_{i,\tau^\star_K-1}  \right]  \label{ineq:obvious_step} \\
	&\qquad = \Ee{\frac{\gamma^{\tau^\star_K}R_{\lambda,K}(\tau^\star_K, y_{i,\tau^\star_K-1})}{1-\gamma} \given[\Bigg]  \tau^\star > K, \; y_{i,\tau^\star_K-1}} \label{eqn:use_of_def_of_R}
	\end{align}
	where \eqref{eqn:another_toer_prop_use}, \eqref{eqn:proof_lemma2_toer_prop} use the tower property and \eqref{ineq:proof_lem_2_use_of_first_bound} follows from the bound in \eqref{ineq:fundamntal_bound_for_lemma_2} because $\tau^\star_K < \tau^\star$, almost surely. Equation \eqref{ineq:obvious_step} follows from statement \eqref{eqn:dist_equal_theta_i} and that the event $\tau^\star > K$ is $\mathcal{F}_{K-1}$-measurable (we can decide whether to pull arm $i$ or retire based on information up to and including time $K-1$). Finally equation \eqref{eqn:use_of_def_of_R} is derived by substituting in the definition of $R_{\lambda,K}$ (as given in Section~\ref{sec:gittins_and_approx}) and noting that $\tau^\star_K = K$ under the above conditioning.
	
	We now condition on the complement of the previous event we considered, namely, $\tau^\star \le K$. Under that event, $\tau^\star$ occurred early enough before time $K+1$ and thus $\tau^\star_K = \tau^\star$. Therefore, it follows from this observation that
	\begin{align} 
	&\E\left[\sum_{t=\tau_K^\star+1}^{\tau^\star}\gamma^{t-1} X_{i,t} + \gamma^{\tau^\star} \frac{R_{\lambda,M}(\tau^\star, y_{i,\tau^\star-1})}{1-\gamma}
	\given[\Bigg] \tau^\star \le K, \;y_{i,\tau^\star_K-1} \right] \nonumber  \\
	&\qquad = \E\left[\gamma^{\tau^\star}  \frac{\lambda}{1-\gamma}
	\given[\Bigg]\tau^\star \le K, \; y_{i,\tau^\star_K-1} \right] \nonumber \\
	&\qquad \le \E\left[\gamma^{\tau^\star_K}  \frac{R_{\lambda,K}(\tau^\star_K, y_{i,\tau^\star_K-1})}{1-\gamma}
	\given[\Bigg]\tau^\star \le K, \; y_{i,\tau^\star_K-1} \right] \label{eqn:proof_lem_2_second_use_of_R_def}
	\end{align}
	where \eqref{eqn:proof_lem_2_second_use_of_R_def} is obtained by noting that $R_{\lambda,K}(\tau, y) \ge \lambda$ for any choice of $\tau, K$ and $y$. Thus, by the law of total expectation and \eqref{eqn:use_of_def_of_R}, \eqref{eqn:proof_lem_2_second_use_of_R_def}, we establish that
	\begin{equation} \label{ineq:proof_lem_2_main_bound_in_proof}
	\E\left[\sum_{t=\tau_K^\star+1}^{\tau^\star}\gamma^{t-1} X_{i,t} + \gamma^{\tau^\star} \frac{R_{\lambda,M}(\tau^\star, y_{i,\tau^\star-1})}{1-\gamma}
	\given[\Bigg] \;y_{i,\tau^\star_K-1} \right] \le \Ee{\gamma^{\tau^\star_K}  \frac{R_{\lambda,K}(\tau^\star_K, y_{i,\tau^\star_K-1})}{1-\gamma} \given[\Bigg] y_{i,\tau^\star_K-1}}.
	\end{equation}
	We are ready to complete our main argument in this proof by using the above bound and `breaking up' the $V_\gamma^M(y, \lambda)$ into rewards from times before $\tau_K^\star$ and after (and bounding the latter terms). More precisely, we obtain that
	\begin{align}
	V_\gamma^M(y,\lambda) & = \E_{y}\left[\sum_{t=1}^{\tau^\star}\gamma^{t-1} X_{i,t} + \gamma^{\tau^\star}\frac{R_{\lambda,M}(\tau^\star, y_{i,\tau^\star-1})}{1-\gamma}\right] \\
	& = \E_{y}\left[\sum_{t=1}^{\tau^\star_K}\gamma^{t-1} X_{i,t} + \sum_{t'=\tau^\star_K+1}^{\tau^\star}\gamma^{t'-1} X_{i,t'} +  \gamma^{\tau^\star}\frac{R_{\lambda,M}(\tau^\star, y_{i,\tau^\star-1})}{1-\gamma}\right] \nonumber \\
	& = \E_{y}\left[\sum_{t=1}^{\tau^\star_K}\gamma^{t-1} X_{i,t} + \Ee{\sum_{t'=\tau^\star_K+1}^{\tau^\star}\gamma^{t'-1} X_{i,t'} +  \gamma^{\tau^\star}\frac{R_{\lambda,M}(\tau^\star, y_{i,\tau^\star-1})}{1-\gamma} \given[\Bigg] y_{i,\tau^\star_K-1} }\right] \label{eqn:proof_lem_2_tower_prop_again} \\
	& \le  \E_{y}\left[\sum_{t=1}^{\tau^\star_K}\gamma^{t-1} X_{i,t} + \Ee{\gamma^{\tau^\star_K}  \frac{R_{\lambda,K}(\tau^\star_K, y_{i,\tau^\star_K-1})}{1-\gamma} \given[\Bigg] y_{i,\tau^\star_K-1}}\right] \label{eqn:proof_lem2_using_the_main_argument} \\
	& =  \E_{y}\left[\sum_{t=1}^{\tau^\star_K}\gamma^{t-1} X_{i,t} +  \gamma^{\tau^\star_K}  \frac{R_{\lambda,K}(\tau^\star_K, y_{i,\tau^\star_K-1})}{1-\gamma}\right] \label{eqn:proof_lem_2_tower_prop_yet_again} \\
	& \le \sup_{1 \le \tau \le K}  \E_{y}\left[\sum_{t=1}^{\tau}\gamma^{t-1} X_{i,t} +  \gamma^{\tau}  \frac{R_{\lambda,K}(\tau, y_{i,\tau-1})}{1-\gamma}\right] \nonumber \\
	& = V^K_{\gamma}(y, \lambda)
	\end{align}
	where Equations \eqref{eqn:proof_lem_2_tower_prop_again}, \eqref{eqn:proof_lem_2_tower_prop_yet_again} use the tower property and \eqref{eqn:proof_lem2_using_the_main_argument} is immediately derived by using the bound of \eqref{ineq:proof_lem_2_main_bound_in_proof}. Finally, we note that an almost identical proof can be given to show that $V^K_\gamma(y, \lambda) \ge V_\gamma(y, \lambda)$ where the lower bound is the continuation value used to compute the Gittins index.
	
	We have shown that for any $\lambda$ and $y$,  that $V^K_\gamma(y, \lambda)$ is non-increasing in $K$, and that $V_\gamma(y, \lambda)$ is a lower bound to this sequence. We make use of these facts to now prove that $v^K_\gamma(y)$ is also non-increasing in $K$. To this end, let us suppose for contradiction that there exist two integers $K_1 \le K_2$ and $v^{K_1}_\gamma(y) < v^{K_2}_\gamma(y)$. From Lemma~\ref{cor:equivalent_event} we know that
	\begin{equation}
		V^{K_2}_\gamma(y, v^K_\gamma(y)) > v^K_\gamma(y)/(1-\gamma) = V^K_\gamma(y, v^K_\gamma(y)),
	\end{equation}
	which contradicts the claim just shown. Therefore, $v^K_\gamma(y)$ must also be a  non-increasing sequence in $K$. The same argument can be used to further show that $v^K_\gamma(y) \ge v_\gamma(y)$.
	
	We now turn our attention to proving the convergence property stated in the Lemma. The first step will be to prove that for all $y \in \mathcal{Y}$ and $\lambda \in \Re_+$, that 
	\begin{equation} \label{eqn:proof_vk_bound_convergence_of_continuation_value}
	\lim_{K \to \infty}V^K_\gamma(y, \lambda) = V_\gamma(y, \lambda).
	\end{equation}
	Indeed, we upper bound the optimistic continuation value for a fixed parameter $M$ as follows:
	\begin{align}
		V^M_\gamma(y, \lambda) & = \sup_{1 \le \tau \le M} \E_y\left[\sum_{t=1}^{\tau} \gamma^{t-1} X_{i,t} + \frac{\gamma^{\tau}R_{\lambda,M}(\tau, y_{i,\tau-1})}{1-\gamma}\right] \nonumber\\
		& = \sup_{1 \le \tau \le M} \E_y\left[\sum_{t=1}^{\tau} \gamma^{t-1} X_{i,t} + \frac{\gamma^{\tau}\lambda}{1-\gamma} + \frac{\gamma^{\tau}R_{\lambda,M}(\tau, y_{i,\tau-1})}{1-\gamma} - \frac{\gamma^{\tau}\lambda}{1-\gamma}\right] \nonumber \\
		& \le \sup_{\tau \ge 1} \E_y\left[\sum_{t=1}^{\tau } \gamma^{t-1} X_{i,t} + \frac{\gamma^{\tau}\lambda}{1-\gamma}\right] + \sup_{1 \le \tau \le M}\E_y\left[\frac{\gamma^{\tau}R_{\lambda,M}(\tau, y_{i,\tau-1})}{1-\gamma} - \frac{\gamma^{\tau -1}\lambda}{1-\gamma}\right] \nonumber \\
		& =V_{\gamma}(y, \lambda) + \sup_{1 \le \tau \le M}\E_y\left[\frac{\gamma^{\tau}[R_{\lambda,M}(\tau, y_{i,\tau-1})-\lambda]}{1-\gamma}\right] \nonumber \\
		& \le V_{\gamma}(y, \lambda)  + \gamma^{M}\E_y\left[\frac{R_{\lambda,M}(M, y_{i,M-1})-\lambda}{1-\gamma}\right] \nonumber \\
		& = V_{\gamma}(y, \lambda)  + \gamma^{M}\E_y\left[\frac{( R(y_{i,M-1}) - \lambda)^+}{1-\gamma}\right] \nonumber \\
		& \le V_{\gamma}(y, \lambda)  + \gamma^{M}\E_y\left[\frac{|R(y_{i,M-1})|}{1-\gamma}\right] \nonumber  \\
		& = V_{\gamma}(y, \lambda)  + \gamma^M \E_y\left[\frac{|\mu(\theta_i)|}{1-\gamma}\right] \label{eqn:proof_lem_vk_bound_iter_exp},
	\end{align}
	where equation \eqref{eqn:proof_lem_vk_bound_iter_exp} follows from the definition of the random variable $R(.)$ and the law of iterated expectation. Now because $0 < \gamma < 1$ and $\E_y\left[|\mu(\theta_i)|\right] < \infty$, the right hand side above converges to $V_{\gamma}(y, \lambda)$. Finally, notice that $V^M_\gamma(y, \lambda) \ge V_\gamma(y, \lambda)$, and from this equation~\eqref{eqn:proof_vk_bound_convergence_of_continuation_value} follows. 
	
	{\color{blue}
	To finish the proof, we consider the sequence of fixed points of the equations $\lambda = V^K_\gamma(y, \lambda)$, $\{v^K_\gamma(y)\}$. 
	Because this sequence is monotone (established in the first part of this proof) and bounded, we know that this sequence, has a limit; $v^K_\gamma(y) \tends \hat v_\gamma(y)$. 	
%	there exists a convergent sub-sequence $\{ \lambda_k \}$ whose elements we index with $k$. Let $\hat \lambda$ denote the limit of this sub-sequence. We will prove that this limit is the Gittins index. 

It remains to show that $\hat v_\gamma(y) = V_\gamma(y, \hat v_\gamma(y))$. For this it suffices to show that $v^K_\gamma(y) \tends V_\gamma(y, \hat v_\gamma(y))$, which we establish as follows: 
%	
%	Indeed, we can bound the difference between each element $\lambda_k$ and $V_\gamma(y, \hat \lambda)$ as follows
	\begin{align*}
	|v^K_\gamma(y) - V_\gamma(y, \hat v_\gamma(y))| &  = |V^K_\gamma(y, v^K_\gamma(y)) - V_\gamma(y, \hat v_\gamma(y)) | \\
	&  \leq \underbrace{|V^K_\gamma(y, v^K_\gamma(y)) - V^K_\gamma(y, \hat v_\gamma(y))|}_{=: a_k} + \underbrace{|V^K_\gamma(y, \hat v_\gamma(y)) - V_\gamma(y, \hat v_\gamma(y))|}_{=:b_k}.
	\end{align*}
	We already proved \eqref{eqn:proof_vk_bound_convergence_of_continuation_value} and therefore know that $b_k \to 0$ as $k \to \infty$. As for the $a_k$ sequence, we have
	\begin{align}
	a_k & = |V^K_\gamma(y, v^K_\gamma(y)) - V^K_\gamma(y, \hat v_\gamma(y))| \nonumber \\
	& \leq | v^K_\gamma(y) - \hat v_\gamma(y) | \label{eqn:use_of_lipshitz_bound}\\
	&  \to 0 \nonumber
	\end{align}
	where \eqref{eqn:use_of_lipshitz_bound} follows from the Lipschitz continuity of $V^K_\gamma(\cdot, \cdot)$ in its second argument as shown in Fact~\ref{fact:v_is_convex}. Therefore $v^K_\gamma(y) \tends V_\gamma(y, \hat v_\gamma(y))$, which completes the proof. 	
%	$\lambda_k \to V_\gamma(y, \hat \lambda)$, which implies that $\hat \lambda = V_\gamma(y, \hat \lambda)$. This proves that $\hat \lambda$ is a fixed point of the function $V_\gamma(y, .)$, namely the Gittins index.
	}
\end{myproof}


The next Lemma will be the final property of the function $V^K_\gamma$ that we prove. This will subsequently be used in the proof of Lemma~\ref{lemma:underestimation}.
\begin{lemma} \label{lemma:vk_bound}
	Let $i$ be any arm. For any look-ahead parameter $K \in \mathbb{Z}_+$, discount factor $\gamma$ and any constant $\eta$, we have
	\begin{equation*}
	\E_y\left[V^K_\gamma(y_{i,1}, \eta) \right] \ge V^K_\gamma(y, \eta)
	\end{equation*}
	where we recall that $y_{i,1}$ is the summary statistic corresponding to the posterior obtained from pulling arm $i$ once.
\end{lemma}
\begin{myproof}[Proof.]
	For any $\hat y \in \mathcal{Y}$, let $\tau^\star(\hat y)$ be the (predictable) optimal stopping time for the problem (involving computing $V^K_\gamma$) whose initial state is $y_{i,0} = \hat y$. With this notation in hand, we conclude that
	\begin{align}
	\E_y \left[V^K_\gamma(y_{i,1}, \eta) \right] & = \E_y\left[\E_{y_{i,1}}\left[\sum_{s=1}^{\tau^\star(y_{i,1})} \gamma^{s-1} X_{i,s} + \frac{\gamma^{\tau^\star(y_{i,1})} R_{\eta, K}(\tau, y_{i,\tau^\star(y_{i,1})-1})}{1-\gamma} \right]\right]\label{eqn:pf_vk_bound_tower_prop1}  \\
	& \ge  \E_y\left[\E_{y_{i,2}}\left[\sum_{s=1}^{\tau^\star(y)} \gamma^{s-1} X_{i,s} + \frac{\gamma^{\tau^\star(y)} R_{\eta, K}(\tau, y_{i,\tau^\star(y)-1})}{1-\gamma}\right] \right] \label{ineq:subopt_of_y}\\
	& = \E_y\left[\sum_{s=1}^{\tau^\star(y)} \gamma^{s-1} X_{i,s} + \frac{\gamma^{\tau^\star(y)} R_{\eta, K}(\tau, y_{i,\tau^\star(y)-1})}{1-\gamma}\right] \label{eqn:pf_vk_bound_tower_prop2} \\
	& = V^K_\gamma(y, \eta) \nonumber
	\end{align}
	where \eqref{eqn:pf_vk_bound_tower_prop1}, \eqref{eqn:pf_vk_bound_tower_prop2} both follow from the tower property and \eqref{ineq:subopt_of_y} is due to the sub-optimality of the stopping rule $\tau^\star(y)$ when the actual starting state is $y_{i,1}$. Intuitively, we lose out revenue by throwing away information about the arm.
\end{myproof}


\section{Results for the frequentist regret bound}
This section contains proofs of results required to show Theorem~\ref{thm:frequentist_optimal_bound}. It is helpful to go over the definitions and some general properties of the Optimistic Gittins index given in Section~\ref{sec:appendix_properties_of_ogi} when reading this.
\subsection{Definitions and properties of Binomial distributions.}
We list notation and facts related to Beta and Binomial distributions, which are used through this section.
\begin{definition}
	$F^B_{n,p}(.)$ is the CDF of the Binomial distribution with parameters $n$ and $p$, and $F^\beta_{a,b}(.)$ is the CDF of the Beta distribution with parameters $a$ and $b$.
\end{definition}

\begin{lemma} \label{fact:equation_for_beta_binomial_cdfs}
	Let $a$ and $b$ be positive integers and $y \in [0,1]$, 
	\[
	F^\beta_{a,b}(y) = 1 - F^B_{a+b-1,y}(a-1)
	\]
\end{lemma}
\begin{myproof}[Proof.]
	Proof is found in \cite{agrawalanalysis}.
\end{myproof}
\begin{lemma} \label{fact:median_of_binomial_dist}
	The median of a Binomial$(n,p)$ distribution is either $\ceil{np}$ or $\floor{np}$.
\end{lemma}
\begin{myproof}[Proof]
	A proof of this fact can be found in \cite{jogdeo1968monotone}.
\end{myproof}

\begin{corollary}[Corollary of Fact~\ref{fact:median_of_binomial_dist}] \label{cor:corollarly_of_binomial_median_property}
	Let $n$ be a positive integer and $p \in (0,1)$. For any non-negative integer $s < np$
	\[
	F^B_{n,p}(s) \le 1/2
	\]
\end{corollary}

\begin{lemma} \label{fact:relationship_with_binom_cdfs}
	Let $n$ be a positive integer and $p \in [0,1]$. Then for any $k \in \{0,\ldots,n\}$,
	\[
	(1-p)F^B_{n-1,p}(k)\le F^B_{n,p}(k) \le F^B_{n-1,p}(k)
	\] 
\end{lemma}
\begin{myproof}[Proof]
	To prove $F^B_{n,p}(k) \le F^B_{n-1,p}(k)$, we let $X_1,\ldots,X_{n}$ be i.i.d samples from a Bernoulli($p$) distribution. We then have
	\begin{align*}
	F^B_{n,p}(k)  = \P{\sum_{i=1}^{n} X_i \le k}  \le  \P{\sum_{i=1}^{n-1} X_i \le k}  = F^B_{n-1,p}(k)
	\end{align*}
	Now to prove $(1-p)F^B_{n-1,p}(k)\le F^B_{n,p}(k)$, it's enough to observe that $F^B_{n,p}(k) = p F^B_{n-1,p}(k-1) + (1-p) F^B_{n-1,p}(k)$.
\end{myproof}

\subsubsection{Ratio of Binomial CDFs.} \label{sec:ratio_of_bin_cdfs}
\begin{lemma} \label{lemma:ratio_of_cdfs}
	Let $0< q < p < 1$. Let $n$ be a positive integer such that $e^{\frac{n}{2} d(q,p)} \ge (n+1)^4$ and let $k$ be a non-negative integer such that $k < nq$. It then follows that
	\[
	F^B_{n,q}(k)/F^B_{n,p}(k) >  e^{\frac{n}{2} d(q,p)}.
	\]
\end{lemma}
\begin{proof}[Proof.]
	From the method of types  (see \cite{cover2012elements}), we have for any $r \in (0,1)$ and $j < nr$
	\begin{equation} \label{eqn:appl_of_sanovs}
	\frac{e^{-nd(j/n, r)}}{(1+n)^2}\le F^B_{n,r}(j) \le (n+1)^2 e^{- n d(j/n, r)}.
	\end{equation}
	Because $k < nq < np$, by applying \eqref{eqn:appl_of_sanovs} to both the numerator and denominator, we get
	\begin{align*}
	\frac{F^B_{n,q}(k)}{F^B_{n,p}(k)} & \ge  \frac{e^{-nd(k/n, q)}}{(n+1)^4 e^{- n d(k/n, p)}} = \frac{e^{n(d(k/n,p) - d(k/n,q))}}{(n+1)^4}.
	\end{align*}
	Examining the exponent, we find
	\begin{align*}
	d(k/n, p) - d(k/n,q) & = \frac{k}{n} \log \frac{q}{p} + \left(1-\frac{k}{n}\right)\log \frac{1-q}{1-p} \\
	& > q \log \frac{q}{p} + (1-q)\log \frac{1-q}{1-p} \\
	& = d(q,p)
	\end{align*}
	where the bound holds because the expression is decreasing in $k$, and $k < nq$. Therefore,
	\begin{align}
	\frac{F^B_{n,q}(k)}{F^B_{n,p}(k)} & > \frac{e^{n  d(q,p)}}{(n+1)^4} = \frac{e^{\frac{n}{2}d(q,p)}}{(n+1)^4} e^{\frac{n}{2}d(q,p)} \ge e^{\frac{n}{2}d(q,p)} \label{bound:log_1minusq_etc}.
	\end{align}
	The final lower bound in \eqref{bound:log_1minusq_etc} follows from the assumption on $n$.
\end{proof}

\subsection{Proof of Lemma~\ref{lemma:underestimation}} \label{proof:underestimation_proof}
\begin{myproof}[Proof.]
	The proof hinges on showing that for any $K$, which is the number of look-ahead steps used to compute the Optimistic Gittins index, that
	\begin{equation} \label{eqn:big_oh_result_for_ogi}
	\P{v^K_{1,t} < \eta} = O\left(\frac{1}{t^{1 + h_\eta}}\right)
	\end{equation}
	where $h_\eta > 0$ is some constant that depends on $\eta$. After showing the above statement, the result would follow due to convergence of the series $\sum_{t=1}^\infty \P{v^K_{1,t} < \eta}$. The first step will be to show that for any $K \ge 1$ and any $\zeta \ge 0$, that there exists $h'_\eta > 0$, such that
	\begin{equation} \label{eqn:big_oh_result_for_vk}
		\P{(1-\gamma_t)V^K_{\gamma_t}(y_{1,\Nt{1}}, \eta) < \eta + \zeta/t} = O_{\eta,\zeta}\left(\frac{1}{t^{1 + h'_\eta}}\right),
	\end{equation}
	where $V^K_{\gamma_t}$ is the continuation value defined in Section~\ref{sec:appendix_properties_of_ogi} and $O_{\eta,\zeta}$ means that the constant in front the big-Oh depends on both $\zeta$ and $\eta$. After showing the above claim, Lemma~\ref{cor:equivalent_event} would imply Equation~\eqref{eqn:big_oh_result_for_ogi} because we know from that result that,
	\begin{align*}
		\P{v^K_{1,t} < \eta} & = \P{(1-\gamma_t)V^K_{\gamma_t}(y_{1,\Nt{1}}, \eta) < \eta} \\
		& = O\left(\frac{1}{t^{1 + h_\eta}}\right)
	\end{align*}
	for some $h_\eta > 0$. The second equation above is just a special case of \eqref{eqn:big_oh_result_for_vk} when $\zeta = 0$.
	
	Ultimately, showing equation \eqref{eqn:big_oh_result_for_vk}, and thus proving the Lemma, is an induction over the parameter $K$ and we begin with the base case, which requires some work using properties of the Beta and Binomial distributions.
	\subsubsection*{Proof of the base case}
	Let us fix $\zeta \ge 0$. We prove that when the algorithm uses a look-ahead parameter of $K = 1$, that there exists a positive constant $h_\eta$ such that
	 \begin{equation} \label{eqn:base_case_lemma_underestimation}
	 \P{(1-\gamma_t)V^1_{\gamma_t}(y_{1,\Nt{1}}, \eta) < \eta + \zeta/t} = O_{\eta,\zeta}\left(\frac{1}{t^{1 + h_\eta}}\right).
	 \end{equation}
	 %To simplify notation, let us abbreviate $v^1_{1,t}$ as $v_{1,t}$. 
	 First, we define $\delta := (\theta_1 - \eta)/2$ and  $\eta' :=  \eta + \delta$. In other words, $\delta$ is half the distance between $\eta$ and $\theta_1$; $\eta'$ is the point half-way. Recall that $\Nt{i}$ refers to the counting process for the number of pulls of arm $i$ up to \emph{but not including} time $t$ and that $S_i(t)$ is the corresponding total reward (or number of successes from all the Bernoulli trials). Showing this base case consists of showing two claims:
	\subsubsection*{Claim 1: $\{(1-\gamma_t)V^1_{\gamma_t}(y_{1,\Nt{1}}, \eta) < \eta + \zeta/t\} \subseteq \left\{F^B_{\Nt{1}+1, \eta'}(S_1(t)) < \frac{\zeta + 1}{\delta t}\right\}$}
	Let $V_t \sim $Beta$(S_1(t)+1,\Nt{1} - S_1(t) + 1)$ be the agent's posterior on the expected reward from the optimal arm (notice that $y_{1,\Nt{1}} = (S_1(t)+1,\Nt{1} - S_1(t) + 1)$ in this case). Using the simplified equation for the continuation value when $K =1$, namely $V^1_{\gamma_t}$ (see Equation~\eqref{eqn:ogi_k1}), 
	\[
		(1-\gamma_t)V^1_{\gamma_t}\left((S_1(t)+1, \Nt{1} - S_1(t) + 1), \eta\right) = \Ee{V_t} + \gamma_t\Ee{(\eta - V_t)^+},
	\] 
	we find that
	\begin{align}
	\left\{(1-\gamma_t)V^1_{\gamma_t}(y_{1,N_{1}(t)}, \eta) < \eta + \frac{\zeta}{t}\right\} & = \left\{ \Ee{V_t } + \gamma_t\Ee{(\eta - V_t)^+} < \eta + \frac{\zeta}{t}\right\} \nonumber\\
	& =  \left\{ (1-1/t)\Ee{(\eta - V_t)^+} < \Ee{\eta - V_t} +\frac{\zeta}{t} \right\} \label{eq:def_gamma_t} \\
	& =  \left\{ \Ee{(\eta - V_t)^+} - \Ee{\eta - V_t} <  \frac{1}{t}\Ee{(\eta - V_t)^+} +\frac{\zeta}{t}\right\} \nonumber\\
	& =  \left\{ \Ee{(V_t - \eta)^+}<  \frac{1}{t}\Ee{(\eta - V_t)^+} +\frac{\zeta}{t}\right\} \nonumber\\
	& \subseteq \left\{ \Ee{ (V_t - \eta)^+}< \frac{\zeta + 1}{t}  \right\} \label{eq:intermediate_event}
	\end{align}
	where \eqref{eq:def_gamma_t} follows from the definition of $\gamma_t$ and \eqref{eq:intermediate_event} is due to $V_t, \eta$ both lying in the interval $[0,1]$. We approximate the conditional expectation in \eqref{eq:intermediate_event} with the following bound:
	\begin{align}
	\Ee{(V_t - \eta)^+ } & = \Ee{(V_t - \eta) \ind{V_t \ge \eta} }\nonumber \\
	& = \Ee{(V_t - \eta) \ind{\eta + \delta > V_t \ge \eta} }  \nonumber \\
	& \qquad + \Ee{(V_t - \eta) \ind{ V_t \ge \eta + \delta} } \nonumber \\
	& > \Ee{(V_t - \eta) \ind{ V_t \ge \eta + \delta} } \nonumber \\
	& \ge \delta\P{ V_t \ge \eta' } \nonumber \\
	& = \delta (1 - F^\beta_{S_1(t)+1,\Nt{1}-S_1(t)+1}(\eta'))  \nonumber\\ 
	& = \delta F^B_{\Nt{1}+1,\eta'}(S_1(t)) \label{ineq:lower_bound_on_ppart_term}
	\end{align}
	where the final equality is due to Fact~\ref{fact:equation_for_beta_binomial_cdfs}. The claim then follows from the above bound and \eqref{eq:intermediate_event}. We proceed with the second part of the base case's proof:
	\subsubsection*{Claim 2: $\mathbb{P}\left(F^B_{\Nt{1}+1, \eta'}(S_1(t)) < \frac{\zeta + 1}{\delta t}\right) = O\left(\frac{1}{t^{1 + h_\eta}}\right)$ for some $h_\eta > 0$}
	Let us fix the sequence $f_t \defeq -\frac{\log (\delta t/(\zeta+1)) }{\log (1-\eta')}-1 = O(\log t)$. We then have by a straightforward decomposition that
	\begin{align}
	\P{F^B_{\Nt{1}+1, \eta'}(S_1(t)) < \frac{\zeta + 1}{\delta t}} & = \P{F^B_{\Nt{1}+1, \eta'}(S_1(t)) < \frac{\zeta + 1}{\delta t}, \; \Nt{1} > f_t}  \nonumber \\
	& \qquad + \P{F^B_{\Nt{1}+1, \eta'}(S_1(t)) < \frac{\zeta + 1}{\delta t}, \; \Nt{1} \le f_t} \label{eq:decomp2}.
	\end{align}
	Then notice that for the second term in the RHS of \eqref{eq:decomp2} we have the following bound,
	\begin{align}
	\P{F^B_{\Nt{1}+1, \eta'}(S_1(t)) < \frac{\zeta + 1}{\delta t}, \; \Nt{1} \le f_t}  &  \le \P{F^B_{\Nt{1}+1,\eta'}(0) < \frac{\zeta + 1}{\delta  t}, \; \Nt{1} \le f_t} \nonumber \\
	& = \P{(1-\eta')^{\Nt{1}+1} <  \frac{\zeta + 1}{\delta  t}, \; \Nt{1} \le f_t} \nonumber \\
	& \le \P{(1-\eta')^{f_t+1} <  \frac{\zeta + 1}{\delta  t}} \nonumber \\
	& = 0. \label{bound:bdd_by_zero}
	\end{align}
	Now we use the following fact to correspondingly bound the left term on the RHS of \eqref{eq:decomp2}. Define the function
	\[
	F^{-B}_{n,p}(u) := \inf\{x : F^B_{n,p}(x) \ge u\}
	\]
	which is the inverse CDF. Then it is known that if $U \sim \text{Uniform}(0,1)$, then $F^{-B}_{n,p}(U) \sim \text{Binomial}(n,p)$. Furthermore, the event $F^B_{n,p}(F^{-B}_{n,p}(U)) \ge U$ occurs with probability 1 due to the definition of the inverse CDF.
	
	Now let us only consider large $t$, in particular $t > M = M(\theta_1, \eta')$ where:
	\begin{enumerate}
		\item $M$ is such that $e^{d(\eta', \theta_1)f_{M}/2} > (f_M + 1)^4$ (we need this condition when we use Lemma~\ref{lemma:ratio_of_cdfs})
		\item $M > \frac{4(\zeta + 1)}{(1-\eta')\delta }$
		\item $\ceil{f_M} > 0$ and $F^B_{t',\eta'}(t' \eta') > 1/4$ for all $t' > \ceil{f_M}$. Note that there is a large enough integer for this because $F^B_{\ceil{f_t},\eta'}(f_t \eta') \to \frac{1}{2}$ as $t \to \infty$.
	\end{enumerate} 
	Suppose that $t > M$. It then follows that the event \[\left\{F^B_{\Nt{1}, \eta'}(S_1(t)) < \frac{\zeta + 1}{(1-\eta')\delta t},\; S_1(t) \ge \Nt{1} \eta', \; \Nt{1} > f_t\right\}\] has measure zero because of the assumptions made on $M$. Therefore if $t > M$, we have
	\begin{align}
	\mathbb{P}\bigg(F^B_{\Nt{1}+1,  \eta'}(S_1(t)) &< \frac{\zeta + 1}{\delta t }  , \; \Nt{1} > f_t \bigg) \nonumber \\
	& \le \P{F^B_{\Nt{1},  \eta'}(S_1(t)) < \frac{\zeta + 1}{(1-\eta')\delta t}, \; \Nt{1} > f_t} \label{eqn:part1_decomp_the_cdf_of_y} \\ 
	& = \P{F^B_{\Nt{1},  \eta'}(S_1(t)) < \frac{\zeta + 1}{(1-\eta')\delta t}, \; S_1(t) < \Nt{1} \eta', \; \Nt{1} > f_t} \nonumber \\ 
	& =  \P{F^B_{\Nt{1},\theta_1}(S_1(t)) \frac{F^B_{\Nt{1},\eta'}(S_1(t))}{F^B_{\Nt{1},\theta_1}(S_1(t))} < \frac{\zeta + 1}{(1-\eta')\delta  t}, \;S_1(t) < \Nt{1} \eta', \; \Nt{1} > f_t} \nonumber \\
	& \le  \P{F_{\Nt{1},\theta_1}^B(S_1(t))  e^{\Nt{1} D} < \frac{\zeta + 1}{(1-\eta')\delta  t} , \; \Nt{1} > f_t} \label{eqn:part1_app_of_lemma2} \\
	& \le  \P{F_{\Nt{1},\theta_1}^B(S_1(t)) e^{f_t D} < \frac{\zeta + 1}{(1-\eta')\delta  t}} \nonumber \\
	& =  \P{F_{\Nt{1},\theta_1}^B(F^{-B}_{\Nt{1},\theta_1}(U)) < \frac{e^{-f_t D}(\zeta + 1)}{(1-\eta')\delta  t} } \label{eqn:part1_propert_of_inverse_sampling}\\
	& \le  \P{U < \frac{e^{-f_t D}(\zeta + 1)}{(1-\eta')\delta  t} } \nonumber \\  
	& =  \frac{e^{-f_t D}(\zeta + 1)}{(1-\eta')\delta  t} \nonumber  \nonumber\\
	& = \mathcal O_{\eta, \zeta}\left( \frac{1}{t^{1+Dc_{\eta'}}} \right)  \label{bound:one_over_t_plus_eps} 
	\end{align}
	where $D = d(\eta',\theta_1) > 0$ and $c_{\eta'} = -\log^{-1}(1-\eta') > 0$ are constant. The bound \eqref{eqn:part1_decomp_the_cdf_of_y} holds due to Fact~\eqref{fact:relationship_with_binom_cdfs}. Bound \eqref{eqn:part1_app_of_lemma2} follows from an application of Lemma~\ref{lemma:ratio_of_cdfs} and the fact that $t > M$. Equation \eqref{eqn:part1_propert_of_inverse_sampling} follows from $S_1(t) \sim \text{Binomial}(\Nt{1}, \theta_1)$ and the inverse sampling technique. By combining bounds \eqref{bound:one_over_t_plus_eps}, \eqref{bound:bdd_by_zero} and \eqref{eq:decomp2}, we finally obtain the result for the base case by taking $h_\eta = Dc_{\eta'}$.

	\subsubsection*{Proof of the inductive step}
	 Now, suppose that for $K-1 \ge 1$ and any $\zeta \ge 0$,  the following induction hypothesis holds
	\[
	\P{(1-\gamma_t)V^{K-1}_{\gamma_t}(y_{1,\Nt{1}}, \eta) < \eta + \frac{\zeta}{t}} = O_{\eta,\zeta}\left(\frac{1}{t^{1 + h_\eta}}\right)
	\]
	for some $h_\eta > 0$. We prove the same result for the next integer $K$. Observe that when $t$ is large enough, using the Bellman equation for $V^K_\gamma$, we have
	\begin{align}
	&\P{(1-\gamma_t)V^K_{\gamma_t}(y_{1,\Nt{1}}, \eta) < \eta + \frac{\zeta}{t}} \nonumber  \\
	&\qquad = \mathbb{P}\left((1-\gamma_t)\Ee{X_{1,t} \given y_{1,\Nt{1}}}  \right. \nonumber\\
	&\hspace{6em} + \gamma_t \Ee{\max(\eta, (1-\gamma_t)V^{K-1}_{\gamma_t}(y_{1,\Nt{1}+1}, \eta)) \given y_{1,\Nt{1}}} < \eta + \frac{\zeta}{t}\bigg) \label{eq:appl_of_lemma_9} \\
	& \qquad \le \P{ \left(1-\frac{1}{t}\right) \Ee{(1-\gamma_t)V^{K-1}_{\gamma_{t}}(y_{1,\Nt{1}+1}, \eta) \given y_{1,\Nt{1}} }< \eta + \frac{\zeta}{t}} \nonumber \\
	& \qquad \le \P{\left(1-\frac{1}{t}\right)(1-\gamma_t) V^{K-1}_{\gamma_{t}}(y_{1,\Nt{1}}, \eta)< \eta  + \frac{\zeta}{t}} \label{ineq:missing_step} \\
	& \qquad \le \P{ (1-\gamma_t)V^{K-1}_{\gamma_{t}}(y_{1,\Nt{1}}, \eta)< \frac{t}{t-1}\left(\eta +  \frac{\zeta}{t}}\right) \nonumber \\
	& \qquad \le \P{ (1-\gamma_t)V^{K-1}_{\gamma_{t}}(y_{1,\Nt{1}}, \eta)< \eta + \frac{\eta}{t-1} +  \frac{\zeta}{t-1}} \nonumber \\
	& \qquad \le \P{ (1-\gamma_t)V^{K-1}_{\gamma_{t}}(y_{1,\Nt{1}}, \eta)< \eta +   \frac{\zeta + 1}{t}} \nonumber \\
	&\qquad =  O_{\eta,\zeta}\left(\frac{1}{t^{1 + h_\eta}}\right) \label{eqn:ind_hyp}
	\end{align}
	where the final inequality holds when $t$ is large enough because $\eta < 1$, equation \eqref{eq:appl_of_lemma_9} results from an expansion of Bellman's equation and bound \eqref{ineq:missing_step} follows from Lemma~\ref{lemma:vk_bound}. %\eqref{eqn:another_appl_of_lemma_9} follows from both \eqref{bnd:conc_result} and Lemma~\ref{cor:equivalent_event}. 
	Finally, equation \eqref{eqn:ind_hyp} follows from the induction hypothesis.
	%We finish the proof by using the following asymptotic argument. Take $M$ to be a large enough integer, then we have, using the result of the preceding induction proof, that
	%\begin{align*}
	%\sum_{t=1}^\infty \P{v^K_{1,t} < \eta} & \le M +  \sum_{t=M+1}^\infty \frac{C_1}{t^{1 + h(\eta)}} \le M +  C_2
	%\end{align*}
	%where $C_2 = C_2(\eta)$ is the limit of the series and $C_1$ is a constant used in the definition of the big-Oh.
\end{myproof}

\subsection{Proof of Lemma~\ref{lemma:overestimation}} \label{proof:overestimation_proof}

\begin{proof}[Proof.]
	See the main proof of Theorem~\ref{thm:frequentist_optimal_bound} to recall the definition of constants $\eta_1$, $\eta_3$ and their relationship with $\theta_2$ and $\theta_1$. As an abbreviation we let $L = L(T)$. Moreover, because for any arm $i$ $v^K_{i,t} \le v^{K-1}_{i,t} \le \ldots \le v^1_{i,t}$ (Lemma~\ref{lemma:approx_bound}), it will be sufficient to consider this proof only for $v^1_{2,t}$, which we also will abbreviate as $v_{2,t} \defeq v^1_{2,t}$. Similarly, we will abbreviate the notation for the OGI policy as $\pi^{OG}$ and suppress the parameter $K$.
	
	Firstly, by the law of total probability and the definition of $P_i(t)$ in Section~\ref{sec:appendix_properties_of_ogi}, we find that
	\begin{align} 
	\sum_{t=1}^T \mathbb{P}(v_{2,t} & \ge \eta_3 ,\; N_{2}(t-1) \ge L,\; \pi^{\rm OG}_t = 2) \nonumber \\
	& = \sum_{t=1}^T \P{v_{2,t} \ge \eta_3 ,\; \Nt{2} \ge L, \; S_2(t) < \floor{\Nt{2} \eta_1}, \; \pi^{\rm OG}_t = 2} \nonumber \\
	& \qquad + \sum_{t=1}^T \P{v_{2,t} \ge \eta_3 ,\; \Nt{2} \ge L, \; S_2(t) \ge \floor{\Nt{2} \eta_1},\; \pi^{\rm OG}_t = 2} \nonumber \\
	& \le \sum_{t=1}^T \P{v_{2,t} \ge \eta_3 ,\; \Nt{2} \ge L, \; S_2(t) < \floor{\Nt{2} \eta_1}} + \sum_{t=1}^T \P{\pi^{\rm OG}_t = 2,\; S_2(t) \ge \floor{\Nt{2} \eta_1}} \label{eqn:splitting_not_underestimate},
	\end{align}
	where $S_2(t)$ is also defined in Section~\ref{sec:appendix_properties_of_ogi} as the total reward from the second arm observed up to time $t-1$. Let $V_t \sim \text{Beta}(S_2(t) + 1, \Nt{2}- S_2(t) + 1)$ denote the agent's posterior on the second arm at time $t$, then
	\begin{align}
	\sum_{t=1}^T \mathbb{P}(v_{2,t} \ge \eta_3 ,\; & \; \Nt{2} \ge L,\; S_2(t) < \floor{\Nt{2} \eta_1})  \nonumber\\
	& = \sum_{t=1}^T \P{\Ee{V_t} + \gamma_t \Ee{(\eta_3 - V_t)^+} \ge \eta_3, \; \Nt{2} \ge L,\; S_2(t) < \floor{\Nt{2} \eta_1}} \nonumber \\
	& = \sum_{t=1}^T \P{\frac{\Ee{(\eta_3-V_t)^+ }}{  \Ee{(V_t - \eta_3)^+ }} \le t , \; \Nt{2} \ge L,\; S_2(t) < \floor{\Nt{2} \eta_1}} \label{eq:complicated_rv_in_part2}
	\end{align}
	where the first equality follows from Lemma~\ref{cor:equivalent_event} and the simplified form of the continuation value (defined in Section~\ref{sec:appendix_properties_of_ogi}) when $K = 1$. The following result lets us bound \eqref{eq:complicated_rv_in_part2},
	\begin{lemma} \label{lem:lb_rv2}
		Let $0 < x < y < 1$. For any non-negative integers $s$ and $k$ with $s < \floor{kx}$, it holds that
		\begin{equation*}
		\frac{\Ee{(y-V)^+ }}{  \Ee{(V - y)^+ } } \ge \frac{(y-x) \exp(k d(x,y))}{2}
		\end{equation*}
		where $V \sim \text{Beta}(s+1,k-s+1)$.
	\end{lemma}
	\begin{myproof}[Proof.]
		See Appendix~\ref{prf:proof_of_lb_rv2}.
	\end{myproof}
	Therefore, from equation \eqref{eq:complicated_rv_in_part2} and Lemma~\ref{lem:lb_rv2}, we find that whenever $T > \left(\frac{2}{\eta_3-\eta_1}\right)^{1/\eps} =: T^*(\eps, \theta)$,
	\begin{align}
	\sum_{t=1}^T \mathbb{P}(v_{2,t} \ge \eta_3 ,\; & \; \Nt{2} \ge L,\; S_2(t) < \floor{\Nt{2} \eta_1}) \nonumber \\
	& \le  \sum_{t=1}^T\P{  (\eta_3-\eta_1) \exp\{\Nt{2} d(\eta_1,\eta_3) \} \le 2t,\; \Nt{2} \ge L} \nonumber \\
	& \le  \sum_{t=1}^T\P{  (\eta_3-\eta_1) \exp\{L d(\eta_1,\eta_3) \} \le 2t} \nonumber \\
	& =   \sum_{t=1}^T\P{  (\eta_3-\eta_1) T^{1+\eps} \le 2t} \nonumber \\
	& = 0. \label{bound:equal_to_zero}
	\end{align}
	All that is left is to bound the second term in \eqref{eqn:splitting_not_underestimate}, and to do so we apply the following Lemma whose proof is in Appendix~\ref{prf:proof_of_acc_sub_means}
	\begin{lemma} \label{lem:accurate_suboptimal_mean}
		There exist positive constants $C = C(\theta_2,\eta_1)$ and $x' > \theta_2$ such that
		\begin{equation*}
		\sum_{t=1}^T \P{S_2(t) \ge \floor{\Nt{2} \eta_1}, \; \pi^{\rm OG}_t = 2} \le  K + \frac{1}{1 - e^{-d(x',\theta_2)}} 
		\end{equation*}
	\end{lemma}
	Combining Lemma~\ref{lem:accurate_suboptimal_mean}, \eqref{bound:equal_to_zero}, \eqref{eqn:splitting_not_underestimate} and \eqref{eq:complicated_rv_in_part2} shows the claim.
\end{proof}

\subsubsection{Proof of Lemma~\ref{lem:lb_rv2}.} \label{prf:proof_of_lb_rv2}
\begin{myproof}[Proof.]
	We upper bound the denominator as follows. Given that $s < \floor{k x}$, we have $s \le kx - 1$. Let $B(a,b)$ denote the Beta function for parameters $a, b > 0$, that is
	\[
	B(a, b) \defeq \int_0^1 t^{a-1}(1-t)^{b-1} \;dt,
	\]
	which is used in the definition of the Beta CDF. We can derive an upper bound on the denominator in the following way. Namely, we have
	\begin{align}
	\Ee{(V - y)^+ } & = \frac{1}{B(s+1,k-s+1)}\int_{y}^1 (t-y) t^s (1-t)^{k-s} \; dt \nonumber \\
	& = \frac{1}{B(s+1,k-s+1)}\int_{y}^1 t^{s+1} (1-t)^{k-s} \; dt - y \P{V \ge y} \nonumber \\
	& = \frac{B(s+2,k-s+1)}{B(s+1,j-s+1)}\left( \frac{1}{B(s+2,k-s+1)} \right)\int_{y}^1 t^{s+1} (1-t)^{k-s}\; dt - y \P{V \ge y} \nonumber \\
	& = \frac{s+1}{k+2} F^B_{k+2,y}(s+1)  - y \P{V \ge y} \label{eq:part2_use_of_equiv_between_beta_and_binom} \\
	& \le \frac{s+1}{k+2} F^B_{k+2,y}(s+1) \nonumber \\
	& \le  F^B_{k,y}(k x)  \nonumber \\
	& \le \exp\left\{- k d(x,y) \label{ineq:chernoff_app} \right\}
	\end{align}
	where we use Fact~\ref{fact:equation_for_beta_binomial_cdfs} and the definition of the Beta CDF to establish equation \eqref{eq:part2_use_of_equiv_between_beta_and_binom}. The final bound in \eqref{ineq:chernoff_app} is the result of the Chernoff-Hoeffding theorem and Fact~\ref{fact:relationship_with_binom_cdfs}. Let $\delta:=y-x$, and note that $s < kx \Longrightarrow s \le \floor{(k+1)x}$ due to $s$ being integer, whence
	\begin{align}
	\Ee{(y - V)^+ } & =  \Ee{(y - V) \ind{V \le y}} \nonumber \\
	& = \Ee{(y - V) \ind{y - \delta \le V \le y} } +  \Ee{(y - V) \ind{V < y - \delta} } \nonumber\\
	& > \Ee{(y - V) \ind{V < y - \delta} }\nonumber \\
	& \ge \delta\Ee{\ind{V < y-\delta} }\\
	& = \delta \P{V < x } \nonumber \\
	& = \delta\left(1 - F^B_{k+1,x}(s) \right) \label{eq:use_of_bin_beta_identity}  \\
	& \ge \delta/2  \label{eq:use_of_median_prop}
	\end{align}
	where equation \eqref{eq:use_of_bin_beta_identity} relies on Fact~\ref{fact:equation_for_beta_binomial_cdfs}. The bound \eqref{eq:use_of_median_prop} is justified from Fact~\ref{fact:median_of_binomial_dist} and $s \le \floor{(k+1) x}$. Thus using the inequalities for both the numerator and denominator, we obtain the desired bound.
\end{myproof}
\subsubsection{Proof of Lemma~\ref{lem:accurate_suboptimal_mean}.} \label{prf:proof_of_acc_sub_means}
\begin{proof}[Proof.]
	The steps in this proof follow a similar one in \cite{agrawal2013further} but we show them for completeness. We bound the number of times the sub-optimal arm's mean is overestimated. Let $\tau_\ell$ be the time step in which the  sub-optimal arm is sampled for the $\ell$\textsuperscript{th} time. Because for any $x$, $\lim_{n\to\infty}\frac{\floor{nx}}{nx} = 1$, we can let $N$ be a large enough integer so that if $\ell \ge N$, then $\eta_1 \frac{\floor{\ell \eta_1}}{\ell \eta_1} > x' := (\theta_2 + \eta_1)/2 > \theta_2$. In that case,
	\begin{align}
	\sum_{t=1}^T\P{S_2(t) \ge \floor{\Nt{2} \eta_1}, \; \pi^{\rm OG}_t = 2} & \le \Ee{\sum_{\ell=1}^T \sum_{t=\tau_\ell}^{\tau_{\ell+1}-1}\ind{S_2(\ell) \ge \floor{\Ntg{2}{\ell} \eta_1}} \ind{\pi^{\rm OG}_t = 2}} \nonumber \\
	& = \Ee{\sum_{\ell=1}^T \ind{S_2(\tau_{\ell}) \ge \floor{(\ell-1) \eta_1}} \sum_{t=\tau_\ell}^{\tau_{\ell+1}-1} \ind{\pi^{\rm OG}_t = 2}} \nonumber\\
	& = \Ee{\sum_{\ell=0}^{T-1} \ind{S_2(\tau_{\ell+1}) \ge \floor{\ell \eta_1}}} \nonumber\\
	& \le  N + \sum_{\ell=N+1}^{T-1} \P{ S_2(\tau_{\ell+1}) \ge \ell \eta_1 \frac{\floor{\ell \eta_1}}{\ell \eta_1}} \nonumber \\
	& \le N + \sum_{\ell=N+1}^{T-1} \P{ S_2(\tau_{\ell+1}) \ge \ell x'} \nonumber \\
	& \le  N + \sum_{\ell=1}^{\infty} \exp(-\ell d(x', \theta_2)) \label{bound:cf_thm} \\
	& = N + \frac{1}{1 - e^{-d(x',\theta_2)}} \nonumber
	\end{align}
	where \eqref{bound:cf_thm} follows from the Chernoff-Hoeffding theorem and the fact that $S_2(\tau_{\ell+1})$ is drawn from a $\text{Binomial}(\Ntg{2}{\ell+1}, \theta_2) \equiv \text{Binomial}(\ell, \theta_2)$ distribution.
\end{proof}

\section{Further experiment results} \label{sec:further_exp}
\subsection{Bayes UCB experiment} \label{exp:bayes_ucb}
This experiment is motivated by \cite{kaufmann2012bayesian} and in it we simulate the Bernoulli bandit problem with a $T = 500$ and two arms. Since we are interested in measuring expected regret over the prior, we draw the arms' mean rewards at random from the uniform distribution. There are 5,000 independent trials and we show the results in Figures~\ref{fig:kaufmann_regret}. OGI offers notable performance improvements over both Thompson Sampling and IDS for this modest horizon.
\begin{figure}[h!]
	\centering
	\input{plots/kaufmann_fig1.pgf}
	\caption{Frequentist regret. The OGI policy is configured with $K=1$ and $\alpha=100$.}
	\label{fig:kaufmann_regret}
\end{figure}
{\color{blue}
\subsection{Additional benchmark algorithms}
In this section, we simulate a few additional algorithms to understand the importance of the varying discount factor, and to try out a different approximation of the Gittins index.
We also simulate the greedy policy to see the inherent value of exploration in our benchmark problems.
Specifically, the algorithms we experiment with are:
\begin{itemize}
	\item OGI with a one-step lookahead and a fixed discount factor of $\gamma$, which we will refer to throughout as ``FOGI($1/(1-\gamma)$)". The quantity $1/(1-\gamma)$ can be interpreted as a rough horizon over which this policy is optimal.
	\item OGI in which the Gittins index approximation equals the closed-form expression given in \cite{brezzi2002optimal}. We will refer to this policy as ``BL-OGI".
	\item The greedy policy, which plays the arm in $\argmax_i \E_{y_{i, N_i(t-1)}}[X_{i,t}]$. Effectively it is equivalent to FOGI($1$), and completely disregards the value of future exploration. We will simply call this policy ``Greedy" in our tables and plots.
\end{itemize}
Recall that the Gittins index policy is optimal for a geometrically distributed horizon with mean $T$. Since FOGI$(T)$ is precisely an approximation for that policy, we would expect it to perform well in our experiments when the horizon is $T$ (even though really it ought to be geometrically distributed).

We reuse the two main experimental setups from Section~\ref{sec:experiments}: the Gaussian bandit with 10 independent arms, and the Bernoulli equivalent. Notice from Table~\ref{table:gaussian_experiment2}, in the Gaussian setup, that there is value in knowing the true horizon $T$ because FOGI($T$) is the dominant policy. 
We also see that either over or under-estimating the horizon leads to worse performance as demonstrated by the regret from FOGI($T/10$), FOGI($10T$) and Greedy. Interestingly, we also see that the BL-OGI shows larger regret than OGI (see Table~\ref{table:gaussian_experiment1}), suggesting that there is perhaps value in using our optimistic approximation for this particular problem. The comparison against FOGI, BL-OGI and Greedy in the Bernoulli case, presented in Table~\ref{table:bernoulli_experiment2}, tells a similar story as in Table~\ref{table:gaussian_experiment2}.

\begin{table}[h!]
	\centering
	{\color{blue}
	\begin{tabular}{lrrrrr}
		\toprule
		{} &  \textbf{BL-OGI} &  \textbf{Greedy} &  \textbf{FOGI($T$)} &  \textbf{FOGI($T/10$)} &  \textbf{FOGI($10T$)} \\
		\midrule
		Mean  &   58.54 &  167.16 &      49.61 &         60.72 &        59.09 \\
		Standard error   &    2.14 &    9.74 &       1.90 &          4.25 &         1.47 \\
		25\%   &   45.83 &  102.75 &      39.28 &         34.85 &        50.94 \\
		50\%   &   56.87 &  156.63 &      47.29 &         52.19 &        57.84 \\
		75\%   &   67.67 &  216.77 &      60.04 &         87.63 &        67.59 \\
		CPU time (s)   &  164.48 &  405.66 &     119.72 &        226.19 &        94.30 \\
		\bottomrule
	\end{tabular}
	\caption[Table caption text]{Comparison against some of OGI's simpler variants in the  Gaussian setup.}
	\label{table:gaussian_experiment2}
	}
\end{table}
}
	
\begin{table}[h!]
	\centering
	{\color{blue}
		\begin{tabular}{lrrrrr}
			\toprule
			\textbf{Algorithm} &  \textbf{BL-OGI} &  \textbf{Greedy} &  \textbf{FOGI($T$)} &  \textbf{FOGI($T/10$)} &  \textbf{FOGI($10T$)} \\
			\midrule
			Mean  &   23.50 &   56.32 &      16.52 &         18.69 &        20.14 \\
			Standard error  &    0.63 &    2.36 &       0.62 &          0.82 &         0.58 \\
			25\%   &   18.74 &   37.61 &      12.36 &         12.70 &        16.36 \\
			50\%   &   22.50 &   55.16 &      15.55 &         17.14 &        19.43 \\
			75\%   &   28.18 &   74.64 &      19.72 &         23.29 &        23.62 \\
			CPU time (s)  &   0.01 &  0.001 &   0.12 &         0.11 &      0.13  \\
			\bottomrule
		\end{tabular}
		\caption[Table caption text]{\color{blue}Comparison against some of OGI's simpler variants in the  Bernoulli setup.}
		\label{table:bernoulli_experiment2}
	}
\end{table}

\subsection{Additional tables for Section~\ref{sec:experiments}}
\begin{table}
	\centering
	\begin{tabular}{rrrrrr} 
		\toprule
		{}    $\alpha$ &   $\beta$ &  OGI(1) &  OGI(3) &  OGI(5) &  Gittins \\
		\midrule
		   1 & 1 &   0.760 &   0.721 &   0.712 &    0.703 \\
		   1 & 2 &   0.571 &   0.522 &   0.511 &    0.500 \\
		   1 & 3 &   0.452 &   0.401 &   0.389 &    0.380 \\
		   1 & 4 &   0.374 &   0.321 &   0.312 &    0.302 \\
		  2 & 1 &   0.853 &   0.818 &   0.809 &    0.800 \\
		  2 & 2 &   0.702 &   0.657 &   0.646 &    0.635 \\
		  2 & 3 &   0.591 &   0.543 &   0.530 &    0.516 \\
		  2 & 4 &   0.508 &   0.458 &   0.445 &    0.434 \\
		  3 & 1 &   0.893 &   0.864 &   0.855 &    0.845 \\
		  3 & 2 &   0.771 &   0.729 &   0.719 &    0.707 \\
		  3 & 3 &   0.671 &   0.626 &   0.613 &    0.601 \\
		  3 & 4 &   0.592 &   0.545 &   0.532 &    0.518 \\
		 4 & 1 &   0.916 &   0.890 &   0.882 &    0.872 \\
		  4 & 2 &   0.813 &   0.776 &   0.765 &    0.754 \\
		  4 & 3 &   0.724 &   0.682 &   0.670 &    0.658 \\
		  4 & 4 &   0.651 &   0.607 &   0.593 &    0.581 \\
		\bottomrule
	\end{tabular}
	\caption{Optimistic and exact Gittins Indices when $\gamma = 0.9$ for different Beta-Bernoulli parameters}
	\label{table:ogi_table_for_gamma_9}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{rrrrrr}
		\toprule
		{}    $\alpha$ &   $\beta$ &  OGI(1) &  OGI(3) &  OGI(5) &  Gittins \\
		\midrule
		 1.0 & 1.0 &   0.817 &   0.784 &   0.774 &    0.761 \\
		 1.0 & 2.0 &   0.637 &   0.590 &   0.577 &    0.560 \\
		 1.0 & 3.0 &   0.514 &   0.463 &   0.449 &    0.433 \\
		 1.0 & 4.0 &   0.430 &   0.376 &   0.364 &    0.348 \\
		 2.0 & 1.0 &   0.890 &   0.860 &   0.851 &    0.838 \\
		 2.0 & 2.0 &   0.752 &   0.710 &   0.698 &    0.681 \\
		 2.0 & 3.0 &   0.643 &   0.596 &   0.581 &    0.562 \\
		2.0 & 4.0 &   0.558 &   0.509 &   0.494 &    0.475 \\
		 3.0 & 1.0 &   0.921 &   0.896 &   0.887 &    0.874 \\
		3.0 & 2.0 &   0.811 &   0.773 &   0.762 &    0.744 \\
		 3.0 & 3.0 &   0.715 &   0.672 &   0.658 &    0.639 \\
		 3.0 & 4.0 &   0.637 &   0.591 &   0.575 &    0.556 \\
		4.0 & 1.0 &   0.938 &   0.916 &   0.908 &    0.895 \\
		4.0 & 2.0 &   0.847 &   0.812 &   0.801 &    0.784 \\
		4.0 & 3.0 &   0.763 &   0.722 &   0.709 &    0.690 \\
		4.0 & 4.0 &   0.691 &   0.648 &   0.633 &    0.613 \\
		\bottomrule
	\end{tabular}
	\caption{Optimistic and exact Gittins Indices when $\gamma = 0.95$ for different Beta-Bernoulli parameters}
	\label{table:ogi_table_for_gamma_95}
\end{table}
