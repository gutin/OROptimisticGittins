\section{Computational Experiments} \label{sec:experiments}
Our goal is to benchmark Optimistic Gittins Indices (OGI) against state-of-the-art Bayesian algorithms. Specifically, we compare ourselves against Thomson Sampling, Bayes UCB and IDS. Each of these algorithms has in turn been shown to substantially dominate other extant schemes. Our experimental setup closely follows that of \cite{russo2014learning,kaufmann2012thompson} and \cite{chapelle2011empirical}. The only difference in our paper is we will randomize arm parameters rather than set them to specific constants. This is for consistency and so that we focus on evaluating algorithms purely on their Bayesian performance. The experiment from \cite{kaufmann2012thompson} is deferred to Appendix~\ref{exp:bayes_ucb} because it is brief and sends a similar message to the rest of this section. We conclude with a novel experiment to test the problem with multiple simultaneous arm pulls.

For the majority of experiments, we configure the OGI algorithm with $K =1$ to keep the computational burden under control. In one experiment, included for completeness, we test OGI with $K = 3$ and $K=\infty$, where the latter is equivalent to using Gittins indices. We use a common discount factor schedule in all experiments setting $\gamma_t = 1 - 1/(100 + t)$. The choice of $\alpha = 100$ is second order and our conclusions remain unchanged, and actually appear to improve in an absolute sense with other choices (we show this in one set of experiments). 

A major consideration in running these experiments is that the CPU time required to execute IDS, the closest competitor, based on the current suggested implementation is orders of magnitudes greater than that of the index schemes or Thompson Sampling. The main bottleneck is that IDS uses numerical integration,  requiring the calculation of a CDF over, at least, hundreds of iterations. By contrast, the version of OGI with $K=1$ uses 10 iterations of the Newton-Raphson method. In the remainder of this section, we discuss the results.

\subsection{Smaller scale experiments with IDS}

\paragraph{Gaussian}We replicate the experiments from \cite{russo2014learning}. In the first experiment (Table~\ref{table:gaussian_experiment1}), the arms generate Gaussian rewards  $X_{i,t} \sim \mathcal{N}(\theta_i, 1)$ where each $\theta_i$ is independently drawn from a standard Gaussian distribution. We simulate 1,000 independent trials with 10 arms and 1,000 time periods. The implementation of OGI in this experiment uses $K = 1$. It is difficult to compute exact Gittins indices in this setting, but a classical approximation for Gaussian bandits does exist; see \cite{powell2012optimal}, Chapter 6.1.3. We term the use of that approximation `OGI($\infty$) Approx'.  In addition to regret, we  show the average CPU time taken, in seconds, to execute each trial.
%We also evaluate a policy (labeled `OGI Approx' in the table) that computes a particular closed-form approximation to the Gittins Index given in Chapter 6.1.3 of Powell and Ryzhov \cite{powell2012optimal}. 

\begin{table}[h!]
	\centering
	\begin{tabular}{cccccc} \toprule
		\textbf{Algorithm}  & \textbf{OGI(1)} & \textbf{OGI($\infty$) Approx.} & \textbf{IDS} & \textbf{TS} & \textbf{Bayes UCB}\\ \midrule
		Mean   & 49.19 & 47.64  &  55.83 & 67.40 & 60.30  \\ 
		Standard error  & 1.61 & 1.6 & 2.08 & 1.5 & 1.43 \\ 
		25\%  & 17.49 & 16.88  & 18.61 & 37.46 & 31.41 \\
		50\%   & 41.72 & 40.99 & 40.79 & 63.06 & 57.71 \\ 
		75\%  & 73.24 & 72.26 & 78.76 & 94.52 & 86.40 \\ 
		CPU time (s) & 0.02 & 0.01 & 11.18 & 0.01 & 0.02 \\
		\bottomrule
	\end{tabular}
	\caption[Table caption text]{Gaussian experiment. OGI(1) denotes OGI with $K =1$, while OGI Approx. uses the approximation to the Gaussian Gittins Index from \cite{powell2012optimal}.}
	\label{table:gaussian_experiment1}
\end{table}

The key feature of the results here is that OGI offers an approximately 10\% improvement in regret over its nearest competitor IDS, and larger improvements (20 and 40 \% respectively) over Bayes UCB and Thompson Sampling. The best performing policy is OGI with the specialized Gaussian approximation since it gives a closer approximation to the Gittins Index. At the same time, OGI is essentially as fast as Thompson sampling, and three orders of magnitude faster than its nearest competitor (in terms of regret). 


\paragraph{Bernoulli}
In this experiment regret is simulated over 1,000 periods, with 10 arms each having a uniformly distributed Bernoulli parameter. We simulate 1,000 independent trials and Table~\ref{table:bernoulli_experiment1} summarizes the results.

\begin{table}[h!]
	\centering
	\begin{tabular}{ccccccc} \toprule
		\textbf{Algorithm} & \textbf{OGI(1)} & \textbf{OGI(3)} &  \textbf{OGI($\infty$)} & \textbf{IDS} & \textbf{TS} & \textbf{Bayes UCB}  \\ \midrule
		Mean &  18.12 & 18.00 & 17.52 & 19.03 & 27.39 & 22.71 \\ 
		Standard error & 0.65 & 0.64 &  0.68 & 0.67 & 0.57 & 0.56 \\ 
		25\% & 6.26 & 5.60 & 4.45 & 5.85 & 14.62 & 10.09 \\
		50\% & 15.08 & 14.84 &12.06 & 14.06 & 23.53 & 18.52 \\
		75\% & 27.63 & 27.74 & 24.93 & 26.48 & 36.11 & 30.58 \\
		CPU time (s) & 0.19 & 0.89 & (?) hours & 8.11 & 0.01 & 0.05  \\ \bottomrule
	\end{tabular}
	\caption[Table caption text]{Bernoulli experiment. OGI($K$) denotes the OGI algorithm with a $K$ step approximation and tuning parameter $\alpha = 100$. OGI($\infty$) is the algorithm that uses Gittins Indices.}
	\label{table:bernoulli_experiment1}
\end{table}
Each version of OGI outperforms other algorithms and the one that uses exact Gittins Indices shows the lowest mean regret.
The regret from IDS is slightly higher than we anticipated, based on the results from \citep{russo2014learning} and we include a link to the code we used to implement the algorithms\footnote{\url{https://github.com/gutin/FastGittins}} as a reference.
Perhaps, unsurprisingly, when OGI  looks ahead 3 steps it performs marginally better than with a single step.
Nevertheless, looking ahead 1 step is a reasonably close approximation to the Gittins Index in the Bernoulli problem.
In fact the approximation error, when using an optimistic 1 step approximation, is around 15\% and if $K$ is increased to 3, the error drops to around 4\% (see Tables~\ref{table:ogi_table_for_gamma_9} and \ref{table:ogi_table_for_gamma_95} in the Appendix).


\subsection{Large scale experiment} \label{exp:ts_sampling_experiment}
This experiment is motivated by one in \cite{chapelle2011empirical}.
The key feature here is that we simulate a  longer horizon of $T = 10^6$ and include a large number of arms, particularly we let $A = 100$. This is an order of magnitude greater than in the majority of bandit experiments we are aware of.
Our goal is to see how the algorithms scale both computationally and in terms of performance.
Such a setup is practically relevant because in applications such as e-commerce or online advertising, the problems of interest are typically modeled with many arms relative to the horizon, where each arm could represent a product or ad.

Because all the methods we test in our numerical experiments are asymptotically optimal, any relative difference in regret must shrink after a sufficiently large number of time periods. The length of time for this `burn in' period depends on the number of arms in the problem.
In fact, we can think of the horizon as giving us a rough number of trials per arm, more specifically, this is the ratio $T/A$.
The idea is that with more trials per arm we should expect a smaller relative difference between the algorithms.
We will see that even when the ratio $T/A$ and $A$ itself are large, there is a substantial difference between OGI and the competing benchmarks in both a relative and absolute sense.

As this experiments requires an order of magnitude more iterations than the earlier ones, we are only able to simulate the fastest algorithms, which are OGI with $K=1$ and $\alpha = 100$, Thompson Sampling and Bayes UCB. 
It was not possible to include IDS because its performance is hindered by the fact that each arm pull decision requires time that is quadratic in the number of arms to compute.
Again, this is a Bernoulli experiment where arm means are independently sampled from a uniform prior and each algorithm assumes this same prior over the unknown mean rewards from the arms.
We show the algorithms' regret averaged over 5,000 trials in Figure~\ref{fig:chapelle_and_li} and Table~\ref{table:additional_cli_table}.
\begin{figure}[h!]
	\centering
	\input{plots/large_scale_exp_a100.pgf}
	\caption{Cumulative regret in the large-scale problem, of this section, averaged over 5,000 independent trials.}
	\label{fig:chapelle_and_li}
\end{figure}

\begin{table}[h!]
	\centering
	\begin{tabular}{c|ccccc}
		\toprule
		Time periods (1000's) &   OGI &  Thompson &   Bayes-UCB &  Rel. adv (\%) &  Abs. adv \\
		\midrule
		200 & 230.5 &     284.4 & 297.9 &                      18.9 &                  53.9 \\
		400 & 254.7 &     311.6 & 333.5 &                      18.3 &                  57.0 \\
		600 & 268.6 &     327.4 & 354.5 &                      18.0 &                  58.8 \\
		800 & 279.1 &     339.2 & 369.6 &                      17.7 &                  60.1 \\
		1,000 & 287.1 &     347.7 & 380.7 &                      17.4 &                  60.6 \\
		\bottomrule
	\end{tabular}
	\caption{Regret in the large scale experiment from OGI, Thompson Sampling and Bayes UCB. The last two columns show the relative and absolute difference from Thompson Sampling, which is the closest competitor to OGI.}
	\label{table:additional_cli_table}
\end{table}

As before, the OGI scheme consistently dominates the other two and the relative margin by which OGI outperforms the other algorithms appears to decline, albeit steadily with the horizon.
In particular, we notice that the improvement in regret, over the nearest competitor Thompson Sampling, is 18.9\% when there are $2\times 10^5$ periods but this improvement decreases to almost 17.4\% with $10^6$ time periods.
On the other hand, the \emph{absolute} difference in regret increases monotonically with $T$.
Therefore our method appears to consistently retain its advantage over Thompson Sampling when $T$ is large even if, in a relative sense, the performance gap shrinks (as we would expect from the asymptotic bound).
\subsection{Bandits with multiple arm pulls}
For this experiment, we consider a more general MAB problem, where the agent is able to pull up to a certain number (say $m < A$) of the arms simultaneously. In order to describe the problem, we recall that $A$ is the total number of arms and define  $\mathcal{D}_m := \{d \in \{0,1\}^A : \sum_i d_i \le m\}$ to be the set of all $A$-dimensional binary vectors with up to $m$ ones in them, which we take to be the action space. Let $X_t = (X_{1,t}, \ldots, X_{1,A})$ be a tuple of (potential) rewards from the $A$ arms at time $t$, where the definition of $X_{i,t}$ for any arm $i$ is the same as in Section~\ref{sec:model_and_prelim}. Given a decision $d \in \mathcal D_m$, which encodes the subset of arms pulled, the reward $d^\top X_t$ is earned and an arm $j$'s reward $X_{j,t}$ is observed if and only if that arm is pulled, i.e. $d_{j} = 1$. We can then define a policy $(\pi_t, t \in \mathbb{N})$ to be a $\mathcal{D}_m$-valued stochastic process where $\pi_{t+1}$ is measurable with respect to $\sigma\left( (\pi_s, (\pi_{i,s} X_{i,s}, \;i =1,\ldots,A)), \; s=1,\ldots,t\right)$. That is a policy's information set comes from its previous decisions and observed feedback. A policy $\pi$'s regret is given by the equation 
\[
\Regret{\pi, T} = T \cdot \Ee{\max_{d \in \mathcal D_m} d^\top  X_t} - \sum_{t=1}^T \E[\pi_t^\top X_t ]
\]
where the expectation is over both the randomness in the rewards, the prior and the policy's actions.

We propose a heuristic to this problem using our scheme, which is to compute the Optimistic Gittins Index of every arm, at time $t$, using a discount factor of $1-1/t$ (just as before). However, for this problem, we pick $m$ arms with the largest indices. This is essentially Whittle's heuristic \citep{whittle1988restless}, which was originally given for the restless bandit problem but can be described as picking several arms with the largest Gittins indices.

To test our policy, we simulate $A = 6$ binary arms with uniformly distributed biases and fix $m=3$. 
We benchmark our heuristic against Thompson Sampling and IDS. Because the arms give independent Bernoulli rewards, we will use a flat Beta prior for all of the algorithms. We implement the version of IDS designed for the linear bandit problem because this experiment is a special case of it and the IDS algorithm there is practical to simulate. Our implementation of IDS also uses 100 Monte Carlo samples per iteration.

The results, produced from 1,000 independent trials, are summarized in Figure~\ref{fig:restless1} and Table~\ref{table:restless1_summary}. We notice a significant spread in the performance between OGI and both Thompson Sampling and IDS. Just like for our main algorithm, the primary computational bottleneck in using OGI comes from solving the stopping problem and this can be onerous if $K$ is large. However, as the results suggest, the policy works well even for low to moderate look-ahead parameters. By contrast, IDS is the slowest algorithm because it needs to generate a hundred Monte Carlo samples in every iteration.
\begin{figure}
	\centering
	\input{plots/multiple_pulls.pgf}
	\caption{Regret for bandits with multiple simultaneous arm pulls}
	\label{fig:restless1}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{lrrrr}
		\toprule
		{} &   IDS &  Thompson &  Whittle(3) &  Whittle(4) \\
		\midrule
		Mean      & 15.50 &     15.12 &       11.07 &       11.23 \\
		SD       & 22.14 &     13.08 &       14.85 &       14.71 \\
		25\% &  1.32 &      6.02 &        1.02 &        1.08 \\
		50\%    & 10.75 &     14.86 &        9.65 &        9.85 \\
		75\% & 24.65 &     23.12 &       20.00 &       19.97 \\
		\bottomrule
	\end{tabular}
	
	\caption{Regret from the multiple arm pulls experiment. ``OGI($K$)" refers to the Whittle heuristic-like policy, using $K$ look-ahead steps.}
	\label{table:restless1_summary}
\end{table}