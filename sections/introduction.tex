\section{Introduction} \label{sec:intro}
The Multi-Armed Bandit (MAB) problem is perhaps the simplest example of a learning problem that exposes the tension between exploration and exploitation. Recent years have seen a resurgence of interest in Bayesian MAB problems wherein we are endowed with a prior on arm rewards, and a number of index schemes that exploit this prior have been proposed and analyzed. These include Thompson Sampling \citep{thompson1933likelihood}, Bayes-UCB \citep{kaufmann2012thompson}, KL-UCB \citep{garivier2011kl}, and Information Directed Sampling \citep{russo2014learning}. The ultimate motivation for these algorithms appears to be two-fold: superior empirical performance and light computational burden. The strongest performance results available for these algorithms establish regret lower bounds that match the Lai-Robbins lower bound \citep{lai1985asymptotically}.  Even among this set of recently proposed algorithms, there is a wide spread in empirically observed performance. Table~\ref{table:intro_algorithm_summary} lists well-known algorithms in the literature and what is known about them.

\begin{table}
	\centering
	\begin{tabular}{@{}llll}\toprule
		Algorithm & Bayes/Frequentist & Regret Optimal & Framework \\ \midrule
		KL-UCB & Frequentist & Yes & Index-based \\
		UCB & Frequentist & Unknown & Index-based \\
		MOSS & Frequentist & Unknown & Index-based \\
		Thompson Sampling &Bayes & Yes & Posterior Sampling \\
		Bayes UCB & Bayes & Yes & Index-based \\
		IDS & Bayes & Unknown  & Mixed \\
		Gittins Index & Bayes & No & Index-based \\
		\bottomrule
	\end{tabular}
	\caption{Summary of some famous bandit policies and their properties.}
	\label{table:intro_algorithm_summary}
\end{table}

Interestingly, the design of the index policies referenced above has been somewhat ad-hoc as opposed to having emerged from a principled analysis of the underlying Markov Decision Process. Now if in contrast to requiring `small' regret for all sufficiently large time horizons, we cared about minimizing Bayesian regret over an infinite horizon, discounted at a fixed, pre-specified rate (or equivalently, maximizing discounted infinite horizon rewards), the celebrated Gittin's index theorem provides an {\em optimal, efficient} solution. Importing this celebrated result to the fundamental problem of designing algorithms that achieve low regret (either frequentist or Bayesian) simultaneously over all sufficiently large time horizons runs into two substantial challenges:

\noindent {\em High-Dimensional State Space: }Even minor `tweaks' to the discounted infinite horizon objective appear to render the corresponding Markov Decision problem for the Bayesian MAB problem intractable. For instance, it is known that a Gittins-like index strategy is sub-optimal for a fixed, finite-horizon \citep{berry1985bandit}. Moreover, the problem of minimizing regret simultaneously over all sufficiently large horizons is not well understood.
\newline
\newline
\noindent {\em Computational Burden: }Even in the context of the discounted infinite horizon problem, the computational burden of calculating a Gittins index is substantially larger than that required for any of the index schemes for the multi-armed bandit discussed thus far. 

The present paper attempts to make progress on these challenges. Specifically, we make the following contributions:
\begin{itemize}
	\item We propose a class of `optimistic' approximations to the Gittins index that can be computed with significantly less effort. In fact, the computation of the simplest of these approximations is no more burdensome than the computation of indices for the Bayes UCB algorithm, and several orders of magnitude faster than the nearest competitor, IDS. 
	\item We establish that an arm selection rule that is greedy with respect to the simplest of these optimistic approximations achieves optimal regret in the sense of meeting the Lai-Robbins lower bound (including matching constants) provided the discount factor is increased at a certain rate.
	\item We show empirically that even the simplest optimistic approximation to the Gittins index proposed here {\em outperforms the state-of-the-art incumbent schemes discussed in this introduction by a non-trivial margin}. We view this as our primary contribution -- the Bayesian MAB problem is fundamental making the performance improvements we demonstrate important.  
\end{itemize}

\subsection{Relevant Literature}
Thompson Sampling \cite{thompson1933likelihood} was proposed as a heuristic to the MAB problem in 1933, but was largely ignored until the last decade. An empirical study by Chapelle and Li \cite{chapelle2011empirical} highlighted Thompson Sampling's superior performance and led to a series of strong theoretical guarantees for the algorithm being proved in \cite{agrawalanalysis,agrawal2013further,kaufmann2012thompson} (for specific cases when Gaussian and Beta priors are used). Recently, these proofs were generalized to the 1D exponential family of distributions in \cite{korda2013thompson}. A few decades after Thompson Sampling was introduced, Gittins \cite{gittins1979bandit} showed that an index policy was optimal for the infinite horizon discounted MAB problem. Several different proofs for the optimality of Gittins index, were shown in \cite{tsitsiklis1994short,weber1992gittins,whittle1980multi,bertsimas1996conservation}. Inspired by this breakthrough, \cite{lai1985asymptotically,lai1987adaptive}, while ignoring the original MDP formulation, proved an asymptotic lower bound on achievable (non-discounted) regret and suggested policies that attained it.

Simple and  efficient UCB algorithms were later developed in \cite{agrawal1995sample,auer2002finite,audibert2010regret}, with finite time regret bounds. These were followed by the KL-UCB (\cite{garivier2011kl}) and Bayes UCB (\cite{kaufmann2012thompson}) algorithms. The Bayes UCB paper drew attention to how well Bayesian algorithms performed in the frequentist setting. In that paper, the authors also demonstrated that a policy using indices similar to Gittins' had the lowest regret. The use of Bayesian techniques for bandits was explored further in \cite{russo2014learning} where the authors propose Information Directed Sampling, an algorithm that exploits complex information structures arising from the prior. There is also a very recent paper, \cite{lattimore2016regret}, which also focuses on regret minimization using approximated Gittins Indices. However, in that paper, the time horizon is assumed to be known and fixed, which is different from the focus in this paper on finding a policy that has low regret over all sufficiently long horizons.

\subsection{Structure of the paper}
The remainder of this paper is organized as follows: in the next section, we state our notation, objectives of interest 
and key results such as the Lai-Robbins lower bound. The third section focuses on the Gittins Index and explains how it fails to minimize regret in a sense that is made clear later. At the end, we address another issue, namely the computational cost of calculating the Gittins Index, which inspires us to develop the Optimistic Gittins Index (OGI) policy. Section~\ref{sec:analysis_of_regret} establishes an optimal regret bound for the algorithm; namely, one that matches the Lai-Robbins lower bound. Next, we consider a more general version of the multi-armed bandit problem in Section~\ref{sec:multiple_pulls}, where multipled simultaneous ``pulls" are allowed and propose heuristics for that setting, which are derived from the framework in this paper. Finally, Section~\ref{sec:experiments} presents experiments showing how OGI achieves lower Bayesian regret than state of the art policies and is computationally efficient and, in Section~\ref{sec:conclusions} we state open questions that remain following this paper.