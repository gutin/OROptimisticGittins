\section{Introduction} \label{sec:intro}

%points to make: (1) current bayesian landscape appears to offer v. good algos. (2) Bayesian setup yields an MDP, but this has not been leveraged. (3) Gitins index is optimal for MDPs. (4) Challenges with Gittins. (5) Our contribution. 

The Multi-Armed Bandit (MAB) problem is perhaps the simplest example of a learning problem that exposes the tension between exploration and exploitation. In its simplest form, we are given a collection of random variables or `arms'. By adaptively sampling these random variables, we seek to eventually sample consistently from the random variable with the highest mean. This is typically formalized by asking that we minimize cumulative `regret'; a notion we make precise in a later section. 

Recent years have seen a resurgence of interest in {\em Bayesian} algorithms for the MAB problem. In this variant of the MAB problem, we are endowed with a prior on arm means, and a number of index schemes that exploit this prior have been proposed and analyzed. These include Thompson Sampling \citep{thompson1933likelihood}, Bayes-UCB \citep{kaufmann2012thompson}, KL-UCB \citep{garivier2011kl}, and Information Directed Sampling \citep{russo2014learning}. The ultimate motivation for these algorithms appears to be the empirical performance they offer. Specifically, these Bayesian algorithms appear to incur smaller regret than their frequentist counterparts such as \cite{auer2002finite}, even when regret is measured in a frequentist sense. This empirical evidence has, very recently, been reinforced by theoretical performance guarantees; the strongest performance results available for these algorithms establish upper bounds on frequentist regret that match the Lai-Robbins lower bound \citep{lai1985asymptotically}. Interestingly, even amongst the various Bayesian algorithm proposed there appears to be a wide range in empirical performance. For instance, empirical evidence presented in \cite{russo2014learning} suggests that the IDS algorithm offer a substantial improvement in frequentist regret over Thompson sampling and the Bayes-UCB algorithm, among others. These algorithms also vary substantially in their design (as opposed to being variations on a theme) and enjoy distinct performance guarantees.    

Now a prior on arm means endows us with the structure of a Markov Decision Process (MDP) and none of the Bayesian algorithms alluded to above exploit this structure. This is especially surprising in light of the celebrated Gittins Index Theorem. That breakthrough result proved the optimality of a certain index policy for a {\em horizon dependent} variant of the Bayesian MAB. Specifically, imagine that we cared about the expected (Bayes) regret incurred over an exponentially distributed horizon, where the mean horizon length is known to the algorithm designer. This problem is nominally a high dimensional MDP. Gittins, however, proved that a simple to compute index rule was optimal for this task resolving a problem that had remained open for several decades \cite{}. Why does the Gittins Index Theorem not immediately help resolve the design of an optimal algorithm for the variant of the Bayesian MAB problem that is the subject of the approaches discussed in the preceding paragraph? As we will discus more carefully in our literature review, this is certainly not from lack of research effort. In fact, one must deal with several substantial challenges:
\begin{enumerate}
\item
Dependence on Horizon: The notion of regret optimality as popularized by \cite{lai1985asymptotically} is `anytime'. Colloquially, this can be thought of as follows: we desire an algorithm that performs well for {\em any} time horizon. This fact is fundamentally at odds with Gittins' variant of the MAB problem that (via a discount factor) effectively specifies a (exponentially distributed) horizon. Gittins' result is intimately connected to this choice of horizon; even seemingly minor changes appear to render the problem intractable. For instance, it is known that a Gittins-like index strategy is sub-optimal for a fixed, finite-horizon \citep{berry1985bandit}. Algorithms for other notions of optimality that one may reasonably conjecture are better aligned with `anytime' regret optimality (such as, say, Cesaro-overtaking optimality) are similarly elusive \cite{katehakis1996finite}.  
\item
Computation: Ignoring the issues made in the previous point, consider the task of computing a Gittins index at every point in time. 
The computation of a Gittins index can be reduced to the solution of a certain infinite horizon stopping problem. For the Bayesian MAB, the state space for this problem must describe all possible posteriors one may encounter on a given arm. Assuming conjugate priors, one may hope for a countable state space, but tractable computation will require some form of state-space truncation. This computation is far more onerous than any of the aforementioned indices. Furthermore, it stands to reason that as time progresses one may require increasingly more accurate estimates of the Gittins index, which further complicates computation.  
\end{enumerate}

Against this backdrop, the present paper makes the following contribution: {\em We show that picking arms according to a certain tractable approximation to their Gittins index, computed for a time dependent discount factor, constitutes a regret optimal bandit policy. The resulting index rule is both simple to compute and in computational experiments appears to outperform all of the aforementioned bandit algorithms by a material margin.} In greater detail, we outline our contributions as follows:

\begin{enumerate}

\item Optimistic Approximations: We propose a sequence of `optimistic' approximations to the Gittins index. These optimistic approximations can be interpreted as providing a tightening sequence of upper bounds on the optimal stopping problem defining a Gittins index, yielding the index itself in the limit. The computation associated with the simplest of these approximations is no more burdensome than the computation of indices for the Bayes UCB algorithm, and several orders of magnitude faster than the lowest regret alternative, IDS. 
\item Regret Optimality: We establish that an arm selection rule that is greedy with respect to any optimistic approximation to the Gittins index achieves optimal regret in the sense of meeting the Lai-Robbins lower bound (including matching constants). A crucial ingredient required for this scheme to work is that as time progresses, the discount factor employed in computing the index must be increased at a certain rate which we characterize precisely. This implicitly resolves the challenge of horizon dependence. 
\item Empirical Performance: We show empirically that even the simplest optimistic approximation to the Gittins index outperforms the state-of-the-art incumbent schemes discussed in this introduction by a non-trivial margin. The margin of improvement increases further as one employs successfully tighter optimistic approximations, at the cost of computational effort. We illustrate this performance improvement on an ensemble of problems considered by previous authors. We view this as a valuable contribution: the Bayesian MAB problem is fundamental, making the performance improvements we demonstrate important.  
\end{enumerate}

In summary, we propose a new index rule for the Baysian MAB problem that employs Gittins indices in a novel way. This new index rule enjoys the strongest possible data-dependent regret guarantees while also offering excellent empirical performance. 

%The ultimate motivation for these algorithms appears to be two-fold: superior empirical performance and light computational burden. The strongest performance results available for these algorithms establish regret lower bounds that match the Lai-Robbins lower bound \citep{lai1985asymptotically}.  Even among this set of recently proposed algorithms, there is a wide spread in empirically observed performance. Table~\ref{table:intro_algorithm_summary} lists well-known algorithms in the literature and what is known about them.

%\begin{table}
%	\centering
%	\begin{tabular}{@{}llll}\toprule
%		Algorithm & Bayes/Frequentist & Regret Optimal & Framework \\ \midrule
%		KL-UCB & Frequentist & Yes & Index-based \\
%		UCB & Frequentist & Unknown & Index-based \\
%		MOSS & Frequentist & Unknown & Index-based \\
%		Thompson Sampling &Bayes & Yes & Posterior Sampling \\
%		Bayes UCB & Bayes & Yes & Index-based \\
%		IDS & Bayes & Unknown  & Mixed \\
%		Gittins Index & Bayes & No & Index-based \\
%		\bottomrule
%	\end{tabular}
%	\caption{Summary of some famous bandit policies and their properties.}
%	\label{table:intro_algorithm_summary}
%\end{table}

%Interestingly, the design of the index policies referenced above has been somewhat ad-hoc as opposed to having emerged from a principled analysis of the underlying Markov Decision Process. Now if in contrast to requiring `small' regret for all sufficiently large time horizons, we cared about minimizing Bayesian regret over an infinite horizon, discounted at a fixed, pre-specified rate (or equivalently, maximizing discounted infinite horizon rewards), the celebrated Gittin's index theorem provides an {\em optimal, efficient} solution. Importing this celebrated result to the fundamental problem of designing algorithms that achieve low regret (either frequentist or Bayesian) simultaneously over all sufficiently large time horizons runs into two substantial challenges:

%\noindent {\em High-Dimensional State Space: }Even minor `tweaks' to the discounted infinite horizon objective appear to render the corresponding Markov Decision problem for the Bayesian MAB problem intractable. For instance, it is known that a Gittins-like index strategy is sub-optimal for a fixed, finite-horizon \citep{berry1985bandit}. Moreover, the problem of minimizing regret simultaneously over all sufficiently large horizons is not well understood.
%\newline
%\newline
%\noindent {\em Computational Burden: }Even in the context of the discounted infinite horizon problem, the computational burden of calculating a Gittins index is substantially larger than that required for any of the index schemes for the multi-armed bandit discussed thus far. 

%The present paper attempts to make progress on these challenges. Specifically, we make the following contributions:
%\begin{itemize}
%	\item We propose a class of `optimistic' approximations to the Gittins index that can be computed with significantly less effort. In fact, the computation of the simplest of these approximations is no more burdensome than the computation of indices for the Bayes UCB algorithm, and several orders of magnitude faster than the nearest competitor, IDS. 
%	\item We establish that an arm selection rule that is greedy with respect to the simplest of these optimistic approximations achieves optimal regret in the sense of meeting the Lai-Robbins lower bound (including matching constants) provided the discount factor is increased at a certain rate.
%	\item We show empirically that even the simplest optimistic approximation to the Gittins index proposed here {\em outperforms the state-of-the-art incumbent schemes discussed in this introduction by a non-trivial margin}. We view this as our primary contribution -- the Bayesian MAB problem is fundamental making the performance improvements we demonstrate important.  
%\end{itemize}



\subsection{Relevant Literature}
We organize our literature review around primary topics that this paper touches on. The study of exploration-exploitation problems is vast, even if it's restricted to a problem with a finite number of arms and so our review will be focused on stochastic, non-contextual, versions of the MAB problem. We discuss the most seminal work in the relevant areas, so this review is by no means complete.

\noindent\emph{Regret optimality and the bandit problem: }\cite{robbins1952some} motivated the study of the MAB problem and left open questions on how to design effective policies. Since then \cite{lai1985asymptotically} proved a cornerstone result, namely an asymptotic lower bound on regret that any `reasonable' strategy incurs. The same paper proposes an upper-confidence bound (UCB) algorithm that asymptotically achieves the  lower bound.

Computationally efficient UCB algorithms were developed in \cite{agrawal1995sample,katehakis1995sequential}. Later, \cite{auer2002finite,audibert2010regret} proved finite time regret bounds for UCB algorithms and demonstrate ways to tune them in order to improve performance. Other algorithms are proposed in \cite{garivier2011kl,maillard2011finite} where the confidence bounds are calculated using the KL-divergence function. They provide a finite-time analysis and their algorithms are shown to achieve the Lai-Robbins bound.

\noindent\emph{Bayesian bandit algorithms: } Another powerful approach to bandit problems is to work with a Bayesian prior to model one's uncertainty about an arm's expected reward. \cite{lai1987adaptive}  proves an asymptotic lower bound on Bayes' risk and develops a horizon-dependent algorithm that achieves it.
%The advantage of such algorithms is that they can make use of existing knowledge about an arm, such as the family of distributions that its rewards comes from, to make better decisions.
Thompson Sampling, \cite{thompson1933likelihood}, one of the earliest algorithms for the MAB problem, is in fact a Bayesian one. Empirical studies in \cite{chapelle2011empirical,scott2010modern} highlight Thompson Sampling's hugely superior performance over some UCB algorithms even when the prior is mismatched. A series of tight regret bounds for Thompson Sampling are proven in \cite{agrawalanalysis,agrawal2013further} and \cite{kaufmann2012thompson}. For specific instances such as the Bernoulli bandit problem, Thompson Sampling was proven to be asymptotically optimal. Recently, \cite{korda2013thompson} generalized the aforementioned results to bandit problems where the arm distributions belong to a 1-D exponential family. Interestingly enough, \cite{robbins1952some} seems to have been unaware of Thompson Sampling and its effectiveness in the non-Bayesian setting.

Several other Bayesian algorithms exist. \cite{kaufmann2012thompson} propose Bayes UCB, which they show is competitive with Thompson Sampling. The main idea behind Bayes UCB is to treat quantiles of the arm's prior as an upper confidence bound and let the quantile grows at some pre-specified rate. \cite{russo2014learning} propose Information Directed Sampling (IDS), an algorithm that exploits information theoretic quantities arising from the prior distributions over the arms. In simulations, IDS is shown to dominate other algorithms.

\noindent\emph{Gittins index and its approximations: }
There is another stream of literature that models each arm as a MDP that evolves, only when pulled,  and independently of other arms' MDPs. For the case of two arms, where one arm's reward is deterministic, \cite{bradt1956sequential} show that for this one-dimensional DP, an index rule is an optimal strategy. When the objective to maximize the infinite sum of expected \emph{discounted} rewards \cite{gittins1979bandit} shows that another index policy is also optimal (where the index is similar to \cite{bradt1956sequential} but takes into account discounting). The remarkable fact about Gittins' result is that it applies to any number of arms. Several alternative proofs of the same result are shown in \cite{tsitsiklis1994short,weber1992gittins,whittle1980multi,bertsimas1996conservation}, which also offer their own interpretations of it. 
 
Since the Gittins index is generally difficult to compute, several approximations to it exist in \cite{yao2006some,katehakis1987multi,varaiya1985extensions} with a survey in \cite{chakravorty2013multi}. This paper also relies on Gittins index approximations and we develop simple, general ones that enable our algorithm to be regret optimal. Finally, we became aware of recent work in \cite{lattimore2016bayesregret} which also focuses on regret minimization using (approximated) Gittins indices. However, the algorithm there is horizon-dependent and the proofs assume a Gaussian prior over the arms. More heuristics inspired by `finite-horizon' Gittins indices have been tested out recently in \cite{kaufmann2016bayesian}.

\subsection{Structure of the paper}
The remainder of this paper is organized as follows: in the next section, we state our notation, objectives of interest 
and key results such as the Lai-Robbins lower bound. The third section focuses on the Gittins Index and explains how it fails to minimize regret in a sense that is made clear later. At the end, we address another issue, namely the computational cost of calculating the Gittins Index, which inspires us to develop the Optimistic Gittins Index (OGI) policy. Section~\ref{sec:analysis_of_regret} establishes an optimal regret bound for the algorithm; namely, one that matches the Lai-Robbins lower bound. Following that, Section~\ref{sec:experiments} presents experiments showing how OGI achieves lower Bayesian regret than state of the art policies and is computationally efficient. In addition to the problem studied in earlier sections of the paper, we also demonstrate computationally the algorithm's effectiveness in a more general setting where it is possible to pull several arms at once in every iteration. Finally, in Section~\ref{sec:conclusions} we state open questions that remain following this work.