\section{Introduction} \label{sec:intro}

%points to make: (1) current bayesian landscape appears to offer v. good algos. (2) Bayesian setup yields an MDP, but this has not been leveraged. (3) Gitins index is optimal for MDPs. (4) Challenges with Gittins. (5) Our contribution. 

The Multi-Armed Bandit (MAB) problem is perhaps the simplest example of a learning problem that exposes the tension between exploration and exploitation. In its simplest form, we are given a collection of random variables or `arms'. By adaptively sampling these random variables, we seek to eventually sample consistently from the random variable with the highest mean. This is typically formalized by asking that we minimize cumulative `regret'; a notion we make precise in a later section. 

Recent years have seen a resurgence of interest in {\em Bayesian} algorithms for the MAB problem. In this variant of the MAB problem, we are endowed with a prior on arm means, and a number of algorithms that exploit this prior have been proposed and analyzed. These include Thompson Sampling \citep{thompson1933likelihood}, Bayes-UCB \citep{kaufmann2012thompson}, KL-UCB \citep{garivier2011kl}, and Information Directed Sampling \citep{russo2014learning}. The ultimate motivation for these algorithms appears to be the empirical performance they offer. Specifically, these Bayesian algorithms appear to incur smaller regret than their frequentist counterparts such as the UCB algorithm proposed by \cite{auer2002finite}, even when regret is measured in a frequentist sense. This empirical evidence has, very recently, been reinforced by theoretical performance guarantees. For instance, it has been shown that both Thompson sampling and Bayes-UCB enjoy upper bounds on frequentist regret that match the Lai-Robbins lower bound \citep{lai1985asymptotically}. Interestingly, even amongst the various Bayesian algorithm proposed there appears to be a wide range in empirical performance. For instance, empirical evidence presented in \cite{russo2014learning} suggests that the IDS algorithm offer a substantial improvement in frequentist regret over Thompson sampling and the Bayes-UCB algorithm, among others. The former algorithm does not however enjoy the optimal data dependent frequentist regret bounds that the latter two do. Perhaps more importantly, these algorithms also vary substantially in their design (as opposed to being variations on a theme).    

Now a prior on arm means endows us with the structure of a Markov Decision Process (MDP) and none of the Bayesian algorithms alluded to above exploit this structure. This is especially surprising in light of the celebrated Gittins Index Theorem. That breakthrough result proved the optimality of a certain index policy for a {\em horizon dependent} variant of the Bayesian MAB. Specifically, imagine that we cared about the expected (Bayes) regret incurred over an exponentially distributed horizon, where the mean horizon length is known to the algorithm designer. This problem is nominally a high dimensional MDP. Gittins, however, proved that a simple to compute index rule was optimal for this task resolving a problem that had remained open for several decades \citep{gittins1979bandit}. Why does the Gittins Index Theorem not immediately help resolve the design of an optimal algorithm for the variant of the Bayesian MAB problem that is the subject of the approaches discussed in the preceding paragraph? As we will discus more carefully in our literature review, this is certainly not from lack of research effort \citep{lattimore2016bayesregret}. In fact, one must deal with several substantial challenges:
\begin{enumerate}
\item
Dependence on Horizon: The notion of regret optimality as popularized by \cite{lai1985asymptotically} is `anytime'. Colloquially, this can be thought of as follows: we desire an algorithm that performs well for {\em any} time horizon. This fact is fundamentally at odds with Gittins' variant of the MAB problem that (via a discount factor) effectively specifies a (exponentially distributed) horizon. Gittins' result is intimately connected to this choice of horizon; even seemingly minor changes appear to render the problem intractable. For instance, it is known that a Gittins-like index strategy is sub-optimal for a fixed, finite-horizon \citep{berry1985bandit}. Algorithms for other notions of optimality that one may reasonably conjecture are better aligned with `anytime' regret optimality (such as, say, Cesaro-overtaking optimality) are similarly elusive \citep{katehakis1996finite}.  
\item
Computation: Separate from the issues made in the previous point, consider the task of computing a Gittins index at every point in time. 
The computation of a Gittins index can be reduced to the solution of a certain infinite horizon stopping problem. For the Bayesian MAB, the state space for this problem must describe all possible posteriors one may encounter on a given arm. Assuming conjugate priors, one may hope for a finite dimensional state space, but tractable computation will typically call for some form of state-space truncation. This computation is far more onerous than any of the aforementioned indices. Furthermore, it is reasonable to conjecture that as time progresses one may require increasingly more accurate estimates of the Gittins index, which further complicates computation, and calls into question the correctness of a naive state-space truncation scheme. 
\end{enumerate}
Against this backdrop, the present paper makes the following contribution: 
\newline
\newline
\noindent
{\em We show that picking arms according to a certain tractable approximation to their Gittins index, computed for a time dependent discount factor we characterize precisely, constitutes a regret optimal bandit policy. The resulting index rule is both simple to compute and in computational experiments appears to outperform state-of-the-art bandit algorithms by a material margin.} 
\newline
\newline
In greater detail, we outline our contributions as follows:

\begin{enumerate}

\item Optimistic Approximations: We propose a sequence of `optimistic' approximations to the Gittins index. These optimistic approximations can be interpreted as providing a tightening sequence of upper bounds on the optimal stopping problem defining a Gittins index, yielding the index itself in the limit. The computation associated with the simplest of these approximations is no more burdensome than the computation of indices for the Bayes UCB algorithm, and several orders of magnitude faster than the best performing alternative from an empirical perspective (the IDS algorithm). 
\item Regret Optimality: We establish that an arm selection rule that is greedy with respect to any optimistic approximation to the Gittins index achieves optimal regret in the sense of meeting the Lai-Robbins lower bound (including matching constants) for the canonical case of Beta-Bernoulli bandits. A crucial ingredient required for this scheme to work is that as time progresses, the discount factor employed in computing the index must be increased at a certain rate which we characterize precisely. This implicitly resolves the challenge of horizon dependence. 
\item Empirical Performance: We show empirically that even the simplest optimistic approximation to the Gittins index outperforms the state-of-the-art incumbent schemes discussed in this introduction by a non-trivial margin. Our empirical study is careful to recreate several ensembles of problem instances considered by previous authors (including a particularly computationally intensive study by \cite{chapelle2011empirical} that prompted the reexamination of the Thompson sampling algorithm in recent years). The margin of improvement we demonstrate increases further as one employs successfully tighter optimistic approximations, at the cost of computational effort. 
%We view these: the Bayesian MAB problem is fundamental, making the performance improvements we demonstrate important.  
\end{enumerate}

In summary, we propose a new index rule for the Baysian MAB problem that employs Gittins indices in a novel way. This new index rule enjoys the strongest possible data-dependent regret guarantees while also offering excellent empirical performance. 

%The ultimate motivation for these algorithms appears to be two-fold: superior empirical performance and light computational burden. The strongest performance results available for these algorithms establish regret lower bounds that match the Lai-Robbins lower bound \citep{lai1985asymptotically}.  Even among this set of recently proposed algorithms, there is a wide spread in empirically observed performance. Table~\ref{table:intro_algorithm_summary} lists well-known algorithms in the literature and what is known about them.

%\begin{table}
%	\centering
%	\begin{tabular}{@{}llll}\toprule
%		Algorithm & Bayes/Frequentist & Regret Optimal & Framework \\ \midrule
%		KL-UCB & Frequentist & Yes & Index-based \\
%		UCB & Frequentist & Unknown & Index-based \\
%		MOSS & Frequentist & Unknown & Index-based \\
%		Thompson Sampling &Bayes & Yes & Posterior Sampling \\
%		Bayes UCB & Bayes & Yes & Index-based \\
%		IDS & Bayes & Unknown  & Mixed \\
%		Gittins Index & Bayes & No & Index-based \\
%		\bottomrule
%	\end{tabular}
%	\caption{Summary of some famous bandit policies and their properties.}
%	\label{table:intro_algorithm_summary}
%\end{table}

%Interestingly, the design of the index policies referenced above has been somewhat ad-hoc as opposed to having emerged from a principled analysis of the underlying Markov Decision Process. Now if in contrast to requiring `small' regret for all sufficiently large time horizons, we cared about minimizing Bayesian regret over an infinite horizon, discounted at a fixed, pre-specified rate (or equivalently, maximizing discounted infinite horizon rewards), the celebrated Gittin's index theorem provides an {\em optimal, efficient} solution. Importing this celebrated result to the fundamental problem of designing algorithms that achieve low regret (either frequentist or Bayesian) simultaneously over all sufficiently large time horizons runs into two substantial challenges:

%\noindent {\em High-Dimensional State Space: }Even minor `tweaks' to the discounted infinite horizon objective appear to render the corresponding Markov Decision problem for the Bayesian MAB problem intractable. For instance, it is known that a Gittins-like index strategy is sub-optimal for a fixed, finite-horizon \citep{berry1985bandit}. Moreover, the problem of minimizing regret simultaneously over all sufficiently large horizons is not well understood.
%\newline
%\newline
%\noindent {\em Computational Burden: }Even in the context of the discounted infinite horizon problem, the computational burden of calculating a Gittins index is substantially larger than that required for any of the index schemes for the multi-armed bandit discussed thus far. 

%The present paper attempts to make progress on these challenges. Specifically, we make the following contributions:
%\begin{itemize}
%	\item We propose a class of `optimistic' approximations to the Gittins index that can be computed with significantly less effort. In fact, the computation of the simplest of these approximations is no more burdensome than the computation of indices for the Bayes UCB algorithm, and several orders of magnitude faster than the nearest competitor, IDS. 
%	\item We establish that an arm selection rule that is greedy with respect to the simplest of these optimistic approximations achieves optimal regret in the sense of meeting the Lai-Robbins lower bound (including matching constants) provided the discount factor is increased at a certain rate.
%	\item We show empirically that even the simplest optimistic approximation to the Gittins index proposed here {\em outperforms the state-of-the-art incumbent schemes discussed in this introduction by a non-trivial margin}. We view this as our primary contribution -- the Bayesian MAB problem is fundamental making the performance improvements we demonstrate important.  
%\end{itemize}



\subsection{Relevant Literature}
We organize our literature review around the primary topics that this paper touches on. The study of exploration-exploitation problems is vast, even if it is restricted to a problem with a finite number of arms. Consequently, our review will be focused on stochastic, non-contextual, versions of the MAB problem. Even with this restriction, the literature remains vast, and so we focus on papers that are either seminal in nature or particularly relevant to our own work; this review is by no means comprehensive with respect to the MAB problem. 
\newline
\noindent\textbf{\textsf{Regret optimality and the bandit problem: }}\cite{robbins1952some} motivated the study of the MAB problem and left open questions on how to design effective policies. Since then \cite{lai1985asymptotically} proved a cornerstone result, namely an asymptotic lower bound on regret that any consistent strategy incurs. The same paper proposes an upper-confidence bound (UCB) algorithm that asymptotically achieves the lower bound. Computationally efficient UCB algorithms were developed by \cite{agrawal1995sample} and \cite{katehakis1995sequential}. Later, \cite{auer2002finite} and \cite{audibert2010regret} proved finite time regret bounds for UCB algorithms and demonstrated ways to tune them in order to improve performance. \cite{garivier2011kl} and \cite{maillard2011finite} have proposed other UCB-type algorithms where the confidence bounds are calculated using the KL-divergence function. Those authors provide a finite-time analysis and their algorithms are shown to achieve the Lai-Robbins bound.
\newline
\noindent\textbf{\textsf{Bayesian bandit algorithms: }} Another powerful approach to bandit problems is to work with a Bayesian prior to model one's uncertainty about an arm's expected reward. \cite{lai1987adaptive}  proves an asymptotic lower bound on Bayes' risk and develops a horizon-dependent algorithm that achieves it.
%The advantage of such algorithms is that they can make use of existing knowledge about an arm, such as the family of distributions that its rewards comes from, to make better decisions.
Thompson Sampling \citep{thompson1933likelihood}, one of the earliest algorithms proposed for the MAB problem, is in fact a Bayesian one. Empirical studies by \cite{chapelle2011empirical} and \cite{scott2010modern} highlight Thompson Sampling's hugely superior performance over some UCB algorithms even when the prior is mismatched. A series of tight regret bounds for Thompson Sampling have been established by \cite{agrawalanalysis,agrawal2013further} and \cite{kaufmann2012thompson}. These authors have shown Thompson sampling to be regret optimal for the canonical Beta-Bernoulli bandit. Recently, \cite{korda2013thompson} generalized the aforementioned results to bandit problems where the arm distributions belong to a one dimensional exponential family. Interestingly enough, \cite{robbins1952some} seems to have been unaware of Thompson Sampling and its effectiveness in the non-Bayesian setting.

Several other Bayesian algorithms exist. \cite{kaufmann2012thompson} propose Bayes UCB, which they show is competitive with Thompson Sampling. The main idea behind Bayes UCB is to treat quantiles of the arm's prior as an upper confidence bound and let the quantile grows at some pre-specified rate. \cite{russo2014learning} propose Information Directed Sampling (IDS), an algorithm that exploits information theoretic quantities arising from the prior distributions over the arms. In simulations, IDS is shown to dominate many of the aforementioned algorithms, including Thompson Sampling, Bayes UCB and KL-UCB. In our empirical investigation, we will see that IDS is the closest competitor to the approach we propose here (we recreate the experiments from \cite{russo2014learning}).

\noindent\textbf{\textsf{Gittins index and its approximations: }}
There is another stream of literature that models the MAB problem as an MDP. For the case of two arms, where one arm's reward is deterministic, \cite{bradt1956sequential} showed that for this one-dimensional DP, an index rule is an optimal strategy. When the objective is to maximize the infinite sum of expected \emph{discounted} rewards \cite{gittins1979bandit} famously showed the optimality of an index policy. The Gittins index is similar to that proposed by \cite{bradt1956sequential} but takes discounting into account. Several alternative proofs of Gittins' result are available; see for example \citep{tsitsiklis1994short,weber1992gittins,whittle1980multi} and \citep{bertsimas1996conservation}. These alternative proofs also provide illuminating alternative interpretations of the Gittins index.  
 
Computing the Gittins index can be an onerous task, especially when the state space corresponding to posterior sufficient statistics is large or high dimensional. As such, approximations to the index have been proposed by \cite{yao2006some,katehakis1987multi} and \cite{varaiya1985extensions}; see \citep{chakravorty2013multi} for a survey. 

{\color{blue}
In this vain, it is worth noting that asymptotic links between Gittins indices and upper confidence bounds have been recognized by \cite{fang1987characterization}. That paper considers a diffusion approximation to the stopping problem associated with the computation of a Gittins index, and shows that the analog to the Gittins index in the context of that problem enjoys an asymptotic expansion that resembles an upper confidence bound. Subsequent to our work here, \cite{russo2019note} establishes an explicit relationship between Gittins indices and Bayesian upper confidence bounds. 
}

This paper also relies on Gittins index approximations and we develop simple, general ones that enable our algorithm to be regret optimal. 

{\color{blue}
Now Bayesian bandit algorithms (such as Bayes-UCB, Thompson sampling, or IDS) all admit natural extensions to settings substantially more complex that the independent arm case that is the focus of this paper. It is also the case that the Gittins index theorem has inspired index calculations for more sophisticated bandit problems (an example is Whittle's heuristic  \citep{whittle1988restless}). Our hope is that together with the use of an increasing discount factor schedule analyzed in this paper such index policies can provide a starting point for algorithm design in more complex bandit problems. 
}

Finally, we note that others have contemporaneously attempted to leverage the Gittins index in the construction of a Bayesian MAB algorithm. For instance, \cite{kaufmann2016bayesian} considers a variety of heuristics based on a finite horizon version of the Gittins index (essentially, the index proposed by \cite{bradt1956sequential}), and shows promising empirical results. \cite{lattimore2016bayesregret} analyzes the regret under a similar index and shows it to be logarithmic for a {\em fixed} horizon. Unfortunately, the index policies studied in both \citep{kaufmann2016bayesian} and \citep{lattimore2016bayesregret} {\em require a-priori knowledge of a horizon}. As such this does not yield an index rule that works for any sufficiently large horizon, but rather one that only works for a fixed pre-specified horizon. In fact, such schemes cannot be expected to work well for time horizons other than the pre-specified horizon determining the index. {\color{blue} In trying to extend such a policy to one that does not require the horizon to be pre-specified, one may rely on the so-called doubling trick (we use such a method for a different purpose in Proposition 1 of this paper). In using such an approach one incurs a multiplicative increase in regret by a factor that grows logarithmically in the horizon yielding a sub-optimal result. It is unclear to us how one can avoid this. }
In contrast, we seek to provide a compelling alternative to the host of state-of-the-art `anytime' regret optimal index rules discussed heretofore. 


 

%we became aware of recent work in \cite{lattimore2016bayesregret} which also focuses on regret minimization using (approximated) Gittins indices. However, the algorithm there is horizon-dependent and the proofs assume a Gaussian prior over the arms. More heuristics inspired by `finite-horizon' Gittins indices have been tested out recently in \cite{kaufmann2016bayesian}.

\subsection{Structure of the paper}
The remainder of this paper is organized as follows: in the next section, we state our notation, objectives of interest 
and key results such as the Lai-Robbins lower bound. The third section focuses on the Gittins Index and explains how it fails to minimize regret in a sense that is made clear later. At the end, we address another issue, namely the computational cost of calculating the Gittins Index, which inspires us to develop the Optimistic Gittins Index (OGI) policy. Section~\ref{sec:analysis_of_regret} establishes an optimal regret bound for the algorithm; namely, one that matches the Lai-Robbins lower bound. Following that, Section~\ref{sec:experiments} presents experiments showing how OGI achieves lower Bayesian regret than state of the art policies and is computationally efficient. In addition to the problem studied in earlier sections of the paper, we also demonstrate computationally the algorithm's effectiveness in a more general setting where it is possible to pull several arms at once in every iteration. Finally, in Section~\ref{sec:conclusions} we state open questions that remain following this work.