\section{Optimistic Gittins Indices} \label{sec:gittins_and_approx}

This section introduces the notion of an optimistic Gittins index, and presents an algorithm for the MAB problem that we will subsequently show is optimal in that it achieves the Lai-Robbins lower bound. We will begin with reviewing the Gittins index theorem for the discounted infinite horizon bandit problem and show that one cannot expect the use of the index from that well known result to yield a regret optimal policy for the MAB problem. We then show that the use of the Gittins index in concert with an increasing discount factor yields poly-logarithmic Bayesian regret. This coarse result motivates the discount factor schedule we eventually propose. Finally, we present a series of `optimistic' approximations to the Gittins index with the view of minimizing the computational burden of index computation. Putting these ingredients together yields the optimistic Gittins index algorithm that is the subject of our paper. 

\subsection{The Gittins Index and Regret}

The Gittins index theorem presents a surprisingly simple solution to the problem of computing an optimal policy for the discounted infinite horizon bandit problem. Specifically, the theorem defines for each arm state $y \in \Yscr$, an index we denote $v_\gamma(y)$; we define this index shortly. The theorem shows that an arm selection rule which at every time selects the arm with the highest index is optimal. The result is powerful in that the computation of the index for a given arm requires the solution of an MDP on the state space $\Yscr$, as opposed to solving an MDP on the considerably larger state space $\Yscr^A$. 

One way to compute the Gittins Index $v_\gamma(y)$ for an arm in state $y$ is via the so-called retirement value formulation \cite{whittle1980multi}. Specifically, $v_\gamma(y)$ is defined as the value of $\lambda$ that solves
\begin{equation} \label{eqn:gittins_index}
\frac{\lambda}{1-\gamma} = \sup_{\tau > 1}\E_{y}\left[\sum_{t=1}^{\tau-1} \gamma^{t-1} X_{i,t} + \gamma^{\tau-1} \frac{\lambda}{1-\gamma}
%	\given y_{i,1} = y
\right],
\end{equation}
where the subscript on the expectation indicates that the prior on the (say, $i$th) arm's mean at time $t=1$, $y_{i,1}$, equals $y$. If one thought of the notion of retiring as receiving a deterministic reward $\lambda$ in every period, then the value of $\lambda$ that solves the above equation could be interpreted as the per-period retirement reward that makes us indifferent between retiring immediately (i.e. at $t=1$), and the option of continuing to play arm $i$ with the potential of retiring at some future time. The Gittins index policy itself, which we denote by $\pi^{G,\gamma}$, can succinctly be stated as follows: 
\begin{center}
{\em At time $t$, play an arm in the set 
$\argmax_{i} v_\gamma
\left(
y_{i,N_i(t)}
\right)$.
} 
\end{center}
Ignoring computational considerations, we cannot hope for the policy $\pi^{G,\gamma}$ to be regret optimal. In fact, as the result below indicates, one cannot even hope for such a policy to be consistent in the sense of \cite{lai1985asymptotically}:
\begin{lemma}
\label{le:linearregret}
	For any $\gamma > 0$, there exists an instance of the multi armed bandit problem for which 
	\[
	{\rm Regret}\left(
	\pi^{G,\gamma},T
	\right)
	= 
	\Omega(T).
	\]
\end{lemma}
The proof, given in Appendix~\ref{proof:linearregret}, rests on the simple fact that for any fixed discount factor, if the posterior means on the two arms are sufficiently apart, the Gittins index policy will pick the arm with the larger posterior mean. The threshold beyond which the Gittins policy `exploits' depends on the discount factor and with a fixed discount factor there is a positive probability that the superior arm is never explored sufficiently so as to establish that it is, in fact, the superior arm. 

\subsection{Increasing Discount Factors yield sub-linear Bayesian Regreat}

Lemma~\ref{le:linearregret} tells us that we cannot hope for sub-linear regret by applying the Gittins index policy with a constant discount factor. One may naturally wonder whether an increasing discount factor might fix this issue. Now observe that any schedule of increasing discount factors effectively implies a change in the trade-off between exploration and exploitation. With a fixed discount factor, we have already seen that once the priors between two arms are sufficiently far apart, the Gittins policy will not explore, thereby leading to the possibility of linear regret. As the discount factor increases, the `gap' between priors above which exploration is not justified goes up over time. If we increase this `gap' too fast, we might incur too much exploration. Too slow, and we might incur too little exploration. As such, the schedule at which we increase the discount factor is likely to play a significant role in determining the regret of the resulting policy. 

Now notice that the Gittins index policy for a discount factor $\gamma$ can be viewed as optimal for a {\em random} finite horizon, distributed geometrically with parameter $1-\gamma$. As $\gamma$ grows small, this may be thought of as a near optimal policy for the fixed finite horizon $1/1-\gamma$. Now consider for a moment that we had access to a policy that has optimal $T$ period expected regret (assuming $T$ is known in advance). One way to convert such a policy into a policy that has `low' regret for any $T$ is to employ the so-called doubling trick: Apply the optimal policy for the horizon $T$ for $T$ steps, then the optimal policy for horizon $2T$ for the following $2T$ steps, followed by the optimal policy for $4T$ for the next $4T$ steps, and so-forth. Such a policy will be `near'-optimal for any horizon, in a manner we now make precise. 

Consider employing discount factors that increase at roughly the rate $1 - 1/t$; specifically, consider setting 
\[
\gamma_t = 
1 - 
\frac{1}
{2^{\floor{\log_2 t}+1}}
\]
and consider using the policy that at time $t$ picks an arm from the set $\argmax_i \nu_{\gamma_t}(y_{i,N_i(t)})$. Denote this policy by $\pi^{\rm D}$. The following proposition shows that this `doubling' policy achieves Bayes risk that is within a factor of $\log T$ of the optimal Bayes risk. Specifically, we have:
\begin{proposition}
	\label{prop:gittins_log3T}
	\[
	{\rm Regret}(\pi^{\rm D},T)
	=
	O
	\left(
	\log^3 T
	\right).
	\] 
	where the constant in the big-Oh term depends on the prior $q$ and $A$.
\end{proposition}
The proof of this simple result (Appendix~\ref{proof:prop_log3T}) relies on showing that the finite horizon regret achieved by using a Gittins index with an appropriate fixed discount factor is within a constant factor of the optimal finite horizon regret. The second ingredient is the doubling trick described above. The coarse analysis above illustrates that the use of the Gittins index policy together with an increasing discount factor does indeed yield an algorithm with sub-linear Bayesian regret. It is worth noting that the result above does not show that such a policy achieves {\em optimal} Bayesian regret (the achievable lower bound being $\log^2T$ \citep{lai1987adaptive}). The analysis does however suggest a candidate discount rate schedule that we will eventually show to yield a regret optimal policy. 
 
\subsection{Effective Computation: Optimistic Approximations to The Gittins Index}\label{sec:approx_agi_deriv}

The retirement value formulation makes clear that computing a Gittins index is equivalent to solving a discounted, infinite horizon stopping problem. Solving this problem requires substantially more computational effort than, say, Thompson Sampling or the Bayes UCB algorithm. In fact, this computation can even be rendered intractable in practice. Specifically, the set $\Yscr$ can be high dimensional; see \cite{chapelle2011empirical} for one such example that arises in the context of contextual news recommendations. This motivates an approximation to the Gittins index that is the subject of this section. Specifically, we introduce a sequence of `optimistic' approximations to the Gittins index that will alleviate computational burden. 

Consider the following alternative stopping problem that requires as input the parameters $\lambda$ (which has the same interpretation as it did before), and $K$, an integer limiting the number of steps that we need to look ahead. For an arm in state $y$ (recall that the state specifies sufficient statistics for the current prior on the arm reward), let $R(y)$ be a random variable drawn from the prior on expected arm reward specified by $y$. Define the retirement value $R_{\lambda,K}(s,y)$ according to 
\[
R_{\lambda,K}(s, y) = 
\begin{cases}
\lambda ,& \text{if } s < K+1\\
\max\left(\lambda, R(y) \right), & \text{otherwise}
\end{cases}
\]
For a given $K$, the \emph{Optimistic Gittins Index} for arm $i$ in state $y$ is now defined as the value for $\lambda$ that solves
\begin{equation} \label{eqn:ogi_general}
\frac{ \lambda}{1-\gamma} = \sup_{1 < \tau \leq K+1}
\E_y\left[
	\sum_{s=1}^{\tau-1} \gamma^{s-1}X_{i,s} + \gamma^{\tau-1} \frac{R_{\lambda,K}(\tau, y_{i,\tau-1})}{1-\gamma}
\right],
\end{equation}
where we recall that the subscript on the expectation indicates that $y_{i,1} = y$. 
We denote the solution to this equation by $v_\gamma^{K}(y)$.
The problem above admits a simple, attractive interpretation: nature reveals the {\em true} mean reward for the arm at time $K+1$ should we choose to not retire prior to that time, which enables the decision maker to then instantaneously decide whether to retire at time $K+1$ or else, never retire. In this manner one is better off than in the stopping problem inherent to the definition of the Gittins index, so that we use the moniker optimistic. The following Lemma formalizes this intuition

\begin{lemma} \label{lemma:approx_bound}
	For all discount factors $\gamma$ and states $y \in \mathcal Y$, we have 
	\[
	v_\gamma^{K}(y) \geq v_\gamma(y) \quad \forall K.
	\]
\end{lemma}
\begin{myproof}[Proof]
	See Appendix~\ref{prf:approx_bound}%.\halmos
\end{myproof}

Now, since we need to look ahead at most $K$ steps in solving the stopping problem implicit in the definition above, the computational burden in index computation is limited. In fact, we will see in a subsequent section that {\em even the choice of $K=1$} will make for a compelling policy. 


\subsection{The Optimistic Gittins Index Algorithm}

The discussion thus far suggests a simple class of bandit algorithms we dub the Optimistic Gittins Index (OGI) algorithm. The algorithm itself requires as input a prior on arm means (as does any Bayesian algorithm for the MAB), and a parameter $K$. 


\noindent The OGI algorithm may be summarized succinctly as follows:

\begin{center}
{\em At time $t$ play an arm in the set $\argmax_i v^K_{\gamma_t}(y_{i,t})$}
\end{center}

\noindent where $\gamma_t = 1 - \frac{1}{t}$. 

The following Section will establish the surprising result that the algorithm above achieves the Lai-Robbins lower bound (and thus is regret optimal), for any finite $K$ (and thus, even for $K=1$!). We will establish this result for Beta priors. To appreciate the computation required for this algorithm in simplest case where $K=1$, we end this section with a few concrete examples. Note that for $K=1$, equation~\eqref{eqn:ogi_general} simplifies to
\begin{equation} \label{eqn:ogi_k1}
\lambda = \hat \mu(y) + \gamma \E[(\lambda - R(y))^+]
\end{equation}
where $\hat \mu(y) := \E[R(y)]$ is the mean reward under the prior given by $y$. The equation for $\lambda$ above can also be viewed as an upper confidence bound to an arm's expected reward. Solving equation~\eqref{eqn:ogi_k1} is often simple in practice, and we list a few examples to illustrate this:
\begin{example}[Beta]
	In this case $y$ is the pair $(a,b)$, which specifices a Beta prior distribution. The 1-step Optimistic Gittins Index, is the value of $\lambda$ that solves
	\begin{align*}
	\lambda = \frac{a}{a+b} + \gamma \E[(\lambda - {\rm Beta}(a,b))^+] =  \frac{a}{a+b}(1 - \gamma F^\beta_{a+1,b}(\lambda)) + \gamma \lambda (1-F^\beta_{a,b}(\lambda))
	\end{align*}
	where $F^\beta_{a,b}$ is the CDF of a Beta distribution with parameters $a, b$.
\end{example}

\begin{example}[Gaussian]
	Here $y = (\mu,\sigma^2)$, which specifices a Gaussian prior and the corresponding equation is
	\begin{align*}
	\lambda & = \mu  + \gamma \E[(\lambda - {\rm \mathcal{N}}(\mu,\sigma^2))^+]  \\
	& = \mu + \gamma \left[( \lambda - \mu) \Phi\left(\frac{\mu - \lambda}{\sigma}\right) + \sigma\phi\left(\frac{\mu-\lambda}{\sigma}\right) \right]
	\end{align*}
\end{example}

Notice that in both the Beta and Gaussian examples, the equations for $\lambda$ are in terms of distribution functions. Therefore it is straightforward to compute a derivative for these equations (which would be in terms of the density and CDF of the prior) and makes finding a solution, using a method such as Newton-Raphson, simple and efficient. 

